+++
title = '全球开源开放大模型发展情况'
date = 2023-12-19T14:43:38+08:00
draft = true

+++

# 开源开放的大模型

旨在记录全球开源开放大模型发展情况


## 基础大模型

| 序号 | 名称                                                         | 参数规模              | 数据规模  | 说明                                                         |
| :--- | :----------------------------------------------------------- | :-------------------- | :-------- | :----------------------------------------------------------- |
| 1    | [LLaMA-2](Open-LLMs/llama2.md)                               | 7B,13B,34B,70B        | 2T        | 可商用                                                       |
| 2    | [Falcon](Open-LLMs/falcon.md)                                | 7B,40B,180B           | 3.5T      | 数据集[ RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) |
| 3    | [baichuan-2](Open-LLMs/baichuan2.md)                         | 7B,13B                | 2.6T      | 开放，商用需授权，[baichuan-1](Open-LLMs/baichuan.md)        |
| 4    | [InternLM](Open-LLMs/internlm.md)                            | 7B,20B                | 2.3T      | 开放，商用需授权                                             |
| 5    | [BLOOM](Open-LLMs/bloom.md)                                  | 3B,7.1B,176B          | 366B      | 可商用，最为宽松，[详细介绍](https://mp.weixin.qq.com/s/ia-yrmXbnlooRA3K1hoTwQ) |
| 6    | GALACTICA                                                    | 6.7B,30B,120B         | 106B      | 开放的科学文本和数据                                         |
| 7    | [LLaMA](Open-LLMs/llama.md)                                  | 7B,13B,30B,65B        | 1.4T      | Meta，代码开源，模型“泄露”,不可商用，[详细介绍](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA) |
| 8    | MOSS-moon                                                    | 16B                   | 700B      | 6.67x1022 FLOPs                                              |
| 9    | ChatGLM2                                                     | 6B                    | 1.4T      |                                                              |
| 10   | StableLM                                                     | 3B,7B                 | 800B      |                                                              |
| 11   | RedPajama-INCITE                                             | 3B,7B                 | 1T        |                                                              |
| 12   | GPT-NeoX                                                     | 20B                   | 3.15M     | 800GB的[The Pile](https://arxiv.org/abs/2101.00027)数据集    |
| 13   | OpenLLaMA                                                    | 3B,7B,13B             | 1T        |                                                              |
| 14   | MPT                                                          | 7B,30B                | 1T        |                                                              |
| 15   | Pythia                                                       | 2.8B,6.9B,12B         | 300B      |                                                              |
| 16   | XGen                                                         | 7B                    | 1.5T      |                                                              |
| 17   | OPT                                                          | 6.7B,13B,30B,66B,175B | 180B      |                                                              |
| 18   | [Qwen](Open-LLMs/qwen.md)                                    | 7B,14B                | 2.4T,3.0T |                                                              |
| 19   | XVERSE                                                       | 13B                   | 1.4T      |                                                              |
| 20   | [Aquila2](https://github.com/FlagAI-Open/Aquila2)            | 7B,34B                |           |                                                              |
| 21   | Prithvi                                                      |                       |           | IBM+NASA,地理空间，100M（图片）                              |
| 22   | [Skywork](Open-LLMs/skywork.md)                              | 13B                   | 3.2T      | 昆仑万维·天工                                                |
| 23   | [Deepseek Coder](https://github.com/deepseek-ai/DeepSeek-Coder) | 1.3B,6.7B,33B         | 2T        | Deepseek Coder comprises a series of code language models trained on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens. |
| 24   | Aquila                                                       | 7B                    |           | 悟道·天鹰                                                    |

## 非基础大模型

- WizardLM，WizardMath，WizardCoder
- Alpaca
- Vicuna
- Guanaco
- [CodeLLaMA](Open-LLMs/codellama.md)
  - 7B,13B,34B

## 模型架构

- [GPTQ](https://github.com/IST-DASLab/gptq)
- [LLaMA](https://github.com/facebookresearch/llama)

