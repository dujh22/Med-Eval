+++
title = '通用大模型榜单'
date = 2023-12-20T18:47:46+08:00
draft = true
+++

| 模型                 | 开源地址                                              | 参数规模 | 所用数据                                                     |
| -------------------- | ----------------------------------------------------- | -------- | ------------------------------------------------------------ |
| ChatGLM-6B           | https://github.com/THUDM/ChatGLM-6B                   | 62亿     | 1T 标识符的中英双语训练                                      |
| ChatGLM2-6B          | https://github.com/THUDM/ChatGLM2-6B                  |          | 1.4万亿中英文tokens数据集上训练，并做了模型对齐+SFT          |
| GLM-130B             | https://github.com/THUDM/GLM-130B                     | 1300亿   | 4000 亿个文本token训练+SFT                                   |
| Chinese-LLaMA-Alpaca | https://github.com/ymcui/Chinese-LLaMA-Alpaca         |          | 在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练 |
| Moss                 | https://github.com/OpenLMLab/MOSS                     |          | 七千亿中英文以及代码单词上预训练                             |
| baichuan-7B          |                                                       |          | 1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096 |
| AquilaChat-7B        | {{https://mp.weixin.qq.com/s/XkoLnFycG1jPWrNT3w_p-g}} |          |                                                              |
| CPM-Bee              |                                                       |          | https://zhuanlan.zhihu.com/p/639459740                       |
| MPT-30b              |                                                       |          | https://www.bilibili.com/video/BV1UW4y1D7N9/?share_source=copy_web |

