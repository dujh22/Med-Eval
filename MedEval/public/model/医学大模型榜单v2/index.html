<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>医学大模型榜单V2 - Med-Eval</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="
  
      
          模型名称
          模型类型
          支持语言
          参数规模
          发布机构
          模型最后一次更新时间
          简介
          GitHub地址
          Hugging Face 地址
          论文地址
          官网地址
          其它信息来源
          API 是否可用
          API 使用地址
          是否可以私有化部署
          Demo 地址
          上下文长度
          训练用数据
          训练基座
          是否可商用
      
  
  
      
          gpt-4-1106-preview
          通用
          多语言
          1.8T (未证实)
          OpenAI
          2023.11.6
          GPT-4，或称Generative Pre-trained Transformer 4，是由OpenAI开发的最先进的语言模型。它的参数量大概是其前身GPT-3的10倍。这一进步使GPT-4能够以惊人的准确性和细微差别理解和生成类似人类的文本。它擅长各种任务，如回答问题、写文章、总结文本等等。它在不同数据集上进行大规模训练，能够深入理解多种语言和科目，使其成为人工智能驱动应用程序中的通用工具。
          无
          无
          无
          https://platform.openai.com/docs/overview
          到处都有
          付费，输入/输出 1k token 花费 0.01$/0.03$
          https://platform.openai.com/docs/api-reference
          否
          https://chat.openai.com/
          128K
          未公布
          无
          是
      
      
          gpt-3.5-turbo-1106
          通用
          多语言
          175B (未证实)
          OpenAI
          2023.11.6
          GPT-3.5是OpenAI开发的先进自然语言处理模型，作为GPT-3系列的改进版本。它继承了GPT-3的基本架构，拥有数以百亿计的参数，能够更有效地处理和生成自然语言文本。GPT-3.5在理解复杂语境、回答问题、撰写文章和文本摘要方面表现出色。通过广泛的数据训练，GPT-3.5能够处理多种语言和领域的内容，是人工智能应用中的多功能工具。
          无
          无
          无
          https://platform.openai.com/docs/overview
          到处都有
          付费，输入/输出 1k token 花费 0.001$/0.002$
          https://platform.openai.com/docs/api-reference
          否
          https://chat.openai.com/
          16K
          未公布
          无
          是
      
      
          ChatGLM3-6B
          通用
          双语
          6.2B
          清华大学
          2023.10
          ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性：更强大的基础模型： ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略；更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景；更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、长文本对话模型 ChatGLM3-6B-32K。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。
          https://github.com/THUDM/ChatGLM3
          https://huggingface.co/THUDM/chatglm3-6b
          https://arxiv.org/pdf/2210.02414.pdf
          https://www.chatglm.cn/
          无
          私有部署免费，官网调用收费
          API 开发文档
          是，32GB内存，13GB显存
          无(可以本地安装)
          8K/32K
          The pile； Wudaocorpora
          ChatGLM3-6B-Base
          填写问卷后可以免费商用
      
      
          BenTsao (本草)
          专业
          中文
          同基座
          哈尔滨工业大学
          2023.8.3
          开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。
          https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese
          无
          https://arxiv.org/pdf/2304.06975.pdf
          无
          无
          不可用
          无
          否
          无
          同基座
          github仓库里提到了若干个微调用数据集，之后可以调研
          活字1.0/Bloom-7B/Alpaca-Chinese-7B/LLaMA-7B
          否
      
      
          DoctorGLM
          专业
          双语
          同基座
          上海交通大学，复旦大学
          2023.6
          基于 ChatGLM-6B的中文问诊模型，通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署
          https://github.com/xionghonglin/DoctorGLM
          无
          https://arxiv.org/pdf/2304.01097.pdf
          https://xionghonglin.github.io/DoctorGLM/
          https://zhuanlan.zhihu.com/p/622649076
          不可用
          无
          否
          无
          同基座
          见仓库
          ChatGLM-6B
          不清楚
      
      
          BianQue
          专业
          中文
          同基座
          华南理工大学
          2023.6.6
          一个经过指令与多轮问询对话联合微调的医疗对话大模型，基于ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。
          https://github.com/scutcyr/BianQue
          https://huggingface.co/scutcyr/BianQue-2
          https://arxiv.org/pdf/2310.15896.pdf
          无
          无
          可以私有部署api
          https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py
          是
          无
          同基座
          我们结合当前开源的中文医疗问答数据集（MedDialog-CN、IMCS-V2、CHIP-MDCFNPC、MedDG、cMedQA2、Chinese-medical-dialogue-data），分析其中的单轮/多轮特性以及医生问询特性，结合实验室长期自建的生活空间健康对话大数据，构建了千万级别规模的扁鹊健康大数据BianQueCorpus。
          ChatGLM-6B
          否
      
      
          HuatuoGPT-II
          专业
          双语
          7B/13B/34B
          深圳大数据研究院，香港中文大学(深圳)
          2023.11.23
          开源了经过中文医学指令精调/指令微调(Instruct-tuning)的一个GPT-like模型
          https://github.com/FreedomIntelligence/HuatuoGPT-II
          好多个，详见github
          https://arxiv.org/pdf/2311.09774.pdf
          无
          无
          私有部署，自己开发
          无
          是
          https://www.huatuogpt.cn/
          同基座
          https://github.com/king-yyf/CMeKG_tools，还用ChatGPT构造了一些，和本草相同
          Baichuan2-7B-Base/Baichuan2-13B-Base/Yi-34B
          不清楚
      
      
          Med-ChatGLM
          专业
          中文
          同基座
          哈尔滨工业大学
          2023.3
          基于中文医学知识的ChatGLM模型微调，微调数据与BenTsao相同。
          https://github.com/SCIR-HI/Med-ChatGLM
          无
          无
          无
          无
          或许可用
          无
          是，下载地址：https://pan.baidu.com/share/init?surl=Sfi1bRwV741GIChIEOUW0A&amp;pwd=i73e，https://drive.google.com/drive/folders/1ZQSN56DloRGQ-Qj7IwzY4jV3ZHKMe9Bc
          无
          同基座
          https://github.com/king-yyf/CMeKG_tools，还用
          ChatGLM-6B
          否
      
      
          QiZhenGPT
          专业
          中文
          7B/6B/13B
          浙江大学
          2023.6.27
          该项目利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。
          https://github.com/CMKRG/QiZhenGPT
          无
          无
          无
          无
          可以私有部署
          无
          是，见github
          无
          同基座
          见仓库
          Chinese-LLaMA-Plus-7B/CaMA-13B/ChatGLM-6B
          否
      
      
          ChatMed
          专业
          中文
          7B（同基座模型Llama-7b）
          华东师范大学
          2023.5.5
          该项目推出ChatMed系列中文医疗大规模语言模型，模型主干为LlaMA-7b并采用LoRA微调，具体包括ChatMed-Consult : 基于中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w&#43;在线问诊&#43;ChatGPT回复作为训练集；ChatMed-TCM : 基于中医药指令数据集ChatMed_TCM_Dataset，以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到2.6w&#43;的围绕中医药的指令数据训练得到。
          https://github.com/michael-wzhu/ChatMed
          https://huggingface.co/michaelwzhu/ChatMed-Consult
          无
          无
          无
          可以私有部署api，需要下载模型后自己开发api
          无
          是，.bin文件80M
          无
          同基座
          https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset
          Llama-7b
          否
      
      
          XrayGLM
          专业
          双语
          6B，同基座
          澳门理工大学
          2023.5
          该项目为促进中文领域医学多模态大模型的研究发展，发布了XrayGLM数据集及模型，其在医学影像诊断和多轮交互对话上显示出了非凡的潜力。
          https://github.com/WangRongsheng/XrayGLM
          https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main
          无
          无
          无
          可以用webUI
          详见github中webUI一部分
          是
          无
          同基座
          https://physionet.org/content/mimic-cxr-jpg/2.0.0/   https://openi.nlm.nih.gov/faq#collection
          VisualGLM-6B
          否
      
      
          MeChat
          专业
          中文
          6B
          浙江大学，西湖大学
          2023.12
          该项目开源的中文心理健康支持通用模型由 ChatGLM-6B LoRA 16-bit 指令微调得到。数据集通过调用gpt-3.5-turbo API扩展真实的心理互助 QA为多轮的心理健康支持多轮对话，提高了通用语言大模型在心理健康支持领域的表现，更加符合在长程多轮对话的应用场景。
          https://github.com/qiuhuachuan/smile
          https://huggingface.co/qiuhuachuan/MeChat
          https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf
          无
          无
          可以，有交互文件可运行
          无
          是
          http://47.97.220.53:8080/ （似乎不能用了）
          同基座
          smileChat，见仓库
          ChatGLM2-6B
          不清楚
      
      
          MedicalGPT
          专业
          双语
          13B
          个人（应该是公司里的，百度/腾讯）
          2023.10.23
          训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。发布中文医疗LoRA模型shibing624/ziya-llama-13b-medical-lora，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重。
          https://github.com/shibing624/MedicalGPT
          https://huggingface.co/shibing624/vicuna-baichuan-13b-chat
          无
          无
          无
          可以，demo的api
          详见github Demo
          是
          详见github Demo部分
          同基座
          240w条中文医疗数据：https://huggingface.co/datasets/shibing624/medical  22万条中文医疗对话数据集(华佗项目)：FreedomIntelligence/HuatuoGPT-sft-data-v1
          BaiChuan-13B
          不清楚
      
      
          Sunsimiao
          专业
          中文
          7B
          华东理工大学
          2023.6
          Sunsimiao是一个开源的中文医疗大模型，该模型基于baichuan-7B和ChatGLM-6B底座模型在十万级高质量的中文医疗数据中微调而得。
          https://github.com/thomas-yanxin/Sunsimiao
          https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary
          无
          无
          无
          有单纯的类似命令行输入输出，一段使用代码
          无
          是
          无
          同基座
          没有提供10W中文医疗数据
          BaiChuan-7B
          不清楚
      
      
          ShenNong-TCM-LLM
          专业
          中文
          7B
          华东师范大学
          2023.6
          该项目开源了ShenNong中医药大规模语言模型，该模型以LlaMA为底座，采用LoRA (rank=16)微调得到。微调代码与ChatMed代码库相同。此外该项目还开源了中医药指令微调数据集。
          https://github.com/michael-wzhu/ShenNong-TCM-LLM
          无
          无
          无
          无
          无
          无
          否
          无
          同基座
          https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset   中医药知识图谱：https://github.com/ywjawmw/TCM_KG
          Llama-7B
          否
      
      
          SoulChat
          专业
          中文
          6B
          华南理工大学
          2023.7
          该项目开源了经过百万规模心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调的心理健康大模型灵心（SoulChat），该模型以ChatGLM-6B作为初始化模型，进行了全量参数的指令微调。
          https://github.com/scutcyr/SoulChat
          扁鹊：https://huggingface.co/scutcyr/BianQue-2  灵心：https://huggingface.co/scutcyr/SoulChat
          https://aclanthology.org/2023.findings-emnlp.83/
          无
          无
          可以，有Demo
          详见github启动服务
          是
          提供soulchat_app.py代码
          同基座
          数据集即将发布
          ChatGLM-6B
          否
      
      
          CareGPT
          专业
          双语
          有多种，包括7b、13B、14B、20B，同基座
          澳门理工大学
          2023.8
          该项目开源了数十个公开可用的医疗微调数据集和开放可用的医疗大语言模型，包含LLM的训练、测评、部署等以促进医疗LLM快速发展。
          https://github.com/WangRongsheng/CareGPT
          https://huggingface.co/wangrongsheng/carellm/tree/main
          无
          无
          无
          可用
          github上有介绍，在部署的部分
          是
          https://huggingface.co/spaces/wangrongsheng/CareLlama
          同基座
          在github数据部分
          llama-7b、baichuan
          是
      
      
          DISC-MedLLM
          专业
          双语
          13B
          复旦大学
          2023.8
          该项目是由复旦大学发布的针对医疗健康对话式场景而设计的医疗领域大模型与数据集，该模型由DISC-Med-SFT数据集基于Baichuan-13B-Base指令微调得到。
          https://github.com/FudanDISC/DISC-MedLLM
          https://huggingface.co/Flmc/DISC-MedLLM
          https://arxiv.org/abs/2308.14346
          无
          无
          可用，cli_demo.py
          详见github demo
          是
          web_demo.py
          同基座
          https://huggingface.co/datasets/Flmc/DISC-Med-SFT
          baichuan-13B-base
          否
      
      
          Taiyi-LLM
          专业
          双语
          7BB
          大连理工大学
          2023.10
          该项目由大连理工大学信息检索研究室开发的中英双语医学大模型&quot;太一&quot;，收集整理了丰富的中英双语生物医学自然语言处理（BioNLP）训练语料，总共包含38个中文数据集，通过丰富的中英双语任务指令数据（超过100W条样本）进行大模型（Qwen-7B-base）指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种BioNLP能力。
          https://github.com/DUTIR-BioNLP/Taiyi-LLM
          https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM
          https://arxiv.org/abs/2311.11608
          
          无
          似乎可用，dialog开头的python文件
          如前
          是
          https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/
          同基座
          https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md
          Qwen-7B -base
          否
      
  

下述模型仍然在信息更新中……

  
      
          模型名称
          模型类型
          简介
          GitHub地址
      
  
  
      
          WiNGPT
          专业
          WiNGPT是一个基于GPT的医疗垂直领域大模型，基于Qwen-7b1作为基础预训练模型，在此技术上进行了继续预训练，指令微调等，该项目具体开源了WiNGPT2-7B-Base与WiNGPT2-7B-Chat模型。
          https://github.com/winninghealth/WiNGPT2
      
      
          ChiMed-GPT
          专业
          ChiMed-GPT是一个开源中文医学大语言模型，通过在中文医学数据上持续训练 Ziya-v2 构建而成，其中涵盖了预训练、有监督微调 (SFT) 和来自人类反馈的强化学习 (RLHF) 等训练过程。
          https://github.com/synlp/ChiMed-GPT
      
      
          ChatGLM
          通用
          中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持
          https://github.com/THUDM/ChatGLM-6B
      
      
          ChatGLM2-6B
          通用
          基于开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练；基座模型的上下文长度扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练；基于 Multi-Query Attention 技术实现更高效的推理速度和更低的显存占用；允许商业使用。
          https://github.com/THUDM/ChatGLM2-6B
      
      
          Chinese-LLaMA-Alpaca
          通用
          中文LLaMA&amp;Alpaca大语言模型&#43;本地CPU/GPU部署，在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练
          https://github.com/ymcui/Chinese-LLaMA-Alpaca
      
      
          Chinese-LLaMA-Alpaca-2
          通用
          该项目将发布中文LLaMA-2 &amp; Alpaca-2大语言模型，基于可商用的LLaMA-2进行二次开发
          https://github.com/ymcui/Chinese-LLaMA-Alpaca-2
      
      
          Chinese-LlaMA2
          通用
          该项目基于可商用的LLaMA-2进行二次开发决定在次开展Llama 2的中文汉化工作，包括Chinese-LlaMA2: 对Llama 2进行中文预训练；第一步：先在42G中文预料上进行训练；后续将会加大训练规模；Chinese-LlaMA2-chat: 对Chinese-LlaMA2进行指令微调和多轮对话微调，以适应各种应用场景和多轮对话交互。同时我们也考虑更为快速的中文适配方案：Chinese-LlaMA2-sft-v0: 采用现有的开源中文指令微调或者是对话数据，对LlaMA-2进行直接微调 (将于近期开源)。
          https://github.com/michael-wzhu/Chinese-LlaMA2
      
      
          Llama2-Chinese
          通用
          该项目专注于Llama2模型在中文方面的优化和上层建设，基于大规模中文数据，从预训练开始对Llama2模型进行中文能力的持续迭代升级。
          https://github.com/FlagAlpha/Llama2-Chinese
      
      
          Qwen
          通用
          通义千问 是阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。各个规模的模型包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。数据集包括文本和代码等多种数据类型，覆盖通用领域和专业领域，能支持8K的上下文长度，针对插件调用相关的对齐数据做了特定优化，当前模型能有效调用插件以及升级为Agent。
          https://github.com/QwenLM/Qwen
      
      
          OpenChineseLLaMA
          通用
          基于 LLaMA-7B 经过中文数据集增量预训练产生的中文大语言模型基座，对比原版 LLaMA，该模型在中文理解能力和生成能力方面均获得较大提升，在众多下游任务中均取得了突出的成绩。
          https://github.com/OpenLMLab/OpenChineseLLaMA
      
      
          BELLE
          通用
          开源了基于BLOOMZ和LLaMA优化后的一系列模型，同时包括训练数据、相关模型、训练代码、应用场景等，也会持续评估不同训练数据、训练算法等对模型表现的影响。
          https://github.com/LianjiaTech/BELLE
      
      
          Panda
          通用
          开源了基于LLaMA-7B, -13B, -33B, -65B 进行中文领域上的持续预训练的语言模型, 使用了接近 15M 条数据进行二次预训练。
          https://github.com/dandelionsllm/pandallm
      
      
          Robin (罗宾)
          通用
          Robin (罗宾)是香港科技大学LMFlow团队开发的中英双语大语言模型。仅使用180K条数据微调得到的Robin第二代模型，在Huggingface榜单上达到了第一名的成绩。LMFlow支持用户快速训练个性化模型，仅需单张3090和5个小时即可微调70亿参数定制化模型。
          https://github.com/OptimalScale/LMFlow
      
      
          Fengshenbang-LM
          通用
          Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，该项目开源了姜子牙通用大模型V1，是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。除姜子牙系列模型之外，该项目还开源了太乙、二郎神系列等模型。
          https://github.com/IDEA-CCNL/Fengshenbang-LM
      
      
          BiLLa
          通用
          该项目开源了推理能力增强的中英双语LLaMA模型。模型的主要特性有：较大提升LLaMA的中文理解能力，并尽可能减少对原始LLaMA英文能力的损伤；训练过程增加较多的任务型数据，利用ChatGPT生成解析，强化模型理解任务求解逻辑；全量参数更新，追求更好的生成效果。
          https://github.com/Neutralzz/BiLLa
      
      
          Moss
          通用
          支持中英双语和多种插件的开源对话语言模型，MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。
          https://github.com/OpenLMLab/MOSS
      
      
          Luotuo-Chinese-LLM
          通用
          囊括了一系列中文大语言模型开源项目，包含了一系列基于已有开源模型（ChatGLM, MOSS, LLaMA）进行二次微调的语言模型，指令微调数据集等。
          https://github.com/LC1332/Luotuo-Chinese-LLM
      
      
          Linly
          通用
          提供中文对话模型 Linly-ChatFlow 、中文基础模型 Linly-Chinese-LLaMA 及其训练数据。 中文基础模型以 LLaMA 为底座，利用中文和中英平行增量预训练。项目汇总了目前公开的多语言指令数据，对中文模型进行了大规模指令跟随训练，实现了 Linly-ChatFlow 对话模型。
          https://github.com/CVI-SZU/Linly
      
      
          Firefly
          通用
          Firefly(流萤) 是一个开源的中文大语言模型项目，开源包括数据、微调代码、多个基于Bloom、baichuan等微调好的模型等；支持全量参数指令微调、QLoRA低成本高效指令微调、LoRA指令微调；支持绝大部分主流的开源大模型，如百川baichuan、Ziya、Bloom、LLaMA等。持lora与base model进行权重合并，推理更便捷。
          https://github.com/yangjianxin1/Firefly
      
      
          ChatYuan
          通用
          元语智能发布的一系列支持中英双语的功能型对话语言大模型，在微调数据、人类反馈强化学习、思维链等方面进行了优化。
          https://github.com/clue-ai/ChatYuan
      
      
          ChatRWKV
          通用
          开源了一系列基于RWKV架构的Chat模型（包括英文和中文），发布了包括Raven，Novel-ChnEng，Novel-Ch与Novel-ChnEng-ChnPro等模型，可以直接闲聊及进行诗歌，小说等创作，包括7B和14B等规模的模型。
          https://github.com/BlinkDL/ChatRWKV
      
      
          CPM-Bee
          通用
          一个完全开源、允许商用的百亿参数中英文基座模型。它采用Transformer自回归架构（auto-regressive），在超万亿（trillion）高质量语料上进行预训练，拥有强大的基础能力。开发者和研究者可以在CPM-Bee基座模型的基础上在各类场景进行适配来以创建特定领域的应用模型。
          https://github.com/OpenBMB/CPM-Bee
      
      
          TigerBot
          通用
          一个多语言多任务的大规模语言模型(LLM)，开源了包括模型：TigerBot-7B, TigerBot-7B-base，TigerBot-180B，基本训练和推理代码，100G预训练数据，涵盖金融、法律、百科的领域数据以及API等。
          https://github.com/TigerResearch/TigerBot
      
      
          书生·浦语
          通用
          商汤科技、上海AI实验室联合香港中文大学、复旦大学和上海交通大学发布千亿级参数大语言模型“书生·浦语”（InternLM）。据悉，“书生·浦语”具有1040亿参数，基于“包含1.6万亿token的多语种高质量数据集”训练而成。
          https://github.com/InternLM/InternLM-techreport
      
      
          Aquila
          通用
          由智源研究院发布，Aquila语言大模型在技术上继承了GPT-3、LLaMA等的架构设计优点，替换了一批更高效的底层算子实现、重新设计实现了中英双语的tokenizer，升级了BMTrain并行训练方法，是在中英文高质量语料基础上从０开始训练的，通过数据质量的控制、多种训练的优化方法，实现在更小的数据集、更短的训练时间，获得比其它开源模型更优的性能。也是首个支持中英双语知识、支持商用许可协议、符合国内数据合规需要的大规模开源语言模型。
          https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila
      
      
          Baichuan-7B
          通用
          Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。该项目发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。
          https://github.com/baichuan-inc/baichuan-7B
      
      
          Baichuan-13B
          通用
          由百川智能开发的一个开源可商用的大规模预训练语言模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。
          https://github.com/baichuan-inc/Baichuan-13B
      
      
          Baichuan2
          通用
          由百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练，在多个权威的中文、英文和多语言的通用、领域 benchmark上取得同尺寸最佳的效果，发布包含有7B、13B的Base和经过PPO训练的Chat版本，并提供了Chat版本的4bits量化。
          https://github.com/baichuan-inc/Baichuan2
      
      
          Anima
          通用
          由艾写科技开发的一个开源的基于QLoRA的33B中文大语言模型，该模型基于QLoRA的Guanaco 33B模型使用Chinese-Vicuna项目开放的训练数据集guanaco_belle_merge_v1.0进行finetune训练了10000个step，基于Elo rating tournament评估效果较好。
          https://github.com/lyogavin/Anima
      
      
          KnowLM
          通用
          KnowLM项目旨在发布开源大模型框架及相应模型权重以助力减轻知识谬误问题，包括大模型的知识难更新及存在潜在的错误和偏见等。该项目一期发布了基于Llama的抽取大模型智析，使用中英文语料对LLaMA（13B）进行进一步全量预训练，并基于知识图谱转换指令技术对知识抽取任务进行优化。
          https://github.com/zjunlp/KnowLM
      
      
          BayLing
          通用
          一个具有增强的跨语言对齐的通用大模型，由中国科学院计算技术研究所自然语言处理团队开发。百聆（BayLing）以LLaMA为基座模型，探索了以交互式翻译任务为核心进行指令微调的方法，旨在同时完成语言间对齐以及与人类意图对齐，将LLaMA的生成能力和指令跟随能力从英语迁移到其他语言（中文）。在多语言翻译、交互翻译、通用任务、标准化考试的测评中，百聆在中文/英语中均展现出更好的表现。百聆提供了在线的内测版demo，以供大家体验。
          https://github.com/ictnlp/BayLing
      
      
          YuLan-Chat
          通用
          YuLan-Chat是中国人民大学GSAI研究人员开发的基于聊天的大语言模型。它是在LLaMA的基础上微调开发的，具有高质量的英文和中文指令。 YuLan-Chat可以与用户聊天，很好地遵循英文或中文指令，并且可以在量化后部署在GPU（A800-80G或RTX3090）上。
          https://github.com/RUC-GSAI/YuLan-Chat
      
      
          PolyLM
          通用
          一个在6400亿个词的数据上从头训练的多语言语言模型，包括两种模型大小(1.7B和13B)。PolyLM覆盖中、英、俄、西、法、葡、德、意、荷、波、阿、土、希伯来、日、韩、泰、越、印尼等语种，特别是对亚洲语种更友好。
          https://github.com/DAMO-NLP-MT/PolyLM
      
      
          huozi
          通用
          由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048，同时还开源了基于RLHF训练的模型以及全人工标注的16.9K中文偏好数据集。
          https://github.com/HIT-SCIR/huozi
      
      
          YaYi
          通用
          雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，进一步提升了模型性能和安全性。已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。
          https://github.com/wenge-research/YaYi
      
      
          XVERSE-13B
          通用
          由深圳元象科技自主研发的支持多语言的大语言模型，使用主流 Decoder-only 的标准Transformer网络结构，支持 8K 的上下文长度（Context Length），为同尺寸模型中最长，构建了 1.4 万亿 token 的高质量、多样化的数据对模型进行充分训练，包含中、英、俄、西等 40 多种语言，通过精细化设置不同类型数据的采样比例，使得中英两种语言表现优异，也能兼顾其他语言效果；基于BPE算法使用上百GB 语料训练了一个词表大小为100,278的分词器，能够同时支持多语言，而无需额外扩展词表。
          https://github.com/xverse-ai/XVERSE-13B
      
      
          Skywork
          通用
          该项目开源了天工系列模型，该系列模型在3.2TB高质量多语言和代码数据上进行预训练，开源了包括模型参数，训练数据，评估数据，评估方法。具体包括Skywork-13B-Base模型、Skywork-13B-Chat模型、Skywork-13B-Math模型和Skywork-13B-MM模型，以及每个模型的量化版模型，以支持用户在消费级显卡进行部署和推理。
          https://github.com/SkyworkAI/Skywork
      
      
          Yi
          通用
          该项目开源了Yi-6B和Yi-34B等模型，该系列模型最长可支持200K的超长上下文窗口版本，可以处理约40万汉字超长文本输入，理解超过1000页的PDF文档。
          https://github.com/01-ai/Yi
      
      
          Yuan-2.0
          通用
          该项目开源了由浪潮信息发布的新一代基础语言大模型，具体开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且提供了预训练，微调，推理服务的相关脚本。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。
          https://github.com/IEIT-Yuan/Yuan-2.0
      
      
          AlpaCare
          专业
          该项目开源了医学大模型AlpaCare，在LLaMA上微调得到。
          https://github.com/XZhang97666/AlpaCare
      
      
          Zhongjing
          专业
          该项目开源了首个包含预训练、有监督微调和 RLHF 完整训练流程的中文医学大模型，展现出了很好的泛化能力，在某些对话场景中甚至接近专业医生的专业水平。此外，还开源了一个包含 70,000 条完全来源于真实医患对话的多轮对话数据集。该数据集包含大量医生主动提问的语句，有助于提升模型的主动医疗询问能力。
          https://github.com/SupritYoung/Zhongjing
      
      
          PMC-LLaMA
          专业
          该项目开源了医疗大模型PMC-LLaMA，包括预训练版本的MedLLaMA_13B和指令微调版本的PMC_LLaMA_13B。
          https://github.com/chaoyi-wu/PMC-LLaMA
      
      
          ChatDoctor
          专业
          该项目开源了医疗大模型ChatDoctor，在LLaMA的基础上训练得到。
          https://github.com/Kent0n-Li/ChatDoctor
      
      
          MING
          专业
          该项目开源了医疗大模型MING，基于bloomz-7b指令微调得到MING-7B，支持医疗问答、智能问诊等功能。
          https://github.com/189569400/MedicalGPT-zh
      
      
          IvyGPT
          专业
          该项目开源了医疗大模型IvyGPT，它在高质量的医学问答数据上进行了监督微调，并使用人类反馈的强化学习进行了训练。
          https://github.com/WangRongsheng/IvyGPT
      
      
          PULSE
          专业
          该项目开源了中文医疗大模型PULSE，该模型使用约4,000,000个中文医学领域和通用领域的指令微调数据进行微调，支持医学领域的各种自然语言处理任务，包括健康教育、医师考试问题、报告解读、医疗记录结构化以及模拟诊断和治疗。
          https://github.com/openmedlab/PULSE
      
      
          HuangDI
          专业
          该项目开源了中医大模型HuangDI (皇帝)，该模型首先在Ziya-LLaMA-13B-V1基座模型的基础上加入中医教材、中医各类网站数据等语料库，训练出一个具有中医知识理解力的预训练模型，之后在此基础上通过海量的中医古籍指令对话数据及通用指令数据进行有监督微调，使得模型具备中医古籍知识问答能力。
          https://github.com/Zlasejd/HuangDI
      
      
          CMLM-ZhongJing
          专业
          该项目开源了中医大模型ZhongJing (仲景)，该模型旨在阐明中医博大精深之知识，传承古代智慧与现代技术创新，最终为医学领域提供可信赖和专业的工具。
          https://github.com/pariskang/CMLM-ZhongJing
      
      
          TCMLLM
          专业
          该项目拟通过大模型方式实现中医临床辅助诊疗（病证诊断、处方推荐等）中医药知识问答等任务，推动中医知识问答、临床辅助诊疗等领域的快速发展。目前针对中医临床智能诊疗问题中的处方推荐任务，发布了中医处方推荐大模型TCMLLM-PR，通过整合真实世界临床病历、医学典籍与中医教科书等数据，构建了包含68k数据条目的处方推荐指令微调数据集，在ChatGLM大模型上进行微调得到。
          https://github.com/2020MEAI/TCMLLM
      
  
" /><meta name="keywords" content='Hugo, FixIt' />
  <meta itemprop="name" content="医学大模型榜单V2">
  <meta itemprop="description" content="模型名称 模型类型 支持语言 参数规模 发布机构 模型最后一次更新时间 简介 GitHub地址 Hugging Face 地址 论文地址 官网地址 其它信息来源 API 是否可用 API 使用地址 是否可以私有化部署 Demo 地址 上下文长度 训练用数据 训练基座 是否可商用 gpt-4-1106-preview 通用 多语言 1.8T (未证实) OpenAI 2023.11.6 GPT-4，或称Generative Pre-trained Transformer 4，是由OpenAI开发的最先进的语言模型。它的参数量大概是其前身GPT-3的10倍。这一进步使GPT-4能够以惊人的准确性和细微差别理解和生成类似人类的文本。它擅长各种任务，如回答问题、写文章、总结文本等等。它在不同数据集上进行大规模训练，能够深入理解多种语言和科目，使其成为人工智能驱动应用程序中的通用工具。 无 无 无 https://platform.openai.com/docs/overview 到处都有 付费，输入/输出 1k token 花费 0.01$/0.03$ https://platform.openai.com/docs/api-reference 否 https://chat.openai.com/ 128K 未公布 无 是 gpt-3.5-turbo-1106 通用 多语言 175B (未证实) OpenAI 2023.11.6 GPT-3.5是OpenAI开发的先进自然语言处理模型，作为GPT-3系列的改进版本。它继承了GPT-3的基本架构，拥有数以百亿计的参数，能够更有效地处理和生成自然语言文本。GPT-3.5在理解复杂语境、回答问题、撰写文章和文本摘要方面表现出色。通过广泛的数据训练，GPT-3.5能够处理多种语言和领域的内容，是人工智能应用中的多功能工具。 无 无 无 https://platform.openai.com/docs/overview 到处都有 付费，输入/输出 1k token 花费 0.001$/0.002$ https://platform.openai.com/docs/api-reference 否 https://chat.openai.com/ 16K 未公布 无 是 ChatGLM3-6B 通用 双语 6.2B 清华大学 2023.10 ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性：更强大的基础模型： ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略；更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景；更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、长文本对话模型 ChatGLM3-6B-32K。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。 https://github.com/THUDM/ChatGLM3 https://huggingface.co/THUDM/chatglm3-6b https://arxiv.org/pdf/2210.02414.pdf https://www.chatglm.cn/ 无 私有部署免费，官网调用收费 API 开发文档 是，32GB内存，13GB显存 无(可以本地安装) 8K/32K The pile； Wudaocorpora ChatGLM3-6B-Base 填写问卷后可以免费商用 BenTsao (本草) 专业 中文 同基座 哈尔滨工业大学 2023.8.3 开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese 无 https://arxiv.org/pdf/2304.06975.pdf 无 无 不可用 无 否 无 同基座 github仓库里提到了若干个微调用数据集，之后可以调研 活字1.0/Bloom-7B/Alpaca-Chinese-7B/LLaMA-7B 否 DoctorGLM 专业 双语 同基座 上海交通大学，复旦大学 2023.6 基于 ChatGLM-6B的中文问诊模型，通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署 https://github.com/xionghonglin/DoctorGLM 无 https://arxiv.org/pdf/2304.01097.pdf https://xionghonglin.github.io/DoctorGLM/ https://zhuanlan.zhihu.com/p/622649076 不可用 无 否 无 同基座 见仓库 ChatGLM-6B 不清楚 BianQue 专业 中文 同基座 华南理工大学 2023.6.6 一个经过指令与多轮问询对话联合微调的医疗对话大模型，基于ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。 https://github.com/scutcyr/BianQue https://huggingface.co/scutcyr/BianQue-2 https://arxiv.org/pdf/2310.15896.pdf 无 无 可以私有部署api https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py 是 无 同基座 我们结合当前开源的中文医疗问答数据集（MedDialog-CN、IMCS-V2、CHIP-MDCFNPC、MedDG、cMedQA2、Chinese-medical-dialogue-data），分析其中的单轮/多轮特性以及医生问询特性，结合实验室长期自建的生活空间健康对话大数据，构建了千万级别规模的扁鹊健康大数据BianQueCorpus。 ChatGLM-6B 否 HuatuoGPT-II 专业 双语 7B/13B/34B 深圳大数据研究院，香港中文大学(深圳) 2023.11.23 开源了经过中文医学指令精调/指令微调(Instruct-tuning)的一个GPT-like模型 https://github.com/FreedomIntelligence/HuatuoGPT-II 好多个，详见github https://arxiv.org/pdf/2311.09774.pdf 无 无 私有部署，自己开发 无 是 https://www.huatuogpt.cn/ 同基座 https://github.com/king-yyf/CMeKG_tools，还用ChatGPT构造了一些，和本草相同 Baichuan2-7B-Base/Baichuan2-13B-Base/Yi-34B 不清楚 Med-ChatGLM 专业 中文 同基座 哈尔滨工业大学 2023.3 基于中文医学知识的ChatGLM模型微调，微调数据与BenTsao相同。 https://github.com/SCIR-HI/Med-ChatGLM 无 无 无 无 或许可用 无 是，下载地址：https://pan.baidu.com/share/init?surl=Sfi1bRwV741GIChIEOUW0A&amp;pwd=i73e，https://drive.google.com/drive/folders/1ZQSN56DloRGQ-Qj7IwzY4jV3ZHKMe9Bc 无 同基座 https://github.com/king-yyf/CMeKG_tools，还用 ChatGLM-6B 否 QiZhenGPT 专业 中文 7B/6B/13B 浙江大学 2023.6.27 该项目利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。 https://github.com/CMKRG/QiZhenGPT 无 无 无 无 可以私有部署 无 是，见github 无 同基座 见仓库 Chinese-LLaMA-Plus-7B/CaMA-13B/ChatGLM-6B 否 ChatMed 专业 中文 7B（同基座模型Llama-7b） 华东师范大学 2023.5.5 该项目推出ChatMed系列中文医疗大规模语言模型，模型主干为LlaMA-7b并采用LoRA微调，具体包括ChatMed-Consult : 基于中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w&#43;在线问诊&#43;ChatGPT回复作为训练集；ChatMed-TCM : 基于中医药指令数据集ChatMed_TCM_Dataset，以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到2.6w&#43;的围绕中医药的指令数据训练得到。 https://github.com/michael-wzhu/ChatMed https://huggingface.co/michaelwzhu/ChatMed-Consult 无 无 无 可以私有部署api，需要下载模型后自己开发api 无 是，.bin文件80M 无 同基座 https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset Llama-7b 否 XrayGLM 专业 双语 6B，同基座 澳门理工大学 2023.5 该项目为促进中文领域医学多模态大模型的研究发展，发布了XrayGLM数据集及模型，其在医学影像诊断和多轮交互对话上显示出了非凡的潜力。 https://github.com/WangRongsheng/XrayGLM https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main 无 无 无 可以用webUI 详见github中webUI一部分 是 无 同基座 https://physionet.org/content/mimic-cxr-jpg/2.0.0/ https://openi.nlm.nih.gov/faq#collection VisualGLM-6B 否 MeChat 专业 中文 6B 浙江大学，西湖大学 2023.12 该项目开源的中文心理健康支持通用模型由 ChatGLM-6B LoRA 16-bit 指令微调得到。数据集通过调用gpt-3.5-turbo API扩展真实的心理互助 QA为多轮的心理健康支持多轮对话，提高了通用语言大模型在心理健康支持领域的表现，更加符合在长程多轮对话的应用场景。 https://github.com/qiuhuachuan/smile https://huggingface.co/qiuhuachuan/MeChat https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf 无 无 可以，有交互文件可运行 无 是 http://47.97.220.53:8080/ （似乎不能用了） 同基座 smileChat，见仓库 ChatGLM2-6B 不清楚 MedicalGPT 专业 双语 13B 个人（应该是公司里的，百度/腾讯） 2023.10.23 训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。发布中文医疗LoRA模型shibing624/ziya-llama-13b-medical-lora，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重。 https://github.com/shibing624/MedicalGPT https://huggingface.co/shibing624/vicuna-baichuan-13b-chat 无 无 无 可以，demo的api 详见github Demo 是 详见github Demo部分 同基座 240w条中文医疗数据：https://huggingface.co/datasets/shibing624/medical 22万条中文医疗对话数据集(华佗项目)：FreedomIntelligence/HuatuoGPT-sft-data-v1 BaiChuan-13B 不清楚 Sunsimiao 专业 中文 7B 华东理工大学 2023.6 Sunsimiao是一个开源的中文医疗大模型，该模型基于baichuan-7B和ChatGLM-6B底座模型在十万级高质量的中文医疗数据中微调而得。 https://github.com/thomas-yanxin/Sunsimiao https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary 无 无 无 有单纯的类似命令行输入输出，一段使用代码 无 是 无 同基座 没有提供10W中文医疗数据 BaiChuan-7B 不清楚 ShenNong-TCM-LLM 专业 中文 7B 华东师范大学 2023.6 该项目开源了ShenNong中医药大规模语言模型，该模型以LlaMA为底座，采用LoRA (rank=16)微调得到。微调代码与ChatMed代码库相同。此外该项目还开源了中医药指令微调数据集。 https://github.com/michael-wzhu/ShenNong-TCM-LLM 无 无 无 无 无 无 否 无 同基座 https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset 中医药知识图谱：https://github.com/ywjawmw/TCM_KG Llama-7B 否 SoulChat 专业 中文 6B 华南理工大学 2023.7 该项目开源了经过百万规模心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调的心理健康大模型灵心（SoulChat），该模型以ChatGLM-6B作为初始化模型，进行了全量参数的指令微调。 https://github.com/scutcyr/SoulChat 扁鹊：https://huggingface.co/scutcyr/BianQue-2 灵心：https://huggingface.co/scutcyr/SoulChat https://aclanthology.org/2023.findings-emnlp.83/ 无 无 可以，有Demo 详见github启动服务 是 提供soulchat_app.py代码 同基座 数据集即将发布 ChatGLM-6B 否 CareGPT 专业 双语 有多种，包括7b、13B、14B、20B，同基座 澳门理工大学 2023.8 该项目开源了数十个公开可用的医疗微调数据集和开放可用的医疗大语言模型，包含LLM的训练、测评、部署等以促进医疗LLM快速发展。 https://github.com/WangRongsheng/CareGPT https://huggingface.co/wangrongsheng/carellm/tree/main 无 无 无 可用 github上有介绍，在部署的部分 是 https://huggingface.co/spaces/wangrongsheng/CareLlama 同基座 在github数据部分 llama-7b、baichuan 是 DISC-MedLLM 专业 双语 13B 复旦大学 2023.8 该项目是由复旦大学发布的针对医疗健康对话式场景而设计的医疗领域大模型与数据集，该模型由DISC-Med-SFT数据集基于Baichuan-13B-Base指令微调得到。 https://github.com/FudanDISC/DISC-MedLLM https://huggingface.co/Flmc/DISC-MedLLM https://arxiv.org/abs/2308.14346 无 无 可用，cli_demo.py 详见github demo 是 web_demo.py 同基座 https://huggingface.co/datasets/Flmc/DISC-Med-SFT baichuan-13B-base 否 Taiyi-LLM 专业 双语 7BB 大连理工大学 2023.10 该项目由大连理工大学信息检索研究室开发的中英双语医学大模型&#34;太一&#34;，收集整理了丰富的中英双语生物医学自然语言处理（BioNLP）训练语料，总共包含38个中文数据集，通过丰富的中英双语任务指令数据（超过100W条样本）进行大模型（Qwen-7B-base）指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种BioNLP能力。 https://github.com/DUTIR-BioNLP/Taiyi-LLM https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM https://arxiv.org/abs/2311.11608 无 似乎可用，dialog开头的python文件 如前 是 https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/ 同基座 https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md Qwen-7B -base 否 下述模型仍然在信息更新中…… 模型名称 模型类型 简介 GitHub地址 WiNGPT 专业 WiNGPT是一个基于GPT的医疗垂直领域大模型，基于Qwen-7b1作为基础预训练模型，在此技术上进行了继续预训练，指令微调等，该项目具体开源了WiNGPT2-7B-Base与WiNGPT2-7B-Chat模型。 https://github.com/winninghealth/WiNGPT2 ChiMed-GPT 专业 ChiMed-GPT是一个开源中文医学大语言模型，通过在中文医学数据上持续训练 Ziya-v2 构建而成，其中涵盖了预训练、有监督微调 (SFT) 和来自人类反馈的强化学习 (RLHF) 等训练过程。 https://github.com/synlp/ChiMed-GPT ChatGLM 通用 中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持 https://github.com/THUDM/ChatGLM-6B ChatGLM2-6B 通用 基于开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练；基座模型的上下文长度扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练；基于 Multi-Query Attention 技术实现更高效的推理速度和更低的显存占用；允许商业使用。 https://github.com/THUDM/ChatGLM2-6B Chinese-LLaMA-Alpaca 通用 中文LLaMA&amp;Alpaca大语言模型&#43;本地CPU/GPU部署，在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练 https://github.com/ymcui/Chinese-LLaMA-Alpaca Chinese-LLaMA-Alpaca-2 通用 该项目将发布中文LLaMA-2 &amp; Alpaca-2大语言模型，基于可商用的LLaMA-2进行二次开发 https://github.com/ymcui/Chinese-LLaMA-Alpaca-2 Chinese-LlaMA2 通用 该项目基于可商用的LLaMA-2进行二次开发决定在次开展Llama 2的中文汉化工作，包括Chinese-LlaMA2: 对Llama 2进行中文预训练；第一步：先在42G中文预料上进行训练；后续将会加大训练规模；Chinese-LlaMA2-chat: 对Chinese-LlaMA2进行指令微调和多轮对话微调，以适应各种应用场景和多轮对话交互。同时我们也考虑更为快速的中文适配方案：Chinese-LlaMA2-sft-v0: 采用现有的开源中文指令微调或者是对话数据，对LlaMA-2进行直接微调 (将于近期开源)。 https://github.com/michael-wzhu/Chinese-LlaMA2 Llama2-Chinese 通用 该项目专注于Llama2模型在中文方面的优化和上层建设，基于大规模中文数据，从预训练开始对Llama2模型进行中文能力的持续迭代升级。 https://github.com/FlagAlpha/Llama2-Chinese Qwen 通用 通义千问 是阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。各个规模的模型包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。数据集包括文本和代码等多种数据类型，覆盖通用领域和专业领域，能支持8K的上下文长度，针对插件调用相关的对齐数据做了特定优化，当前模型能有效调用插件以及升级为Agent。 https://github.com/QwenLM/Qwen OpenChineseLLaMA 通用 基于 LLaMA-7B 经过中文数据集增量预训练产生的中文大语言模型基座，对比原版 LLaMA，该模型在中文理解能力和生成能力方面均获得较大提升，在众多下游任务中均取得了突出的成绩。 https://github.com/OpenLMLab/OpenChineseLLaMA BELLE 通用 开源了基于BLOOMZ和LLaMA优化后的一系列模型，同时包括训练数据、相关模型、训练代码、应用场景等，也会持续评估不同训练数据、训练算法等对模型表现的影响。 https://github.com/LianjiaTech/BELLE Panda 通用 开源了基于LLaMA-7B, -13B, -33B, -65B 进行中文领域上的持续预训练的语言模型, 使用了接近 15M 条数据进行二次预训练。 https://github.com/dandelionsllm/pandallm Robin (罗宾) 通用 Robin (罗宾)是香港科技大学LMFlow团队开发的中英双语大语言模型。仅使用180K条数据微调得到的Robin第二代模型，在Huggingface榜单上达到了第一名的成绩。LMFlow支持用户快速训练个性化模型，仅需单张3090和5个小时即可微调70亿参数定制化模型。 https://github.com/OptimalScale/LMFlow Fengshenbang-LM 通用 Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，该项目开源了姜子牙通用大模型V1，是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。除姜子牙系列模型之外，该项目还开源了太乙、二郎神系列等模型。 https://github.com/IDEA-CCNL/Fengshenbang-LM BiLLa 通用 该项目开源了推理能力增强的中英双语LLaMA模型。模型的主要特性有：较大提升LLaMA的中文理解能力，并尽可能减少对原始LLaMA英文能力的损伤；训练过程增加较多的任务型数据，利用ChatGPT生成解析，强化模型理解任务求解逻辑；全量参数更新，追求更好的生成效果。 https://github.com/Neutralzz/BiLLa Moss 通用 支持中英双语和多种插件的开源对话语言模型，MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。 https://github.com/OpenLMLab/MOSS Luotuo-Chinese-LLM 通用 囊括了一系列中文大语言模型开源项目，包含了一系列基于已有开源模型（ChatGLM, MOSS, LLaMA）进行二次微调的语言模型，指令微调数据集等。 https://github.com/LC1332/Luotuo-Chinese-LLM Linly 通用 提供中文对话模型 Linly-ChatFlow 、中文基础模型 Linly-Chinese-LLaMA 及其训练数据。 中文基础模型以 LLaMA 为底座，利用中文和中英平行增量预训练。项目汇总了目前公开的多语言指令数据，对中文模型进行了大规模指令跟随训练，实现了 Linly-ChatFlow 对话模型。 https://github.com/CVI-SZU/Linly Firefly 通用 Firefly(流萤) 是一个开源的中文大语言模型项目，开源包括数据、微调代码、多个基于Bloom、baichuan等微调好的模型等；支持全量参数指令微调、QLoRA低成本高效指令微调、LoRA指令微调；支持绝大部分主流的开源大模型，如百川baichuan、Ziya、Bloom、LLaMA等。持lora与base model进行权重合并，推理更便捷。 https://github.com/yangjianxin1/Firefly ChatYuan 通用 元语智能发布的一系列支持中英双语的功能型对话语言大模型，在微调数据、人类反馈强化学习、思维链等方面进行了优化。 https://github.com/clue-ai/ChatYuan ChatRWKV 通用 开源了一系列基于RWKV架构的Chat模型（包括英文和中文），发布了包括Raven，Novel-ChnEng，Novel-Ch与Novel-ChnEng-ChnPro等模型，可以直接闲聊及进行诗歌，小说等创作，包括7B和14B等规模的模型。 https://github.com/BlinkDL/ChatRWKV CPM-Bee 通用 一个完全开源、允许商用的百亿参数中英文基座模型。它采用Transformer自回归架构（auto-regressive），在超万亿（trillion）高质量语料上进行预训练，拥有强大的基础能力。开发者和研究者可以在CPM-Bee基座模型的基础上在各类场景进行适配来以创建特定领域的应用模型。 https://github.com/OpenBMB/CPM-Bee TigerBot 通用 一个多语言多任务的大规模语言模型(LLM)，开源了包括模型：TigerBot-7B, TigerBot-7B-base，TigerBot-180B，基本训练和推理代码，100G预训练数据，涵盖金融、法律、百科的领域数据以及API等。 https://github.com/TigerResearch/TigerBot 书生·浦语 通用 商汤科技、上海AI实验室联合香港中文大学、复旦大学和上海交通大学发布千亿级参数大语言模型“书生·浦语”（InternLM）。据悉，“书生·浦语”具有1040亿参数，基于“包含1.6万亿token的多语种高质量数据集”训练而成。 https://github.com/InternLM/InternLM-techreport Aquila 通用 由智源研究院发布，Aquila语言大模型在技术上继承了GPT-3、LLaMA等的架构设计优点，替换了一批更高效的底层算子实现、重新设计实现了中英双语的tokenizer，升级了BMTrain并行训练方法，是在中英文高质量语料基础上从０开始训练的，通过数据质量的控制、多种训练的优化方法，实现在更小的数据集、更短的训练时间，获得比其它开源模型更优的性能。也是首个支持中英双语知识、支持商用许可协议、符合国内数据合规需要的大规模开源语言模型。 https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila Baichuan-7B 通用 Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。该项目发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。 https://github.com/baichuan-inc/baichuan-7B Baichuan-13B 通用 由百川智能开发的一个开源可商用的大规模预训练语言模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。 https://github.com/baichuan-inc/Baichuan-13B Baichuan2 通用 由百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练，在多个权威的中文、英文和多语言的通用、领域 benchmark上取得同尺寸最佳的效果，发布包含有7B、13B的Base和经过PPO训练的Chat版本，并提供了Chat版本的4bits量化。 https://github.com/baichuan-inc/Baichuan2 Anima 通用 由艾写科技开发的一个开源的基于QLoRA的33B中文大语言模型，该模型基于QLoRA的Guanaco 33B模型使用Chinese-Vicuna项目开放的训练数据集guanaco_belle_merge_v1.0进行finetune训练了10000个step，基于Elo rating tournament评估效果较好。 https://github.com/lyogavin/Anima KnowLM 通用 KnowLM项目旨在发布开源大模型框架及相应模型权重以助力减轻知识谬误问题，包括大模型的知识难更新及存在潜在的错误和偏见等。该项目一期发布了基于Llama的抽取大模型智析，使用中英文语料对LLaMA（13B）进行进一步全量预训练，并基于知识图谱转换指令技术对知识抽取任务进行优化。 https://github.com/zjunlp/KnowLM BayLing 通用 一个具有增强的跨语言对齐的通用大模型，由中国科学院计算技术研究所自然语言处理团队开发。百聆（BayLing）以LLaMA为基座模型，探索了以交互式翻译任务为核心进行指令微调的方法，旨在同时完成语言间对齐以及与人类意图对齐，将LLaMA的生成能力和指令跟随能力从英语迁移到其他语言（中文）。在多语言翻译、交互翻译、通用任务、标准化考试的测评中，百聆在中文/英语中均展现出更好的表现。百聆提供了在线的内测版demo，以供大家体验。 https://github.com/ictnlp/BayLing YuLan-Chat 通用 YuLan-Chat是中国人民大学GSAI研究人员开发的基于聊天的大语言模型。它是在LLaMA的基础上微调开发的，具有高质量的英文和中文指令。 YuLan-Chat可以与用户聊天，很好地遵循英文或中文指令，并且可以在量化后部署在GPU（A800-80G或RTX3090）上。 https://github.com/RUC-GSAI/YuLan-Chat PolyLM 通用 一个在6400亿个词的数据上从头训练的多语言语言模型，包括两种模型大小(1.7B和13B)。PolyLM覆盖中、英、俄、西、法、葡、德、意、荷、波、阿、土、希伯来、日、韩、泰、越、印尼等语种，特别是对亚洲语种更友好。 https://github.com/DAMO-NLP-MT/PolyLM huozi 通用 由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048，同时还开源了基于RLHF训练的模型以及全人工标注的16.9K中文偏好数据集。 https://github.com/HIT-SCIR/huozi YaYi 通用 雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，进一步提升了模型性能和安全性。已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。 https://github.com/wenge-research/YaYi XVERSE-13B 通用 由深圳元象科技自主研发的支持多语言的大语言模型，使用主流 Decoder-only 的标准Transformer网络结构，支持 8K 的上下文长度（Context Length），为同尺寸模型中最长，构建了 1.4 万亿 token 的高质量、多样化的数据对模型进行充分训练，包含中、英、俄、西等 40 多种语言，通过精细化设置不同类型数据的采样比例，使得中英两种语言表现优异，也能兼顾其他语言效果；基于BPE算法使用上百GB 语料训练了一个词表大小为100,278的分词器，能够同时支持多语言，而无需额外扩展词表。 https://github.com/xverse-ai/XVERSE-13B Skywork 通用 该项目开源了天工系列模型，该系列模型在3.2TB高质量多语言和代码数据上进行预训练，开源了包括模型参数，训练数据，评估数据，评估方法。具体包括Skywork-13B-Base模型、Skywork-13B-Chat模型、Skywork-13B-Math模型和Skywork-13B-MM模型，以及每个模型的量化版模型，以支持用户在消费级显卡进行部署和推理。 https://github.com/SkyworkAI/Skywork Yi 通用 该项目开源了Yi-6B和Yi-34B等模型，该系列模型最长可支持200K的超长上下文窗口版本，可以处理约40万汉字超长文本输入，理解超过1000页的PDF文档。 https://github.com/01-ai/Yi Yuan-2.0 通用 该项目开源了由浪潮信息发布的新一代基础语言大模型，具体开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且提供了预训练，微调，推理服务的相关脚本。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。 https://github.com/IEIT-Yuan/Yuan-2.0 AlpaCare 专业 该项目开源了医学大模型AlpaCare，在LLaMA上微调得到。 https://github.com/XZhang97666/AlpaCare Zhongjing 专业 该项目开源了首个包含预训练、有监督微调和 RLHF 完整训练流程的中文医学大模型，展现出了很好的泛化能力，在某些对话场景中甚至接近专业医生的专业水平。此外，还开源了一个包含 70,000 条完全来源于真实医患对话的多轮对话数据集。该数据集包含大量医生主动提问的语句，有助于提升模型的主动医疗询问能力。 https://github.com/SupritYoung/Zhongjing PMC-LLaMA 专业 该项目开源了医疗大模型PMC-LLaMA，包括预训练版本的MedLLaMA_13B和指令微调版本的PMC_LLaMA_13B。 https://github.com/chaoyi-wu/PMC-LLaMA ChatDoctor 专业 该项目开源了医疗大模型ChatDoctor，在LLaMA的基础上训练得到。 https://github.com/Kent0n-Li/ChatDoctor MING 专业 该项目开源了医疗大模型MING，基于bloomz-7b指令微调得到MING-7B，支持医疗问答、智能问诊等功能。 https://github.com/189569400/MedicalGPT-zh IvyGPT 专业 该项目开源了医疗大模型IvyGPT，它在高质量的医学问答数据上进行了监督微调，并使用人类反馈的强化学习进行了训练。 https://github.com/WangRongsheng/IvyGPT PULSE 专业 该项目开源了中文医疗大模型PULSE，该模型使用约4,000,000个中文医学领域和通用领域的指令微调数据进行微调，支持医学领域的各种自然语言处理任务，包括健康教育、医师考试问题、报告解读、医疗记录结构化以及模拟诊断和治疗。 https://github.com/openmedlab/PULSE HuangDI 专业 该项目开源了中医大模型HuangDI (皇帝)，该模型首先在Ziya-LLaMA-13B-V1基座模型的基础上加入中医教材、中医各类网站数据等语料库，训练出一个具有中医知识理解力的预训练模型，之后在此基础上通过海量的中医古籍指令对话数据及通用指令数据进行有监督微调，使得模型具备中医古籍知识问答能力。 https://github.com/Zlasejd/HuangDI CMLM-ZhongJing 专业 该项目开源了中医大模型ZhongJing (仲景)，该模型旨在阐明中医博大精深之知识，传承古代智慧与现代技术创新，最终为医学领域提供可信赖和专业的工具。 https://github.com/pariskang/CMLM-ZhongJing TCMLLM 专业 该项目拟通过大模型方式实现中医临床辅助诊疗（病证诊断、处方推荐等）中医药知识问答等任务，推动中医知识问答、临床辅助诊疗等领域的快速发展。目前针对中医临床智能诊疗问题中的处方推荐任务，发布了中医处方推荐大模型TCMLLM-PR，通过整合真实世界临床病历、医学典籍与中医教科书等数据，构建了包含68k数据条目的处方推荐指令微调数据集，在ChatGLM大模型上进行微调得到。 https://github.com/2020MEAI/TCMLLM">
  <meta itemprop="datePublished" content="2024-01-03T12:18:27+08:00">
  <meta itemprop="dateModified" content="2024-01-03T12:18:27+08:00">
  <meta itemprop="wordCount" content="767"><meta property="og:url" content="https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95v2/">
  <meta property="og:site_name" content="Med-Eval">
  <meta property="og:title" content="医学大模型榜单V2">
  <meta property="og:description" content="模型名称 模型类型 支持语言 参数规模 发布机构 模型最后一次更新时间 简介 GitHub地址 Hugging Face 地址 论文地址 官网地址 其它信息来源 API 是否可用 API 使用地址 是否可以私有化部署 Demo 地址 上下文长度 训练用数据 训练基座 是否可商用 gpt-4-1106-preview 通用 多语言 1.8T (未证实) OpenAI 2023.11.6 GPT-4，或称Generative Pre-trained Transformer 4，是由OpenAI开发的最先进的语言模型。它的参数量大概是其前身GPT-3的10倍。这一进步使GPT-4能够以惊人的准确性和细微差别理解和生成类似人类的文本。它擅长各种任务，如回答问题、写文章、总结文本等等。它在不同数据集上进行大规模训练，能够深入理解多种语言和科目，使其成为人工智能驱动应用程序中的通用工具。 无 无 无 https://platform.openai.com/docs/overview 到处都有 付费，输入/输出 1k token 花费 0.01$/0.03$ https://platform.openai.com/docs/api-reference 否 https://chat.openai.com/ 128K 未公布 无 是 gpt-3.5-turbo-1106 通用 多语言 175B (未证实) OpenAI 2023.11.6 GPT-3.5是OpenAI开发的先进自然语言处理模型，作为GPT-3系列的改进版本。它继承了GPT-3的基本架构，拥有数以百亿计的参数，能够更有效地处理和生成自然语言文本。GPT-3.5在理解复杂语境、回答问题、撰写文章和文本摘要方面表现出色。通过广泛的数据训练，GPT-3.5能够处理多种语言和领域的内容，是人工智能应用中的多功能工具。 无 无 无 https://platform.openai.com/docs/overview 到处都有 付费，输入/输出 1k token 花费 0.001$/0.002$ https://platform.openai.com/docs/api-reference 否 https://chat.openai.com/ 16K 未公布 无 是 ChatGLM3-6B 通用 双语 6.2B 清华大学 2023.10 ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性：更强大的基础模型： ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略；更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景；更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、长文本对话模型 ChatGLM3-6B-32K。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。 https://github.com/THUDM/ChatGLM3 https://huggingface.co/THUDM/chatglm3-6b https://arxiv.org/pdf/2210.02414.pdf https://www.chatglm.cn/ 无 私有部署免费，官网调用收费 API 开发文档 是，32GB内存，13GB显存 无(可以本地安装) 8K/32K The pile； Wudaocorpora ChatGLM3-6B-Base 填写问卷后可以免费商用 BenTsao (本草) 专业 中文 同基座 哈尔滨工业大学 2023.8.3 开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese 无 https://arxiv.org/pdf/2304.06975.pdf 无 无 不可用 无 否 无 同基座 github仓库里提到了若干个微调用数据集，之后可以调研 活字1.0/Bloom-7B/Alpaca-Chinese-7B/LLaMA-7B 否 DoctorGLM 专业 双语 同基座 上海交通大学，复旦大学 2023.6 基于 ChatGLM-6B的中文问诊模型，通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署 https://github.com/xionghonglin/DoctorGLM 无 https://arxiv.org/pdf/2304.01097.pdf https://xionghonglin.github.io/DoctorGLM/ https://zhuanlan.zhihu.com/p/622649076 不可用 无 否 无 同基座 见仓库 ChatGLM-6B 不清楚 BianQue 专业 中文 同基座 华南理工大学 2023.6.6 一个经过指令与多轮问询对话联合微调的医疗对话大模型，基于ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。 https://github.com/scutcyr/BianQue https://huggingface.co/scutcyr/BianQue-2 https://arxiv.org/pdf/2310.15896.pdf 无 无 可以私有部署api https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py 是 无 同基座 我们结合当前开源的中文医疗问答数据集（MedDialog-CN、IMCS-V2、CHIP-MDCFNPC、MedDG、cMedQA2、Chinese-medical-dialogue-data），分析其中的单轮/多轮特性以及医生问询特性，结合实验室长期自建的生活空间健康对话大数据，构建了千万级别规模的扁鹊健康大数据BianQueCorpus。 ChatGLM-6B 否 HuatuoGPT-II 专业 双语 7B/13B/34B 深圳大数据研究院，香港中文大学(深圳) 2023.11.23 开源了经过中文医学指令精调/指令微调(Instruct-tuning)的一个GPT-like模型 https://github.com/FreedomIntelligence/HuatuoGPT-II 好多个，详见github https://arxiv.org/pdf/2311.09774.pdf 无 无 私有部署，自己开发 无 是 https://www.huatuogpt.cn/ 同基座 https://github.com/king-yyf/CMeKG_tools，还用ChatGPT构造了一些，和本草相同 Baichuan2-7B-Base/Baichuan2-13B-Base/Yi-34B 不清楚 Med-ChatGLM 专业 中文 同基座 哈尔滨工业大学 2023.3 基于中文医学知识的ChatGLM模型微调，微调数据与BenTsao相同。 https://github.com/SCIR-HI/Med-ChatGLM 无 无 无 无 或许可用 无 是，下载地址：https://pan.baidu.com/share/init?surl=Sfi1bRwV741GIChIEOUW0A&amp;pwd=i73e，https://drive.google.com/drive/folders/1ZQSN56DloRGQ-Qj7IwzY4jV3ZHKMe9Bc 无 同基座 https://github.com/king-yyf/CMeKG_tools，还用 ChatGLM-6B 否 QiZhenGPT 专业 中文 7B/6B/13B 浙江大学 2023.6.27 该项目利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。 https://github.com/CMKRG/QiZhenGPT 无 无 无 无 可以私有部署 无 是，见github 无 同基座 见仓库 Chinese-LLaMA-Plus-7B/CaMA-13B/ChatGLM-6B 否 ChatMed 专业 中文 7B（同基座模型Llama-7b） 华东师范大学 2023.5.5 该项目推出ChatMed系列中文医疗大规模语言模型，模型主干为LlaMA-7b并采用LoRA微调，具体包括ChatMed-Consult : 基于中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w&#43;在线问诊&#43;ChatGPT回复作为训练集；ChatMed-TCM : 基于中医药指令数据集ChatMed_TCM_Dataset，以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到2.6w&#43;的围绕中医药的指令数据训练得到。 https://github.com/michael-wzhu/ChatMed https://huggingface.co/michaelwzhu/ChatMed-Consult 无 无 无 可以私有部署api，需要下载模型后自己开发api 无 是，.bin文件80M 无 同基座 https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset Llama-7b 否 XrayGLM 专业 双语 6B，同基座 澳门理工大学 2023.5 该项目为促进中文领域医学多模态大模型的研究发展，发布了XrayGLM数据集及模型，其在医学影像诊断和多轮交互对话上显示出了非凡的潜力。 https://github.com/WangRongsheng/XrayGLM https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main 无 无 无 可以用webUI 详见github中webUI一部分 是 无 同基座 https://physionet.org/content/mimic-cxr-jpg/2.0.0/ https://openi.nlm.nih.gov/faq#collection VisualGLM-6B 否 MeChat 专业 中文 6B 浙江大学，西湖大学 2023.12 该项目开源的中文心理健康支持通用模型由 ChatGLM-6B LoRA 16-bit 指令微调得到。数据集通过调用gpt-3.5-turbo API扩展真实的心理互助 QA为多轮的心理健康支持多轮对话，提高了通用语言大模型在心理健康支持领域的表现，更加符合在长程多轮对话的应用场景。 https://github.com/qiuhuachuan/smile https://huggingface.co/qiuhuachuan/MeChat https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf 无 无 可以，有交互文件可运行 无 是 http://47.97.220.53:8080/ （似乎不能用了） 同基座 smileChat，见仓库 ChatGLM2-6B 不清楚 MedicalGPT 专业 双语 13B 个人（应该是公司里的，百度/腾讯） 2023.10.23 训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。发布中文医疗LoRA模型shibing624/ziya-llama-13b-medical-lora，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重。 https://github.com/shibing624/MedicalGPT https://huggingface.co/shibing624/vicuna-baichuan-13b-chat 无 无 无 可以，demo的api 详见github Demo 是 详见github Demo部分 同基座 240w条中文医疗数据：https://huggingface.co/datasets/shibing624/medical 22万条中文医疗对话数据集(华佗项目)：FreedomIntelligence/HuatuoGPT-sft-data-v1 BaiChuan-13B 不清楚 Sunsimiao 专业 中文 7B 华东理工大学 2023.6 Sunsimiao是一个开源的中文医疗大模型，该模型基于baichuan-7B和ChatGLM-6B底座模型在十万级高质量的中文医疗数据中微调而得。 https://github.com/thomas-yanxin/Sunsimiao https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary 无 无 无 有单纯的类似命令行输入输出，一段使用代码 无 是 无 同基座 没有提供10W中文医疗数据 BaiChuan-7B 不清楚 ShenNong-TCM-LLM 专业 中文 7B 华东师范大学 2023.6 该项目开源了ShenNong中医药大规模语言模型，该模型以LlaMA为底座，采用LoRA (rank=16)微调得到。微调代码与ChatMed代码库相同。此外该项目还开源了中医药指令微调数据集。 https://github.com/michael-wzhu/ShenNong-TCM-LLM 无 无 无 无 无 无 否 无 同基座 https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset 中医药知识图谱：https://github.com/ywjawmw/TCM_KG Llama-7B 否 SoulChat 专业 中文 6B 华南理工大学 2023.7 该项目开源了经过百万规模心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调的心理健康大模型灵心（SoulChat），该模型以ChatGLM-6B作为初始化模型，进行了全量参数的指令微调。 https://github.com/scutcyr/SoulChat 扁鹊：https://huggingface.co/scutcyr/BianQue-2 灵心：https://huggingface.co/scutcyr/SoulChat https://aclanthology.org/2023.findings-emnlp.83/ 无 无 可以，有Demo 详见github启动服务 是 提供soulchat_app.py代码 同基座 数据集即将发布 ChatGLM-6B 否 CareGPT 专业 双语 有多种，包括7b、13B、14B、20B，同基座 澳门理工大学 2023.8 该项目开源了数十个公开可用的医疗微调数据集和开放可用的医疗大语言模型，包含LLM的训练、测评、部署等以促进医疗LLM快速发展。 https://github.com/WangRongsheng/CareGPT https://huggingface.co/wangrongsheng/carellm/tree/main 无 无 无 可用 github上有介绍，在部署的部分 是 https://huggingface.co/spaces/wangrongsheng/CareLlama 同基座 在github数据部分 llama-7b、baichuan 是 DISC-MedLLM 专业 双语 13B 复旦大学 2023.8 该项目是由复旦大学发布的针对医疗健康对话式场景而设计的医疗领域大模型与数据集，该模型由DISC-Med-SFT数据集基于Baichuan-13B-Base指令微调得到。 https://github.com/FudanDISC/DISC-MedLLM https://huggingface.co/Flmc/DISC-MedLLM https://arxiv.org/abs/2308.14346 无 无 可用，cli_demo.py 详见github demo 是 web_demo.py 同基座 https://huggingface.co/datasets/Flmc/DISC-Med-SFT baichuan-13B-base 否 Taiyi-LLM 专业 双语 7BB 大连理工大学 2023.10 该项目由大连理工大学信息检索研究室开发的中英双语医学大模型&#34;太一&#34;，收集整理了丰富的中英双语生物医学自然语言处理（BioNLP）训练语料，总共包含38个中文数据集，通过丰富的中英双语任务指令数据（超过100W条样本）进行大模型（Qwen-7B-base）指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种BioNLP能力。 https://github.com/DUTIR-BioNLP/Taiyi-LLM https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM https://arxiv.org/abs/2311.11608 无 似乎可用，dialog开头的python文件 如前 是 https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/ 同基座 https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md Qwen-7B -base 否 下述模型仍然在信息更新中…… 模型名称 模型类型 简介 GitHub地址 WiNGPT 专业 WiNGPT是一个基于GPT的医疗垂直领域大模型，基于Qwen-7b1作为基础预训练模型，在此技术上进行了继续预训练，指令微调等，该项目具体开源了WiNGPT2-7B-Base与WiNGPT2-7B-Chat模型。 https://github.com/winninghealth/WiNGPT2 ChiMed-GPT 专业 ChiMed-GPT是一个开源中文医学大语言模型，通过在中文医学数据上持续训练 Ziya-v2 构建而成，其中涵盖了预训练、有监督微调 (SFT) 和来自人类反馈的强化学习 (RLHF) 等训练过程。 https://github.com/synlp/ChiMed-GPT ChatGLM 通用 中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持 https://github.com/THUDM/ChatGLM-6B ChatGLM2-6B 通用 基于开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练；基座模型的上下文长度扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练；基于 Multi-Query Attention 技术实现更高效的推理速度和更低的显存占用；允许商业使用。 https://github.com/THUDM/ChatGLM2-6B Chinese-LLaMA-Alpaca 通用 中文LLaMA&amp;Alpaca大语言模型&#43;本地CPU/GPU部署，在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练 https://github.com/ymcui/Chinese-LLaMA-Alpaca Chinese-LLaMA-Alpaca-2 通用 该项目将发布中文LLaMA-2 &amp; Alpaca-2大语言模型，基于可商用的LLaMA-2进行二次开发 https://github.com/ymcui/Chinese-LLaMA-Alpaca-2 Chinese-LlaMA2 通用 该项目基于可商用的LLaMA-2进行二次开发决定在次开展Llama 2的中文汉化工作，包括Chinese-LlaMA2: 对Llama 2进行中文预训练；第一步：先在42G中文预料上进行训练；后续将会加大训练规模；Chinese-LlaMA2-chat: 对Chinese-LlaMA2进行指令微调和多轮对话微调，以适应各种应用场景和多轮对话交互。同时我们也考虑更为快速的中文适配方案：Chinese-LlaMA2-sft-v0: 采用现有的开源中文指令微调或者是对话数据，对LlaMA-2进行直接微调 (将于近期开源)。 https://github.com/michael-wzhu/Chinese-LlaMA2 Llama2-Chinese 通用 该项目专注于Llama2模型在中文方面的优化和上层建设，基于大规模中文数据，从预训练开始对Llama2模型进行中文能力的持续迭代升级。 https://github.com/FlagAlpha/Llama2-Chinese Qwen 通用 通义千问 是阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。各个规模的模型包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。数据集包括文本和代码等多种数据类型，覆盖通用领域和专业领域，能支持8K的上下文长度，针对插件调用相关的对齐数据做了特定优化，当前模型能有效调用插件以及升级为Agent。 https://github.com/QwenLM/Qwen OpenChineseLLaMA 通用 基于 LLaMA-7B 经过中文数据集增量预训练产生的中文大语言模型基座，对比原版 LLaMA，该模型在中文理解能力和生成能力方面均获得较大提升，在众多下游任务中均取得了突出的成绩。 https://github.com/OpenLMLab/OpenChineseLLaMA BELLE 通用 开源了基于BLOOMZ和LLaMA优化后的一系列模型，同时包括训练数据、相关模型、训练代码、应用场景等，也会持续评估不同训练数据、训练算法等对模型表现的影响。 https://github.com/LianjiaTech/BELLE Panda 通用 开源了基于LLaMA-7B, -13B, -33B, -65B 进行中文领域上的持续预训练的语言模型, 使用了接近 15M 条数据进行二次预训练。 https://github.com/dandelionsllm/pandallm Robin (罗宾) 通用 Robin (罗宾)是香港科技大学LMFlow团队开发的中英双语大语言模型。仅使用180K条数据微调得到的Robin第二代模型，在Huggingface榜单上达到了第一名的成绩。LMFlow支持用户快速训练个性化模型，仅需单张3090和5个小时即可微调70亿参数定制化模型。 https://github.com/OptimalScale/LMFlow Fengshenbang-LM 通用 Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，该项目开源了姜子牙通用大模型V1，是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。除姜子牙系列模型之外，该项目还开源了太乙、二郎神系列等模型。 https://github.com/IDEA-CCNL/Fengshenbang-LM BiLLa 通用 该项目开源了推理能力增强的中英双语LLaMA模型。模型的主要特性有：较大提升LLaMA的中文理解能力，并尽可能减少对原始LLaMA英文能力的损伤；训练过程增加较多的任务型数据，利用ChatGPT生成解析，强化模型理解任务求解逻辑；全量参数更新，追求更好的生成效果。 https://github.com/Neutralzz/BiLLa Moss 通用 支持中英双语和多种插件的开源对话语言模型，MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。 https://github.com/OpenLMLab/MOSS Luotuo-Chinese-LLM 通用 囊括了一系列中文大语言模型开源项目，包含了一系列基于已有开源模型（ChatGLM, MOSS, LLaMA）进行二次微调的语言模型，指令微调数据集等。 https://github.com/LC1332/Luotuo-Chinese-LLM Linly 通用 提供中文对话模型 Linly-ChatFlow 、中文基础模型 Linly-Chinese-LLaMA 及其训练数据。 中文基础模型以 LLaMA 为底座，利用中文和中英平行增量预训练。项目汇总了目前公开的多语言指令数据，对中文模型进行了大规模指令跟随训练，实现了 Linly-ChatFlow 对话模型。 https://github.com/CVI-SZU/Linly Firefly 通用 Firefly(流萤) 是一个开源的中文大语言模型项目，开源包括数据、微调代码、多个基于Bloom、baichuan等微调好的模型等；支持全量参数指令微调、QLoRA低成本高效指令微调、LoRA指令微调；支持绝大部分主流的开源大模型，如百川baichuan、Ziya、Bloom、LLaMA等。持lora与base model进行权重合并，推理更便捷。 https://github.com/yangjianxin1/Firefly ChatYuan 通用 元语智能发布的一系列支持中英双语的功能型对话语言大模型，在微调数据、人类反馈强化学习、思维链等方面进行了优化。 https://github.com/clue-ai/ChatYuan ChatRWKV 通用 开源了一系列基于RWKV架构的Chat模型（包括英文和中文），发布了包括Raven，Novel-ChnEng，Novel-Ch与Novel-ChnEng-ChnPro等模型，可以直接闲聊及进行诗歌，小说等创作，包括7B和14B等规模的模型。 https://github.com/BlinkDL/ChatRWKV CPM-Bee 通用 一个完全开源、允许商用的百亿参数中英文基座模型。它采用Transformer自回归架构（auto-regressive），在超万亿（trillion）高质量语料上进行预训练，拥有强大的基础能力。开发者和研究者可以在CPM-Bee基座模型的基础上在各类场景进行适配来以创建特定领域的应用模型。 https://github.com/OpenBMB/CPM-Bee TigerBot 通用 一个多语言多任务的大规模语言模型(LLM)，开源了包括模型：TigerBot-7B, TigerBot-7B-base，TigerBot-180B，基本训练和推理代码，100G预训练数据，涵盖金融、法律、百科的领域数据以及API等。 https://github.com/TigerResearch/TigerBot 书生·浦语 通用 商汤科技、上海AI实验室联合香港中文大学、复旦大学和上海交通大学发布千亿级参数大语言模型“书生·浦语”（InternLM）。据悉，“书生·浦语”具有1040亿参数，基于“包含1.6万亿token的多语种高质量数据集”训练而成。 https://github.com/InternLM/InternLM-techreport Aquila 通用 由智源研究院发布，Aquila语言大模型在技术上继承了GPT-3、LLaMA等的架构设计优点，替换了一批更高效的底层算子实现、重新设计实现了中英双语的tokenizer，升级了BMTrain并行训练方法，是在中英文高质量语料基础上从０开始训练的，通过数据质量的控制、多种训练的优化方法，实现在更小的数据集、更短的训练时间，获得比其它开源模型更优的性能。也是首个支持中英双语知识、支持商用许可协议、符合国内数据合规需要的大规模开源语言模型。 https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila Baichuan-7B 通用 Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。该项目发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。 https://github.com/baichuan-inc/baichuan-7B Baichuan-13B 通用 由百川智能开发的一个开源可商用的大规模预训练语言模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。 https://github.com/baichuan-inc/Baichuan-13B Baichuan2 通用 由百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练，在多个权威的中文、英文和多语言的通用、领域 benchmark上取得同尺寸最佳的效果，发布包含有7B、13B的Base和经过PPO训练的Chat版本，并提供了Chat版本的4bits量化。 https://github.com/baichuan-inc/Baichuan2 Anima 通用 由艾写科技开发的一个开源的基于QLoRA的33B中文大语言模型，该模型基于QLoRA的Guanaco 33B模型使用Chinese-Vicuna项目开放的训练数据集guanaco_belle_merge_v1.0进行finetune训练了10000个step，基于Elo rating tournament评估效果较好。 https://github.com/lyogavin/Anima KnowLM 通用 KnowLM项目旨在发布开源大模型框架及相应模型权重以助力减轻知识谬误问题，包括大模型的知识难更新及存在潜在的错误和偏见等。该项目一期发布了基于Llama的抽取大模型智析，使用中英文语料对LLaMA（13B）进行进一步全量预训练，并基于知识图谱转换指令技术对知识抽取任务进行优化。 https://github.com/zjunlp/KnowLM BayLing 通用 一个具有增强的跨语言对齐的通用大模型，由中国科学院计算技术研究所自然语言处理团队开发。百聆（BayLing）以LLaMA为基座模型，探索了以交互式翻译任务为核心进行指令微调的方法，旨在同时完成语言间对齐以及与人类意图对齐，将LLaMA的生成能力和指令跟随能力从英语迁移到其他语言（中文）。在多语言翻译、交互翻译、通用任务、标准化考试的测评中，百聆在中文/英语中均展现出更好的表现。百聆提供了在线的内测版demo，以供大家体验。 https://github.com/ictnlp/BayLing YuLan-Chat 通用 YuLan-Chat是中国人民大学GSAI研究人员开发的基于聊天的大语言模型。它是在LLaMA的基础上微调开发的，具有高质量的英文和中文指令。 YuLan-Chat可以与用户聊天，很好地遵循英文或中文指令，并且可以在量化后部署在GPU（A800-80G或RTX3090）上。 https://github.com/RUC-GSAI/YuLan-Chat PolyLM 通用 一个在6400亿个词的数据上从头训练的多语言语言模型，包括两种模型大小(1.7B和13B)。PolyLM覆盖中、英、俄、西、法、葡、德、意、荷、波、阿、土、希伯来、日、韩、泰、越、印尼等语种，特别是对亚洲语种更友好。 https://github.com/DAMO-NLP-MT/PolyLM huozi 通用 由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048，同时还开源了基于RLHF训练的模型以及全人工标注的16.9K中文偏好数据集。 https://github.com/HIT-SCIR/huozi YaYi 通用 雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，进一步提升了模型性能和安全性。已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。 https://github.com/wenge-research/YaYi XVERSE-13B 通用 由深圳元象科技自主研发的支持多语言的大语言模型，使用主流 Decoder-only 的标准Transformer网络结构，支持 8K 的上下文长度（Context Length），为同尺寸模型中最长，构建了 1.4 万亿 token 的高质量、多样化的数据对模型进行充分训练，包含中、英、俄、西等 40 多种语言，通过精细化设置不同类型数据的采样比例，使得中英两种语言表现优异，也能兼顾其他语言效果；基于BPE算法使用上百GB 语料训练了一个词表大小为100,278的分词器，能够同时支持多语言，而无需额外扩展词表。 https://github.com/xverse-ai/XVERSE-13B Skywork 通用 该项目开源了天工系列模型，该系列模型在3.2TB高质量多语言和代码数据上进行预训练，开源了包括模型参数，训练数据，评估数据，评估方法。具体包括Skywork-13B-Base模型、Skywork-13B-Chat模型、Skywork-13B-Math模型和Skywork-13B-MM模型，以及每个模型的量化版模型，以支持用户在消费级显卡进行部署和推理。 https://github.com/SkyworkAI/Skywork Yi 通用 该项目开源了Yi-6B和Yi-34B等模型，该系列模型最长可支持200K的超长上下文窗口版本，可以处理约40万汉字超长文本输入，理解超过1000页的PDF文档。 https://github.com/01-ai/Yi Yuan-2.0 通用 该项目开源了由浪潮信息发布的新一代基础语言大模型，具体开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且提供了预训练，微调，推理服务的相关脚本。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。 https://github.com/IEIT-Yuan/Yuan-2.0 AlpaCare 专业 该项目开源了医学大模型AlpaCare，在LLaMA上微调得到。 https://github.com/XZhang97666/AlpaCare Zhongjing 专业 该项目开源了首个包含预训练、有监督微调和 RLHF 完整训练流程的中文医学大模型，展现出了很好的泛化能力，在某些对话场景中甚至接近专业医生的专业水平。此外，还开源了一个包含 70,000 条完全来源于真实医患对话的多轮对话数据集。该数据集包含大量医生主动提问的语句，有助于提升模型的主动医疗询问能力。 https://github.com/SupritYoung/Zhongjing PMC-LLaMA 专业 该项目开源了医疗大模型PMC-LLaMA，包括预训练版本的MedLLaMA_13B和指令微调版本的PMC_LLaMA_13B。 https://github.com/chaoyi-wu/PMC-LLaMA ChatDoctor 专业 该项目开源了医疗大模型ChatDoctor，在LLaMA的基础上训练得到。 https://github.com/Kent0n-Li/ChatDoctor MING 专业 该项目开源了医疗大模型MING，基于bloomz-7b指令微调得到MING-7B，支持医疗问答、智能问诊等功能。 https://github.com/189569400/MedicalGPT-zh IvyGPT 专业 该项目开源了医疗大模型IvyGPT，它在高质量的医学问答数据上进行了监督微调，并使用人类反馈的强化学习进行了训练。 https://github.com/WangRongsheng/IvyGPT PULSE 专业 该项目开源了中文医疗大模型PULSE，该模型使用约4,000,000个中文医学领域和通用领域的指令微调数据进行微调，支持医学领域的各种自然语言处理任务，包括健康教育、医师考试问题、报告解读、医疗记录结构化以及模拟诊断和治疗。 https://github.com/openmedlab/PULSE HuangDI 专业 该项目开源了中医大模型HuangDI (皇帝)，该模型首先在Ziya-LLaMA-13B-V1基座模型的基础上加入中医教材、中医各类网站数据等语料库，训练出一个具有中医知识理解力的预训练模型，之后在此基础上通过海量的中医古籍指令对话数据及通用指令数据进行有监督微调，使得模型具备中医古籍知识问答能力。 https://github.com/Zlasejd/HuangDI CMLM-ZhongJing 专业 该项目开源了中医大模型ZhongJing (仲景)，该模型旨在阐明中医博大精深之知识，传承古代智慧与现代技术创新，最终为医学领域提供可信赖和专业的工具。 https://github.com/pariskang/CMLM-ZhongJing TCMLLM 专业 该项目拟通过大模型方式实现中医临床辅助诊疗（病证诊断、处方推荐等）中医药知识问答等任务，推动中医知识问答、临床辅助诊疗等领域的快速发展。目前针对中医临床智能诊疗问题中的处方推荐任务，发布了中医处方推荐大模型TCMLLM-PR，通过整合真实世界临床病历、医学典籍与中医教科书等数据，构建了包含68k数据条目的处方推荐指令微调数据集，在ChatGLM大模型上进行微调得到。 https://github.com/2020MEAI/TCMLLM">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="model">
    <meta property="article:published_time" content="2024-01-03T12:18:27+08:00">
    <meta property="article:modified_time" content="2024-01-03T12:18:27+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="医学大模型榜单V2">
  <meta name="twitter:description" content="模型名称 模型类型 支持语言 参数规模 发布机构 模型最后一次更新时间 简介 GitHub地址 Hugging Face 地址 论文地址 官网地址 其它信息来源 API 是否可用 API 使用地址 是否可以私有化部署 Demo 地址 上下文长度 训练用数据 训练基座 是否可商用 gpt-4-1106-preview 通用 多语言 1.8T (未证实) OpenAI 2023.11.6 GPT-4，或称Generative Pre-trained Transformer 4，是由OpenAI开发的最先进的语言模型。它的参数量大概是其前身GPT-3的10倍。这一进步使GPT-4能够以惊人的准确性和细微差别理解和生成类似人类的文本。它擅长各种任务，如回答问题、写文章、总结文本等等。它在不同数据集上进行大规模训练，能够深入理解多种语言和科目，使其成为人工智能驱动应用程序中的通用工具。 无 无 无 https://platform.openai.com/docs/overview 到处都有 付费，输入/输出 1k token 花费 0.01$/0.03$ https://platform.openai.com/docs/api-reference 否 https://chat.openai.com/ 128K 未公布 无 是 gpt-3.5-turbo-1106 通用 多语言 175B (未证实) OpenAI 2023.11.6 GPT-3.5是OpenAI开发的先进自然语言处理模型，作为GPT-3系列的改进版本。它继承了GPT-3的基本架构，拥有数以百亿计的参数，能够更有效地处理和生成自然语言文本。GPT-3.5在理解复杂语境、回答问题、撰写文章和文本摘要方面表现出色。通过广泛的数据训练，GPT-3.5能够处理多种语言和领域的内容，是人工智能应用中的多功能工具。 无 无 无 https://platform.openai.com/docs/overview 到处都有 付费，输入/输出 1k token 花费 0.001$/0.002$ https://platform.openai.com/docs/api-reference 否 https://chat.openai.com/ 16K 未公布 无 是 ChatGLM3-6B 通用 双语 6.2B 清华大学 2023.10 ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性：更强大的基础模型： ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略；更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景；更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、长文本对话模型 ChatGLM3-6B-32K。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。 https://github.com/THUDM/ChatGLM3 https://huggingface.co/THUDM/chatglm3-6b https://arxiv.org/pdf/2210.02414.pdf https://www.chatglm.cn/ 无 私有部署免费，官网调用收费 API 开发文档 是，32GB内存，13GB显存 无(可以本地安装) 8K/32K The pile； Wudaocorpora ChatGLM3-6B-Base 填写问卷后可以免费商用 BenTsao (本草) 专业 中文 同基座 哈尔滨工业大学 2023.8.3 开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese 无 https://arxiv.org/pdf/2304.06975.pdf 无 无 不可用 无 否 无 同基座 github仓库里提到了若干个微调用数据集，之后可以调研 活字1.0/Bloom-7B/Alpaca-Chinese-7B/LLaMA-7B 否 DoctorGLM 专业 双语 同基座 上海交通大学，复旦大学 2023.6 基于 ChatGLM-6B的中文问诊模型，通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署 https://github.com/xionghonglin/DoctorGLM 无 https://arxiv.org/pdf/2304.01097.pdf https://xionghonglin.github.io/DoctorGLM/ https://zhuanlan.zhihu.com/p/622649076 不可用 无 否 无 同基座 见仓库 ChatGLM-6B 不清楚 BianQue 专业 中文 同基座 华南理工大学 2023.6.6 一个经过指令与多轮问询对话联合微调的医疗对话大模型，基于ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。 https://github.com/scutcyr/BianQue https://huggingface.co/scutcyr/BianQue-2 https://arxiv.org/pdf/2310.15896.pdf 无 无 可以私有部署api https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py 是 无 同基座 我们结合当前开源的中文医疗问答数据集（MedDialog-CN、IMCS-V2、CHIP-MDCFNPC、MedDG、cMedQA2、Chinese-medical-dialogue-data），分析其中的单轮/多轮特性以及医生问询特性，结合实验室长期自建的生活空间健康对话大数据，构建了千万级别规模的扁鹊健康大数据BianQueCorpus。 ChatGLM-6B 否 HuatuoGPT-II 专业 双语 7B/13B/34B 深圳大数据研究院，香港中文大学(深圳) 2023.11.23 开源了经过中文医学指令精调/指令微调(Instruct-tuning)的一个GPT-like模型 https://github.com/FreedomIntelligence/HuatuoGPT-II 好多个，详见github https://arxiv.org/pdf/2311.09774.pdf 无 无 私有部署，自己开发 无 是 https://www.huatuogpt.cn/ 同基座 https://github.com/king-yyf/CMeKG_tools，还用ChatGPT构造了一些，和本草相同 Baichuan2-7B-Base/Baichuan2-13B-Base/Yi-34B 不清楚 Med-ChatGLM 专业 中文 同基座 哈尔滨工业大学 2023.3 基于中文医学知识的ChatGLM模型微调，微调数据与BenTsao相同。 https://github.com/SCIR-HI/Med-ChatGLM 无 无 无 无 或许可用 无 是，下载地址：https://pan.baidu.com/share/init?surl=Sfi1bRwV741GIChIEOUW0A&amp;pwd=i73e，https://drive.google.com/drive/folders/1ZQSN56DloRGQ-Qj7IwzY4jV3ZHKMe9Bc 无 同基座 https://github.com/king-yyf/CMeKG_tools，还用 ChatGLM-6B 否 QiZhenGPT 专业 中文 7B/6B/13B 浙江大学 2023.6.27 该项目利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。 https://github.com/CMKRG/QiZhenGPT 无 无 无 无 可以私有部署 无 是，见github 无 同基座 见仓库 Chinese-LLaMA-Plus-7B/CaMA-13B/ChatGLM-6B 否 ChatMed 专业 中文 7B（同基座模型Llama-7b） 华东师范大学 2023.5.5 该项目推出ChatMed系列中文医疗大规模语言模型，模型主干为LlaMA-7b并采用LoRA微调，具体包括ChatMed-Consult : 基于中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w&#43;在线问诊&#43;ChatGPT回复作为训练集；ChatMed-TCM : 基于中医药指令数据集ChatMed_TCM_Dataset，以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到2.6w&#43;的围绕中医药的指令数据训练得到。 https://github.com/michael-wzhu/ChatMed https://huggingface.co/michaelwzhu/ChatMed-Consult 无 无 无 可以私有部署api，需要下载模型后自己开发api 无 是，.bin文件80M 无 同基座 https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset Llama-7b 否 XrayGLM 专业 双语 6B，同基座 澳门理工大学 2023.5 该项目为促进中文领域医学多模态大模型的研究发展，发布了XrayGLM数据集及模型，其在医学影像诊断和多轮交互对话上显示出了非凡的潜力。 https://github.com/WangRongsheng/XrayGLM https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main 无 无 无 可以用webUI 详见github中webUI一部分 是 无 同基座 https://physionet.org/content/mimic-cxr-jpg/2.0.0/ https://openi.nlm.nih.gov/faq#collection VisualGLM-6B 否 MeChat 专业 中文 6B 浙江大学，西湖大学 2023.12 该项目开源的中文心理健康支持通用模型由 ChatGLM-6B LoRA 16-bit 指令微调得到。数据集通过调用gpt-3.5-turbo API扩展真实的心理互助 QA为多轮的心理健康支持多轮对话，提高了通用语言大模型在心理健康支持领域的表现，更加符合在长程多轮对话的应用场景。 https://github.com/qiuhuachuan/smile https://huggingface.co/qiuhuachuan/MeChat https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf 无 无 可以，有交互文件可运行 无 是 http://47.97.220.53:8080/ （似乎不能用了） 同基座 smileChat，见仓库 ChatGLM2-6B 不清楚 MedicalGPT 专业 双语 13B 个人（应该是公司里的，百度/腾讯） 2023.10.23 训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。发布中文医疗LoRA模型shibing624/ziya-llama-13b-medical-lora，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重。 https://github.com/shibing624/MedicalGPT https://huggingface.co/shibing624/vicuna-baichuan-13b-chat 无 无 无 可以，demo的api 详见github Demo 是 详见github Demo部分 同基座 240w条中文医疗数据：https://huggingface.co/datasets/shibing624/medical 22万条中文医疗对话数据集(华佗项目)：FreedomIntelligence/HuatuoGPT-sft-data-v1 BaiChuan-13B 不清楚 Sunsimiao 专业 中文 7B 华东理工大学 2023.6 Sunsimiao是一个开源的中文医疗大模型，该模型基于baichuan-7B和ChatGLM-6B底座模型在十万级高质量的中文医疗数据中微调而得。 https://github.com/thomas-yanxin/Sunsimiao https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary 无 无 无 有单纯的类似命令行输入输出，一段使用代码 无 是 无 同基座 没有提供10W中文医疗数据 BaiChuan-7B 不清楚 ShenNong-TCM-LLM 专业 中文 7B 华东师范大学 2023.6 该项目开源了ShenNong中医药大规模语言模型，该模型以LlaMA为底座，采用LoRA (rank=16)微调得到。微调代码与ChatMed代码库相同。此外该项目还开源了中医药指令微调数据集。 https://github.com/michael-wzhu/ShenNong-TCM-LLM 无 无 无 无 无 无 否 无 同基座 https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset 中医药知识图谱：https://github.com/ywjawmw/TCM_KG Llama-7B 否 SoulChat 专业 中文 6B 华南理工大学 2023.7 该项目开源了经过百万规模心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调的心理健康大模型灵心（SoulChat），该模型以ChatGLM-6B作为初始化模型，进行了全量参数的指令微调。 https://github.com/scutcyr/SoulChat 扁鹊：https://huggingface.co/scutcyr/BianQue-2 灵心：https://huggingface.co/scutcyr/SoulChat https://aclanthology.org/2023.findings-emnlp.83/ 无 无 可以，有Demo 详见github启动服务 是 提供soulchat_app.py代码 同基座 数据集即将发布 ChatGLM-6B 否 CareGPT 专业 双语 有多种，包括7b、13B、14B、20B，同基座 澳门理工大学 2023.8 该项目开源了数十个公开可用的医疗微调数据集和开放可用的医疗大语言模型，包含LLM的训练、测评、部署等以促进医疗LLM快速发展。 https://github.com/WangRongsheng/CareGPT https://huggingface.co/wangrongsheng/carellm/tree/main 无 无 无 可用 github上有介绍，在部署的部分 是 https://huggingface.co/spaces/wangrongsheng/CareLlama 同基座 在github数据部分 llama-7b、baichuan 是 DISC-MedLLM 专业 双语 13B 复旦大学 2023.8 该项目是由复旦大学发布的针对医疗健康对话式场景而设计的医疗领域大模型与数据集，该模型由DISC-Med-SFT数据集基于Baichuan-13B-Base指令微调得到。 https://github.com/FudanDISC/DISC-MedLLM https://huggingface.co/Flmc/DISC-MedLLM https://arxiv.org/abs/2308.14346 无 无 可用，cli_demo.py 详见github demo 是 web_demo.py 同基座 https://huggingface.co/datasets/Flmc/DISC-Med-SFT baichuan-13B-base 否 Taiyi-LLM 专业 双语 7BB 大连理工大学 2023.10 该项目由大连理工大学信息检索研究室开发的中英双语医学大模型&#34;太一&#34;，收集整理了丰富的中英双语生物医学自然语言处理（BioNLP）训练语料，总共包含38个中文数据集，通过丰富的中英双语任务指令数据（超过100W条样本）进行大模型（Qwen-7B-base）指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种BioNLP能力。 https://github.com/DUTIR-BioNLP/Taiyi-LLM https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM https://arxiv.org/abs/2311.11608 无 似乎可用，dialog开头的python文件 如前 是 https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/ 同基座 https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md Qwen-7B -base 否 下述模型仍然在信息更新中…… 模型名称 模型类型 简介 GitHub地址 WiNGPT 专业 WiNGPT是一个基于GPT的医疗垂直领域大模型，基于Qwen-7b1作为基础预训练模型，在此技术上进行了继续预训练，指令微调等，该项目具体开源了WiNGPT2-7B-Base与WiNGPT2-7B-Chat模型。 https://github.com/winninghealth/WiNGPT2 ChiMed-GPT 专业 ChiMed-GPT是一个开源中文医学大语言模型，通过在中文医学数据上持续训练 Ziya-v2 构建而成，其中涵盖了预训练、有监督微调 (SFT) 和来自人类反馈的强化学习 (RLHF) 等训练过程。 https://github.com/synlp/ChiMed-GPT ChatGLM 通用 中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持 https://github.com/THUDM/ChatGLM-6B ChatGLM2-6B 通用 基于开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练；基座模型的上下文长度扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练；基于 Multi-Query Attention 技术实现更高效的推理速度和更低的显存占用；允许商业使用。 https://github.com/THUDM/ChatGLM2-6B Chinese-LLaMA-Alpaca 通用 中文LLaMA&amp;Alpaca大语言模型&#43;本地CPU/GPU部署，在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练 https://github.com/ymcui/Chinese-LLaMA-Alpaca Chinese-LLaMA-Alpaca-2 通用 该项目将发布中文LLaMA-2 &amp; Alpaca-2大语言模型，基于可商用的LLaMA-2进行二次开发 https://github.com/ymcui/Chinese-LLaMA-Alpaca-2 Chinese-LlaMA2 通用 该项目基于可商用的LLaMA-2进行二次开发决定在次开展Llama 2的中文汉化工作，包括Chinese-LlaMA2: 对Llama 2进行中文预训练；第一步：先在42G中文预料上进行训练；后续将会加大训练规模；Chinese-LlaMA2-chat: 对Chinese-LlaMA2进行指令微调和多轮对话微调，以适应各种应用场景和多轮对话交互。同时我们也考虑更为快速的中文适配方案：Chinese-LlaMA2-sft-v0: 采用现有的开源中文指令微调或者是对话数据，对LlaMA-2进行直接微调 (将于近期开源)。 https://github.com/michael-wzhu/Chinese-LlaMA2 Llama2-Chinese 通用 该项目专注于Llama2模型在中文方面的优化和上层建设，基于大规模中文数据，从预训练开始对Llama2模型进行中文能力的持续迭代升级。 https://github.com/FlagAlpha/Llama2-Chinese Qwen 通用 通义千问 是阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。各个规模的模型包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。数据集包括文本和代码等多种数据类型，覆盖通用领域和专业领域，能支持8K的上下文长度，针对插件调用相关的对齐数据做了特定优化，当前模型能有效调用插件以及升级为Agent。 https://github.com/QwenLM/Qwen OpenChineseLLaMA 通用 基于 LLaMA-7B 经过中文数据集增量预训练产生的中文大语言模型基座，对比原版 LLaMA，该模型在中文理解能力和生成能力方面均获得较大提升，在众多下游任务中均取得了突出的成绩。 https://github.com/OpenLMLab/OpenChineseLLaMA BELLE 通用 开源了基于BLOOMZ和LLaMA优化后的一系列模型，同时包括训练数据、相关模型、训练代码、应用场景等，也会持续评估不同训练数据、训练算法等对模型表现的影响。 https://github.com/LianjiaTech/BELLE Panda 通用 开源了基于LLaMA-7B, -13B, -33B, -65B 进行中文领域上的持续预训练的语言模型, 使用了接近 15M 条数据进行二次预训练。 https://github.com/dandelionsllm/pandallm Robin (罗宾) 通用 Robin (罗宾)是香港科技大学LMFlow团队开发的中英双语大语言模型。仅使用180K条数据微调得到的Robin第二代模型，在Huggingface榜单上达到了第一名的成绩。LMFlow支持用户快速训练个性化模型，仅需单张3090和5个小时即可微调70亿参数定制化模型。 https://github.com/OptimalScale/LMFlow Fengshenbang-LM 通用 Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，该项目开源了姜子牙通用大模型V1，是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。除姜子牙系列模型之外，该项目还开源了太乙、二郎神系列等模型。 https://github.com/IDEA-CCNL/Fengshenbang-LM BiLLa 通用 该项目开源了推理能力增强的中英双语LLaMA模型。模型的主要特性有：较大提升LLaMA的中文理解能力，并尽可能减少对原始LLaMA英文能力的损伤；训练过程增加较多的任务型数据，利用ChatGPT生成解析，强化模型理解任务求解逻辑；全量参数更新，追求更好的生成效果。 https://github.com/Neutralzz/BiLLa Moss 通用 支持中英双语和多种插件的开源对话语言模型，MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。 https://github.com/OpenLMLab/MOSS Luotuo-Chinese-LLM 通用 囊括了一系列中文大语言模型开源项目，包含了一系列基于已有开源模型（ChatGLM, MOSS, LLaMA）进行二次微调的语言模型，指令微调数据集等。 https://github.com/LC1332/Luotuo-Chinese-LLM Linly 通用 提供中文对话模型 Linly-ChatFlow 、中文基础模型 Linly-Chinese-LLaMA 及其训练数据。 中文基础模型以 LLaMA 为底座，利用中文和中英平行增量预训练。项目汇总了目前公开的多语言指令数据，对中文模型进行了大规模指令跟随训练，实现了 Linly-ChatFlow 对话模型。 https://github.com/CVI-SZU/Linly Firefly 通用 Firefly(流萤) 是一个开源的中文大语言模型项目，开源包括数据、微调代码、多个基于Bloom、baichuan等微调好的模型等；支持全量参数指令微调、QLoRA低成本高效指令微调、LoRA指令微调；支持绝大部分主流的开源大模型，如百川baichuan、Ziya、Bloom、LLaMA等。持lora与base model进行权重合并，推理更便捷。 https://github.com/yangjianxin1/Firefly ChatYuan 通用 元语智能发布的一系列支持中英双语的功能型对话语言大模型，在微调数据、人类反馈强化学习、思维链等方面进行了优化。 https://github.com/clue-ai/ChatYuan ChatRWKV 通用 开源了一系列基于RWKV架构的Chat模型（包括英文和中文），发布了包括Raven，Novel-ChnEng，Novel-Ch与Novel-ChnEng-ChnPro等模型，可以直接闲聊及进行诗歌，小说等创作，包括7B和14B等规模的模型。 https://github.com/BlinkDL/ChatRWKV CPM-Bee 通用 一个完全开源、允许商用的百亿参数中英文基座模型。它采用Transformer自回归架构（auto-regressive），在超万亿（trillion）高质量语料上进行预训练，拥有强大的基础能力。开发者和研究者可以在CPM-Bee基座模型的基础上在各类场景进行适配来以创建特定领域的应用模型。 https://github.com/OpenBMB/CPM-Bee TigerBot 通用 一个多语言多任务的大规模语言模型(LLM)，开源了包括模型：TigerBot-7B, TigerBot-7B-base，TigerBot-180B，基本训练和推理代码，100G预训练数据，涵盖金融、法律、百科的领域数据以及API等。 https://github.com/TigerResearch/TigerBot 书生·浦语 通用 商汤科技、上海AI实验室联合香港中文大学、复旦大学和上海交通大学发布千亿级参数大语言模型“书生·浦语”（InternLM）。据悉，“书生·浦语”具有1040亿参数，基于“包含1.6万亿token的多语种高质量数据集”训练而成。 https://github.com/InternLM/InternLM-techreport Aquila 通用 由智源研究院发布，Aquila语言大模型在技术上继承了GPT-3、LLaMA等的架构设计优点，替换了一批更高效的底层算子实现、重新设计实现了中英双语的tokenizer，升级了BMTrain并行训练方法，是在中英文高质量语料基础上从０开始训练的，通过数据质量的控制、多种训练的优化方法，实现在更小的数据集、更短的训练时间，获得比其它开源模型更优的性能。也是首个支持中英双语知识、支持商用许可协议、符合国内数据合规需要的大规模开源语言模型。 https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila Baichuan-7B 通用 Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。该项目发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。 https://github.com/baichuan-inc/baichuan-7B Baichuan-13B 通用 由百川智能开发的一个开源可商用的大规模预训练语言模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。 https://github.com/baichuan-inc/Baichuan-13B Baichuan2 通用 由百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练，在多个权威的中文、英文和多语言的通用、领域 benchmark上取得同尺寸最佳的效果，发布包含有7B、13B的Base和经过PPO训练的Chat版本，并提供了Chat版本的4bits量化。 https://github.com/baichuan-inc/Baichuan2 Anima 通用 由艾写科技开发的一个开源的基于QLoRA的33B中文大语言模型，该模型基于QLoRA的Guanaco 33B模型使用Chinese-Vicuna项目开放的训练数据集guanaco_belle_merge_v1.0进行finetune训练了10000个step，基于Elo rating tournament评估效果较好。 https://github.com/lyogavin/Anima KnowLM 通用 KnowLM项目旨在发布开源大模型框架及相应模型权重以助力减轻知识谬误问题，包括大模型的知识难更新及存在潜在的错误和偏见等。该项目一期发布了基于Llama的抽取大模型智析，使用中英文语料对LLaMA（13B）进行进一步全量预训练，并基于知识图谱转换指令技术对知识抽取任务进行优化。 https://github.com/zjunlp/KnowLM BayLing 通用 一个具有增强的跨语言对齐的通用大模型，由中国科学院计算技术研究所自然语言处理团队开发。百聆（BayLing）以LLaMA为基座模型，探索了以交互式翻译任务为核心进行指令微调的方法，旨在同时完成语言间对齐以及与人类意图对齐，将LLaMA的生成能力和指令跟随能力从英语迁移到其他语言（中文）。在多语言翻译、交互翻译、通用任务、标准化考试的测评中，百聆在中文/英语中均展现出更好的表现。百聆提供了在线的内测版demo，以供大家体验。 https://github.com/ictnlp/BayLing YuLan-Chat 通用 YuLan-Chat是中国人民大学GSAI研究人员开发的基于聊天的大语言模型。它是在LLaMA的基础上微调开发的，具有高质量的英文和中文指令。 YuLan-Chat可以与用户聊天，很好地遵循英文或中文指令，并且可以在量化后部署在GPU（A800-80G或RTX3090）上。 https://github.com/RUC-GSAI/YuLan-Chat PolyLM 通用 一个在6400亿个词的数据上从头训练的多语言语言模型，包括两种模型大小(1.7B和13B)。PolyLM覆盖中、英、俄、西、法、葡、德、意、荷、波、阿、土、希伯来、日、韩、泰、越、印尼等语种，特别是对亚洲语种更友好。 https://github.com/DAMO-NLP-MT/PolyLM huozi 通用 由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048，同时还开源了基于RLHF训练的模型以及全人工标注的16.9K中文偏好数据集。 https://github.com/HIT-SCIR/huozi YaYi 通用 雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，进一步提升了模型性能和安全性。已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。 https://github.com/wenge-research/YaYi XVERSE-13B 通用 由深圳元象科技自主研发的支持多语言的大语言模型，使用主流 Decoder-only 的标准Transformer网络结构，支持 8K 的上下文长度（Context Length），为同尺寸模型中最长，构建了 1.4 万亿 token 的高质量、多样化的数据对模型进行充分训练，包含中、英、俄、西等 40 多种语言，通过精细化设置不同类型数据的采样比例，使得中英两种语言表现优异，也能兼顾其他语言效果；基于BPE算法使用上百GB 语料训练了一个词表大小为100,278的分词器，能够同时支持多语言，而无需额外扩展词表。 https://github.com/xverse-ai/XVERSE-13B Skywork 通用 该项目开源了天工系列模型，该系列模型在3.2TB高质量多语言和代码数据上进行预训练，开源了包括模型参数，训练数据，评估数据，评估方法。具体包括Skywork-13B-Base模型、Skywork-13B-Chat模型、Skywork-13B-Math模型和Skywork-13B-MM模型，以及每个模型的量化版模型，以支持用户在消费级显卡进行部署和推理。 https://github.com/SkyworkAI/Skywork Yi 通用 该项目开源了Yi-6B和Yi-34B等模型，该系列模型最长可支持200K的超长上下文窗口版本，可以处理约40万汉字超长文本输入，理解超过1000页的PDF文档。 https://github.com/01-ai/Yi Yuan-2.0 通用 该项目开源了由浪潮信息发布的新一代基础语言大模型，具体开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且提供了预训练，微调，推理服务的相关脚本。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。 https://github.com/IEIT-Yuan/Yuan-2.0 AlpaCare 专业 该项目开源了医学大模型AlpaCare，在LLaMA上微调得到。 https://github.com/XZhang97666/AlpaCare Zhongjing 专业 该项目开源了首个包含预训练、有监督微调和 RLHF 完整训练流程的中文医学大模型，展现出了很好的泛化能力，在某些对话场景中甚至接近专业医生的专业水平。此外，还开源了一个包含 70,000 条完全来源于真实医患对话的多轮对话数据集。该数据集包含大量医生主动提问的语句，有助于提升模型的主动医疗询问能力。 https://github.com/SupritYoung/Zhongjing PMC-LLaMA 专业 该项目开源了医疗大模型PMC-LLaMA，包括预训练版本的MedLLaMA_13B和指令微调版本的PMC_LLaMA_13B。 https://github.com/chaoyi-wu/PMC-LLaMA ChatDoctor 专业 该项目开源了医疗大模型ChatDoctor，在LLaMA的基础上训练得到。 https://github.com/Kent0n-Li/ChatDoctor MING 专业 该项目开源了医疗大模型MING，基于bloomz-7b指令微调得到MING-7B，支持医疗问答、智能问诊等功能。 https://github.com/189569400/MedicalGPT-zh IvyGPT 专业 该项目开源了医疗大模型IvyGPT，它在高质量的医学问答数据上进行了监督微调，并使用人类反馈的强化学习进行了训练。 https://github.com/WangRongsheng/IvyGPT PULSE 专业 该项目开源了中文医疗大模型PULSE，该模型使用约4,000,000个中文医学领域和通用领域的指令微调数据进行微调，支持医学领域的各种自然语言处理任务，包括健康教育、医师考试问题、报告解读、医疗记录结构化以及模拟诊断和治疗。 https://github.com/openmedlab/PULSE HuangDI 专业 该项目开源了中医大模型HuangDI (皇帝)，该模型首先在Ziya-LLaMA-13B-V1基座模型的基础上加入中医教材、中医各类网站数据等语料库，训练出一个具有中医知识理解力的预训练模型，之后在此基础上通过海量的中医古籍指令对话数据及通用指令数据进行有监督微调，使得模型具备中医古籍知识问答能力。 https://github.com/Zlasejd/HuangDI CMLM-ZhongJing 专业 该项目开源了中医大模型ZhongJing (仲景)，该模型旨在阐明中医博大精深之知识，传承古代智慧与现代技术创新，最终为医学领域提供可信赖和专业的工具。 https://github.com/pariskang/CMLM-ZhongJing TCMLLM 专业 该项目拟通过大模型方式实现中医临床辅助诊疗（病证诊断、处方推荐等）中医药知识问答等任务，推动中医知识问答、临床辅助诊疗等领域的快速发展。目前针对中医临床智能诊疗问题中的处方推荐任务，发布了中医处方推荐大模型TCMLLM-PR，通过整合真实世界临床病历、医学典籍与中医教科书等数据，构建了包含68k数据条目的处方推荐指令微调数据集，在ChatGLM大模型上进行微调得到。 https://github.com/2020MEAI/TCMLLM">
<meta name="application-name" content="Med-Eval">
<meta name="apple-mobile-web-app-title" content="Med-Eval"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95v2/" /><link rel="prev" href="https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/" /><link rel="next" href="https://dujh22.github.io/model/medrad%E4%B8%80%E4%B8%AA%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E9%9D%A0%E8%BE%85%E5%8A%A9%E5%86%B3%E7%AD%96%E6%A1%86%E6%9E%B6/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "医学大模型榜单V2",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/dujh22.github.io\/model\/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95v2\/"
    },"genre": "model","wordcount":  767 ,
    "url": "https:\/\/dujh22.github.io\/model\/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95v2\/","datePublished": "2024-01-03T12:18:27+08:00","dateModified": "2024-01-03T12:18:27+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="Med-Eval"><img loading="lazy" src="/fixit.svg" data-title="Med-Eval" data-alt="Med-Eval" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-desktop" class="typeit"></span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/med-eval"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li class="menu-item has-children">
              <a
                class="menu-link"
                href="/board"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 榜单</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
                <ul class="sub-menu">
                  <li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/model/"
                          
                          
                        >模型榜单</a>
                      </li><li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/data/"
                          
                          
                        >数据榜单</a>
                      </li><li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/tool/%E6%B5%8B%E8%AF%84%E5%B7%A5%E5%85%B7"
                          
                          
                        >测评工具</a>
                      </li></ul></li><li class="menu-item has-children">
              <a
                class="menu-link"
                href="https://www.opende.org.cn/arena/"
                
                rel="noopener noreferrer" target="_blank"
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 竞技场</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
                <ul class="sub-menu">
                  <li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="https://www.opende.org.cn/demo/chat"
                          
                          rel="noopener noreferrer" target="_blank"
                        ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> AiMed</a>
                      </li></ul></li><li class="menu-item">
              <a
                class="menu-link"
                href="/more"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 加入竞技</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="Med-Eval"><img loading="lazy" src="/fixit.svg" data-title="/fixit.svg" data-alt="/fixit.svg" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-title-mobile" class="typeit"></span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/med-eval"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li
              class="menu-item"
            ><span class="nested-item">
                  <a
                    class="menu-link"
                    href="/board"
                    
                    
                  ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 榜单</a>
                  <i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden="true"></i>
                </span>
                <ul class="sub-menu">
                  <li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/model/"
                          
                          
                        >模型榜单</a>
                      </li><li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/data/"
                          
                          
                        >数据榜单</a>
                      </li><li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/tool/%E6%B5%8B%E8%AF%84%E5%B7%A5%E5%85%B7"
                          
                          
                        >测评工具</a>
                      </li></ul></li><li
              class="menu-item"
            ><span class="nested-item">
                  <a
                    class="menu-link"
                    href="https://www.opende.org.cn/arena/"
                    
                    rel="noopener noreferrer" target="_blank"
                  ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 竞技场</a>
                  <i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden="true"></i>
                </span>
                <ul class="sub-menu">
                  <li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="https://www.opende.org.cn/demo/chat"
                          
                          rel="noopener noreferrer" target="_blank"
                        ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> AiMed</a>
                      </li></ul></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/more"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 加入竞技</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><article class="page single special">
    <div class="header"><h1 class="single-title animate__animated animate__pulse animate__faster">医学大模型榜单V2</h1></div><div
      class="content"
      id="content"
      
      
    ><table>
  <thead>
      <tr>
          <th>模型名称</th>
          <th>模型类型</th>
          <th>支持语言</th>
          <th>参数规模</th>
          <th>发布机构</th>
          <th>模型最后一次更新时间</th>
          <th>简介</th>
          <th>GitHub地址</th>
          <th>Hugging Face 地址</th>
          <th>论文地址</th>
          <th>官网地址</th>
          <th>其它信息来源</th>
          <th>API 是否可用</th>
          <th>API 使用地址</th>
          <th>是否可以私有化部署</th>
          <th>Demo 地址</th>
          <th>上下文长度</th>
          <th>训练用数据</th>
          <th>训练基座</th>
          <th>是否可商用</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>gpt-4-1106-preview</td>
          <td>通用</td>
          <td>多语言</td>
          <td>1.8T (未证实)</td>
          <td>OpenAI</td>
          <td>2023.11.6</td>
          <td>GPT-4，或称Generative Pre-trained Transformer 4，是由OpenAI开发的最先进的语言模型。它的参数量大概是其前身GPT-3的10倍。这一进步使GPT-4能够以惊人的准确性和细微差别理解和生成类似人类的文本。它擅长各种任务，如回答问题、写文章、总结文本等等。它在不同数据集上进行大规模训练，能够深入理解多种语言和科目，使其成为人工智能驱动应用程序中的通用工具。</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td><a href="https://platform.openai.com/docs/overview"target="_blank" rel="external nofollow noopener noreferrer">https://platform.openai.com/docs/overview</a></td>
          <td>到处都有</td>
          <td>付费，输入/输出 1k token 花费 0.01$/0.03$</td>
          <td><a href="https://platform.openai.com/docs/api-reference"target="_blank" rel="external nofollow noopener noreferrer">https://platform.openai.com/docs/api-reference</a></td>
          <td>否</td>
          <td><a href="https://chat.openai.com/"target="_blank" rel="external nofollow noopener noreferrer">https://chat.openai.com/</a></td>
          <td>128K</td>
          <td>未公布</td>
          <td>无</td>
          <td>是</td>
      </tr>
      <tr>
          <td>gpt-3.5-turbo-1106</td>
          <td>通用</td>
          <td>多语言</td>
          <td>175B (未证实)</td>
          <td>OpenAI</td>
          <td>2023.11.6</td>
          <td>GPT-3.5是OpenAI开发的先进自然语言处理模型，作为GPT-3系列的改进版本。它继承了GPT-3的基本架构，拥有数以百亿计的参数，能够更有效地处理和生成自然语言文本。GPT-3.5在理解复杂语境、回答问题、撰写文章和文本摘要方面表现出色。通过广泛的数据训练，GPT-3.5能够处理多种语言和领域的内容，是人工智能应用中的多功能工具。</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td><a href="https://platform.openai.com/docs/overview"target="_blank" rel="external nofollow noopener noreferrer">https://platform.openai.com/docs/overview</a></td>
          <td>到处都有</td>
          <td>付费，输入/输出 1k token 花费 0.001$/0.002$</td>
          <td><a href="https://platform.openai.com/docs/api-reference"target="_blank" rel="external nofollow noopener noreferrer">https://platform.openai.com/docs/api-reference</a></td>
          <td>否</td>
          <td><a href="https://chat.openai.com/"target="_blank" rel="external nofollow noopener noreferrer">https://chat.openai.com/</a></td>
          <td>16K</td>
          <td>未公布</td>
          <td>无</td>
          <td>是</td>
      </tr>
      <tr>
          <td>ChatGLM3-6B</td>
          <td>通用</td>
          <td>双语</td>
          <td>6.2B</td>
          <td>清华大学</td>
          <td>2023.10</td>
          <td>ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性：更强大的基础模型： ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略；更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景；更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、长文本对话模型 ChatGLM3-6B-32K。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。</td>
          <td><a href="https://github.com/THUDM/ChatGLM3"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/ChatGLM3</a></td>
          <td><a href="https://huggingface.co/THUDM/chatglm3-6b"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/THUDM/chatglm3-6b</a></td>
          <td><a href="https://arxiv.org/pdf/2210.02414.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2210.02414.pdf</a></td>
          <td><a href="https://www.chatglm.cn/"target="_blank" rel="external nofollow noopener noreferrer">https://www.chatglm.cn/</a></td>
          <td>无</td>
          <td>私有部署免费，官网调用收费</td>
          <td><a href="https://zhipu-ai.feishu.cn/wiki/FelEwysrFiM81ekrRqfcWN24nXb"target="_blank" rel="external nofollow noopener noreferrer">API 开发文档</a></td>
          <td>是，32GB内存，13GB显存</td>
          <td>无(可以本地安装)</td>
          <td>8K/32K</td>
          <td>The pile； Wudaocorpora</td>
          <td>ChatGLM3-6B-Base</td>
          <td>填写问卷后可以免费商用</td>
      </tr>
      <tr>
          <td>BenTsao (本草)</td>
          <td>专业</td>
          <td>中文</td>
          <td>同基座</td>
          <td>哈尔滨工业大学</td>
          <td>2023.8.3</td>
          <td>开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。</td>
          <td><a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese</a></td>
          <td>无</td>
          <td><a href="https://arxiv.org/pdf/2304.06975.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2304.06975.pdf</a></td>
          <td>无</td>
          <td>无</td>
          <td>不可用</td>
          <td>无</td>
          <td>否</td>
          <td>无</td>
          <td>同基座</td>
          <td>github仓库里提到了若干个微调用数据集，之后可以调研</td>
          <td>活字1.0/Bloom-7B/Alpaca-Chinese-7B/LLaMA-7B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>DoctorGLM</td>
          <td>专业</td>
          <td>双语</td>
          <td>同基座</td>
          <td>上海交通大学，复旦大学</td>
          <td>2023.6</td>
          <td>基于 ChatGLM-6B的中文问诊模型，通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署</td>
          <td><a href="https://github.com/xionghonglin/DoctorGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xionghonglin/DoctorGLM</a></td>
          <td>无</td>
          <td><a href="https://arxiv.org/pdf/2304.01097.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2304.01097.pdf</a></td>
          <td><a href="https://xionghonglin.github.io/DoctorGLM/"target="_blank" rel="external nofollow noopener noreferrer">https://xionghonglin.github.io/DoctorGLM/</a></td>
          <td><a href="https://zhuanlan.zhihu.com/p/622649076"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/622649076</a></td>
          <td>不可用</td>
          <td>无</td>
          <td>否</td>
          <td>无</td>
          <td>同基座</td>
          <td>见仓库</td>
          <td>ChatGLM-6B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>BianQue</td>
          <td>专业</td>
          <td>中文</td>
          <td>同基座</td>
          <td>华南理工大学</td>
          <td>2023.6.6</td>
          <td>一个经过指令与多轮问询对话联合微调的医疗对话大模型，基于ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。</td>
          <td><a href="https://github.com/scutcyr/BianQue"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/scutcyr/BianQue</a></td>
          <td><a href="https://huggingface.co/scutcyr/BianQue-2"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/scutcyr/BianQue-2</a></td>
          <td><a href="https://arxiv.org/pdf/2310.15896.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2310.15896.pdf</a></td>
          <td>无</td>
          <td>无</td>
          <td>可以私有部署api</td>
          <td><a href="https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py</a></td>
          <td>是</td>
          <td>无</td>
          <td>同基座</td>
          <td>我们结合当前开源的中文医疗问答数据集（<a href="https://github.com/UCSD-AI4H/Medical-Dialogue-System"target="_blank" rel="external nofollow noopener noreferrer">MedDialog-CN</a>、<a href="https://github.com/lemuria-wchen/imcs21"target="_blank" rel="external nofollow noopener noreferrer">IMCS-V2</a>、<a href="https://tianchi.aliyun.com/dataset/95414"target="_blank" rel="external nofollow noopener noreferrer">CHIP-MDCFNPC</a>、<a href="https://tianchi.aliyun.com/dataset/95414"target="_blank" rel="external nofollow noopener noreferrer">MedDG</a>、<a href="https://github.com/zhangsheng93/cMedQA2"target="_blank" rel="external nofollow noopener noreferrer">cMedQA2</a>、<a href="https://github.com/Toyhom/Chinese-medical-dialogue-data"target="_blank" rel="external nofollow noopener noreferrer">Chinese-medical-dialogue-data</a>），分析其中的单轮/多轮特性以及医生问询特性，结合实验室长期自建的生活空间健康对话大数据，构建了千万级别规模的扁鹊健康大数据BianQueCorpus。</td>
          <td>ChatGLM-6B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>HuatuoGPT-II</td>
          <td>专业</td>
          <td>双语</td>
          <td>7B/13B/34B</td>
          <td>深圳大数据研究院，香港中文大学(深圳)</td>
          <td>2023.11.23</td>
          <td>开源了经过中文医学指令精调/指令微调(Instruct-tuning)的一个GPT-like模型</td>
          <td><a href="https://github.com/FreedomIntelligence/HuatuoGPT-II"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FreedomIntelligence/HuatuoGPT-II</a></td>
          <td>好多个，详见github</td>
          <td><a href="https://arxiv.org/pdf/2311.09774.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2311.09774.pdf</a></td>
          <td>无</td>
          <td>无</td>
          <td>私有部署，自己开发</td>
          <td>无</td>
          <td>是</td>
          <td><a href="https://www.huatuogpt.cn/"target="_blank" rel="external nofollow noopener noreferrer">https://www.huatuogpt.cn/</a></td>
          <td>同基座</td>
          <td><a href="https://github.com/king-yyf/CMeKG_tools"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/king-yyf/CMeKG_tools</a>，还用ChatGPT构造了一些，和本草相同</td>
          <td>Baichuan2-7B-Base/Baichuan2-13B-Base/Yi-34B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>Med-ChatGLM</td>
          <td>专业</td>
          <td>中文</td>
          <td>同基座</td>
          <td>哈尔滨工业大学</td>
          <td>2023.3</td>
          <td>基于中文医学知识的ChatGLM模型微调，微调数据与BenTsao相同。</td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>或许可用</td>
          <td>无</td>
          <td>是，下载地址：https://pan.baidu.com/share/init?surl=Sfi1bRwV741GIChIEOUW0A&amp;pwd=i73e，https://drive.google.com/drive/folders/1ZQSN56DloRGQ-Qj7IwzY4jV3ZHKMe9Bc</td>
          <td>无</td>
          <td>同基座</td>
          <td><a href="https://github.com/king-yyf/CMeKG_tools"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/king-yyf/CMeKG_tools</a>，还用</td>
          <td>ChatGLM-6B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>QiZhenGPT</td>
          <td>专业</td>
          <td>中文</td>
          <td>7B/6B/13B</td>
          <td>浙江大学</td>
          <td>2023.6.27</td>
          <td>该项目利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。</td>
          <td><a href="https://github.com/CMKRG/QiZhenGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CMKRG/QiZhenGPT</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可以私有部署</td>
          <td>无</td>
          <td>是，见github</td>
          <td>无</td>
          <td>同基座</td>
          <td>见仓库</td>
          <td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca"target="_blank" rel="external nofollow noopener noreferrer">Chinese-LLaMA-Plus-7B</a>/<a href="https://github.com/zjunlp/CaMA"target="_blank" rel="external nofollow noopener noreferrer">CaMA-13B</a>/<a href="https://github.com/THUDM/ChatGLM-6B"target="_blank" rel="external nofollow noopener noreferrer">ChatGLM-6B</a></td>
          <td>否</td>
      </tr>
      <tr>
          <td>ChatMed</td>
          <td>专业</td>
          <td>中文</td>
          <td>7B（同基座模型Llama-7b）</td>
          <td>华东师范大学</td>
          <td>2023.5.5</td>
          <td>该项目推出ChatMed系列中文医疗大规模语言模型，模型主干为LlaMA-7b并采用LoRA微调，具体包括ChatMed-Consult : 基于中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w+在线问诊+ChatGPT回复作为训练集；ChatMed-TCM : 基于中医药指令数据集ChatMed_TCM_Dataset，以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到2.6w+的围绕中医药的指令数据训练得到。</td>
          <td><a href="https://github.com/michael-wzhu/ChatMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/michael-wzhu/ChatMed</a></td>
          <td><a href="https://huggingface.co/michaelwzhu/ChatMed-Consult"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/michaelwzhu/ChatMed-Consult</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可以私有部署api，需要下载模型后自己开发api</td>
          <td>无</td>
          <td>是，.bin文件80M</td>
          <td>无</td>
          <td>同基座</td>
          <td><a href="https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset</a></td>
          <td>Llama-7b</td>
          <td>否</td>
      </tr>
      <tr>
          <td>XrayGLM</td>
          <td>专业</td>
          <td>双语</td>
          <td>6B，同基座</td>
          <td>澳门理工大学</td>
          <td>2023.5</td>
          <td>该项目为促进中文领域医学多模态大模型的研究发展，发布了XrayGLM数据集及模型，其在医学影像诊断和多轮交互对话上显示出了非凡的潜力。</td>
          <td><a href="https://github.com/WangRongsheng/XrayGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/WangRongsheng/XrayGLM</a></td>
          <td><a href="https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可以用webUI</td>
          <td>详见github中webUI一部分</td>
          <td>是</td>
          <td>无</td>
          <td>同基座</td>
          <td><a href="https://physionet.org/content/mimic-cxr-jpg/2.0.0/"target="_blank" rel="external nofollow noopener noreferrer">https://physionet.org/content/mimic-cxr-jpg/2.0.0/</a>   <a href="https://openi.nlm.nih.gov/faq#collection"target="_blank" rel="external nofollow noopener noreferrer">https://openi.nlm.nih.gov/faq#collection</a></td>
          <td>VisualGLM-6B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>MeChat</td>
          <td>专业</td>
          <td>中文</td>
          <td>6B</td>
          <td>浙江大学，西湖大学</td>
          <td>2023.12</td>
          <td>该项目开源的中文心理健康支持通用模型由 ChatGLM-6B LoRA 16-bit 指令微调得到。数据集通过调用gpt-3.5-turbo API扩展真实的心理互助 QA为多轮的心理健康支持多轮对话，提高了通用语言大模型在心理健康支持领域的表现，更加符合在长程多轮对话的应用场景。</td>
          <td><a href="https://github.com/qiuhuachuan/smile"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/qiuhuachuan/smile</a></td>
          <td><a href="https://huggingface.co/qiuhuachuan/MeChat"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/qiuhuachuan/MeChat</a></td>
          <td><a href="https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf</a></td>
          <td>无</td>
          <td>无</td>
          <td>可以，有交互文件可运行</td>
          <td>无</td>
          <td>是</td>
          <td>http://47.97.220.53:8080/ （似乎不能用了）</td>
          <td>同基座</td>
          <td>smileChat，见仓库</td>
          <td>ChatGLM2-6B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>MedicalGPT</td>
          <td>专业</td>
          <td>双语</td>
          <td>13B</td>
          <td>个人（应该是公司里的，百度/腾讯）</td>
          <td>2023.10.23</td>
          <td>训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。发布中文医疗LoRA模型shibing624/ziya-llama-13b-medical-lora，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重。</td>
          <td><a href="https://github.com/shibing624/MedicalGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/shibing624/MedicalGPT</a></td>
          <td><a href="https://huggingface.co/shibing624/vicuna-baichuan-13b-chat"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/shibing624/vicuna-baichuan-13b-chat</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可以，demo的api</td>
          <td>详见github Demo</td>
          <td>是</td>
          <td>详见github Demo部分</td>
          <td>同基座</td>
          <td>240w条中文医疗数据：https://huggingface.co/datasets/shibing624/medical  22万条中文医疗对话数据集(华佗项目)：<a href="https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1"target="_blank" rel="external nofollow noopener noreferrer">FreedomIntelligence/HuatuoGPT-sft-data-v1</a></td>
          <td>BaiChuan-13B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>Sunsimiao</td>
          <td>专业</td>
          <td>中文</td>
          <td>7B</td>
          <td>华东理工大学</td>
          <td>2023.6</td>
          <td>Sunsimiao是一个开源的中文医疗大模型，该模型基于baichuan-7B和ChatGLM-6B底座模型在十万级高质量的中文医疗数据中微调而得。</td>
          <td><a href="https://github.com/thomas-yanxin/Sunsimiao"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/thomas-yanxin/Sunsimiao</a></td>
          <td><a href="https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary"target="_blank" rel="external nofollow noopener noreferrer">https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>有单纯的类似命令行输入输出，一段使用代码</td>
          <td>无</td>
          <td>是</td>
          <td>无</td>
          <td>同基座</td>
          <td>没有提供10W中文医疗数据</td>
          <td>BaiChuan-7B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>ShenNong-TCM-LLM</td>
          <td>专业</td>
          <td>中文</td>
          <td>7B</td>
          <td>华东师范大学</td>
          <td>2023.6</td>
          <td>该项目开源了ShenNong中医药大规模语言模型，该模型以LlaMA为底座，采用LoRA (rank=16)微调得到。微调代码与ChatMed代码库相同。此外该项目还开源了中医药指令微调数据集。</td>
          <td><a href="https://github.com/michael-wzhu/ShenNong-TCM-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/michael-wzhu/ShenNong-TCM-LLM</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>否</td>
          <td>无</td>
          <td>同基座</td>
          <td><a href="https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset</a>   中医药知识图谱：https://github.com/ywjawmw/TCM_KG</td>
          <td>Llama-7B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>SoulChat</td>
          <td>专业</td>
          <td>中文</td>
          <td>6B</td>
          <td>华南理工大学</td>
          <td>2023.7</td>
          <td>该项目开源了经过百万规模心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调的心理健康大模型灵心（SoulChat），该模型以ChatGLM-6B作为初始化模型，进行了全量参数的指令微调。</td>
          <td><a href="https://github.com/scutcyr/SoulChat"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/scutcyr/SoulChat</a></td>
          <td>扁鹊：https://huggingface.co/scutcyr/BianQue-2  灵心：https://huggingface.co/scutcyr/SoulChat</td>
          <td><a href="https://aclanthology.org/2023.findings-emnlp.83/"target="_blank" rel="external nofollow noopener noreferrer">https://aclanthology.org/2023.findings-emnlp.83/</a></td>
          <td>无</td>
          <td>无</td>
          <td>可以，有Demo</td>
          <td>详见github启动服务</td>
          <td>是</td>
          <td>提供<a href="https://github.com/scutcyr/SoulChat/blob/main/soulchat_app.py"target="_blank" rel="external nofollow noopener noreferrer">soulchat_app.py</a>代码</td>
          <td>同基座</td>
          <td>数据集即将发布</td>
          <td>ChatGLM-6B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>CareGPT</td>
          <td>专业</td>
          <td>双语</td>
          <td>有多种，包括7b、13B、14B、20B，同基座</td>
          <td>澳门理工大学</td>
          <td>2023.8</td>
          <td>该项目开源了数十个公开可用的医疗微调数据集和开放可用的医疗大语言模型，包含LLM的训练、测评、部署等以促进医疗LLM快速发展。</td>
          <td><a href="https://github.com/WangRongsheng/CareGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/WangRongsheng/CareGPT</a></td>
          <td><a href="https://huggingface.co/wangrongsheng/carellm/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/wangrongsheng/carellm/tree/main</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可用</td>
          <td>github上有介绍，在部署的部分</td>
          <td>是</td>
          <td><a href="https://huggingface.co/spaces/wangrongsheng/CareLlama"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/spaces/wangrongsheng/CareLlama</a></td>
          <td>同基座</td>
          <td>在github数据部分</td>
          <td>llama-7b、baichuan</td>
          <td>是</td>
      </tr>
      <tr>
          <td>DISC-MedLLM</td>
          <td>专业</td>
          <td>双语</td>
          <td>13B</td>
          <td>复旦大学</td>
          <td>2023.8</td>
          <td>该项目是由复旦大学发布的针对医疗健康对话式场景而设计的医疗领域大模型与数据集，该模型由DISC-Med-SFT数据集基于Baichuan-13B-Base指令微调得到。</td>
          <td><a href="https://github.com/FudanDISC/DISC-MedLLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FudanDISC/DISC-MedLLM</a></td>
          <td><a href="https://huggingface.co/Flmc/DISC-MedLLM"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/Flmc/DISC-MedLLM</a></td>
          <td><a href="https://arxiv.org/abs/2308.14346"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2308.14346</a></td>
          <td>无</td>
          <td>无</td>
          <td>可用，cli_demo.py</td>
          <td>详见github demo</td>
          <td>是</td>
          <td>web_demo.py</td>
          <td>同基座</td>
          <td><a href="https://huggingface.co/datasets/Flmc/DISC-Med-SFT"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/datasets/Flmc/DISC-Med-SFT</a></td>
          <td>baichuan-13B-base</td>
          <td>否</td>
      </tr>
      <tr>
          <td>Taiyi-LLM</td>
          <td>专业</td>
          <td>双语</td>
          <td>7BB</td>
          <td>大连理工大学</td>
          <td>2023.10</td>
          <td>该项目由大连理工大学信息检索研究室开发的中英双语医学大模型&quot;太一&quot;，收集整理了丰富的中英双语生物医学自然语言处理（BioNLP）训练语料，总共包含38个中文数据集，通过丰富的中英双语任务指令数据（超过100W条样本）进行大模型（Qwen-7B-base）指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种BioNLP能力。</td>
          <td><a href="https://github.com/DUTIR-BioNLP/Taiyi-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DUTIR-BioNLP/Taiyi-LLM</a></td>
          <td><a href="https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM</a></td>
          <td><a href="https://arxiv.org/abs/2311.11608"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2311.11608</a></td>
          <td></td>
          <td>无</td>
          <td>似乎可用，dialog开头的python文件</td>
          <td>如前</td>
          <td>是</td>
          <td><a href="https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/"target="_blank" rel="external nofollow noopener noreferrer">https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/</a></td>
          <td>同基座</td>
          <td><a href="https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md</a></td>
          <td>Qwen-7B -base</td>
          <td>否</td>
      </tr>
  </tbody>
</table>
<h2 id="下述模型仍然在信息更新中">下述模型仍然在信息更新中……</h2>
<table>
  <thead>
      <tr>
          <th>模型名称</th>
          <th>模型类型</th>
          <th>简介</th>
          <th>GitHub地址</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>WiNGPT</td>
          <td>专业</td>
          <td>WiNGPT是一个基于GPT的医疗垂直领域大模型，基于Qwen-7b1作为基础预训练模型，在此技术上进行了继续预训练，指令微调等，该项目具体开源了WiNGPT2-7B-Base与WiNGPT2-7B-Chat模型。</td>
          <td><a href="https://github.com/winninghealth/WiNGPT2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/winninghealth/WiNGPT2</a></td>
      </tr>
      <tr>
          <td>ChiMed-GPT</td>
          <td>专业</td>
          <td>ChiMed-GPT是一个开源中文医学大语言模型，通过在中文医学数据上持续训练 Ziya-v2 构建而成，其中涵盖了预训练、有监督微调 (SFT) 和来自人类反馈的强化学习 (RLHF) 等训练过程。</td>
          <td><a href="https://github.com/synlp/ChiMed-GPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/synlp/ChiMed-GPT</a></td>
      </tr>
      <tr>
          <td>ChatGLM</td>
          <td>通用</td>
          <td>中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持</td>
          <td><a href="https://github.com/THUDM/ChatGLM-6B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/ChatGLM-6B</a></td>
      </tr>
      <tr>
          <td>ChatGLM2-6B</td>
          <td>通用</td>
          <td>基于开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练；基座模型的上下文长度扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练；基于 Multi-Query Attention 技术实现更高效的推理速度和更低的显存占用；允许商业使用。</td>
          <td><a href="https://github.com/THUDM/ChatGLM2-6B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/ChatGLM2-6B</a></td>
      </tr>
      <tr>
          <td>Chinese-LLaMA-Alpaca</td>
          <td>通用</td>
          <td>中文LLaMA&amp;Alpaca大语言模型+本地CPU/GPU部署，在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练</td>
          <td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></td>
      </tr>
      <tr>
          <td>Chinese-LLaMA-Alpaca-2</td>
          <td>通用</td>
          <td>该项目将发布中文LLaMA-2 &amp; Alpaca-2大语言模型，基于可商用的LLaMA-2进行二次开发</td>
          <td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ymcui/Chinese-LLaMA-Alpaca-2</a></td>
      </tr>
      <tr>
          <td>Chinese-LlaMA2</td>
          <td>通用</td>
          <td>该项目基于可商用的LLaMA-2进行二次开发决定在次开展Llama 2的中文汉化工作，包括Chinese-LlaMA2: 对Llama 2进行中文预训练；第一步：先在42G中文预料上进行训练；后续将会加大训练规模；Chinese-LlaMA2-chat: 对Chinese-LlaMA2进行指令微调和多轮对话微调，以适应各种应用场景和多轮对话交互。同时我们也考虑更为快速的中文适配方案：Chinese-LlaMA2-sft-v0: 采用现有的开源中文指令微调或者是对话数据，对LlaMA-2进行直接微调 (将于近期开源)。</td>
          <td><a href="https://github.com/michael-wzhu/Chinese-LlaMA2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/michael-wzhu/Chinese-LlaMA2</a></td>
      </tr>
      <tr>
          <td>Llama2-Chinese</td>
          <td>通用</td>
          <td>该项目专注于Llama2模型在中文方面的优化和上层建设，基于大规模中文数据，从预训练开始对Llama2模型进行中文能力的持续迭代升级。</td>
          <td><a href="https://github.com/FlagAlpha/Llama2-Chinese"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FlagAlpha/Llama2-Chinese</a></td>
      </tr>
      <tr>
          <td>Qwen</td>
          <td>通用</td>
          <td>通义千问 是阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。各个规模的模型包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。数据集包括文本和代码等多种数据类型，覆盖通用领域和专业领域，能支持8K的上下文长度，针对插件调用相关的对齐数据做了特定优化，当前模型能有效调用插件以及升级为Agent。</td>
          <td><a href="https://github.com/QwenLM/Qwen"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/QwenLM/Qwen</a></td>
      </tr>
      <tr>
          <td>OpenChineseLLaMA</td>
          <td>通用</td>
          <td>基于 LLaMA-7B 经过中文数据集增量预训练产生的中文大语言模型基座，对比原版 LLaMA，该模型在中文理解能力和生成能力方面均获得较大提升，在众多下游任务中均取得了突出的成绩。</td>
          <td><a href="https://github.com/OpenLMLab/OpenChineseLLaMA"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OpenLMLab/OpenChineseLLaMA</a></td>
      </tr>
      <tr>
          <td>BELLE</td>
          <td>通用</td>
          <td>开源了基于BLOOMZ和LLaMA优化后的一系列模型，同时包括训练数据、相关模型、训练代码、应用场景等，也会持续评估不同训练数据、训练算法等对模型表现的影响。</td>
          <td><a href="https://github.com/LianjiaTech/BELLE"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/LianjiaTech/BELLE</a></td>
      </tr>
      <tr>
          <td>Panda</td>
          <td>通用</td>
          <td>开源了基于LLaMA-7B, -13B, -33B, -65B 进行中文领域上的持续预训练的语言模型, 使用了接近 15M 条数据进行二次预训练。</td>
          <td><a href="https://github.com/dandelionsllm/pandallm"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/dandelionsllm/pandallm</a></td>
      </tr>
      <tr>
          <td>Robin (罗宾)</td>
          <td>通用</td>
          <td>Robin (罗宾)是香港科技大学LMFlow团队开发的中英双语大语言模型。仅使用180K条数据微调得到的Robin第二代模型，在Huggingface榜单上达到了第一名的成绩。LMFlow支持用户快速训练个性化模型，仅需单张3090和5个小时即可微调70亿参数定制化模型。</td>
          <td><a href="https://github.com/OptimalScale/LMFlow"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OptimalScale/LMFlow</a></td>
      </tr>
      <tr>
          <td>Fengshenbang-LM</td>
          <td>通用</td>
          <td>Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，该项目开源了姜子牙通用大模型V1，是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。除姜子牙系列模型之外，该项目还开源了太乙、二郎神系列等模型。</td>
          <td><a href="https://github.com/IDEA-CCNL/Fengshenbang-LM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/IDEA-CCNL/Fengshenbang-LM</a></td>
      </tr>
      <tr>
          <td>BiLLa</td>
          <td>通用</td>
          <td>该项目开源了推理能力增强的中英双语LLaMA模型。模型的主要特性有：较大提升LLaMA的中文理解能力，并尽可能减少对原始LLaMA英文能力的损伤；训练过程增加较多的任务型数据，利用ChatGPT生成解析，强化模型理解任务求解逻辑；全量参数更新，追求更好的生成效果。</td>
          <td><a href="https://github.com/Neutralzz/BiLLa"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Neutralzz/BiLLa</a></td>
      </tr>
      <tr>
          <td>Moss</td>
          <td>通用</td>
          <td>支持中英双语和多种插件的开源对话语言模型，MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。</td>
          <td><a href="https://github.com/OpenLMLab/MOSS"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OpenLMLab/MOSS</a></td>
      </tr>
      <tr>
          <td>Luotuo-Chinese-LLM</td>
          <td>通用</td>
          <td>囊括了一系列中文大语言模型开源项目，包含了一系列基于已有开源模型（ChatGLM, MOSS, LLaMA）进行二次微调的语言模型，指令微调数据集等。</td>
          <td><a href="https://github.com/LC1332/Luotuo-Chinese-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/LC1332/Luotuo-Chinese-LLM</a></td>
      </tr>
      <tr>
          <td>Linly</td>
          <td>通用</td>
          <td>提供中文对话模型 Linly-ChatFlow 、中文基础模型 Linly-Chinese-LLaMA 及其训练数据。 中文基础模型以 LLaMA 为底座，利用中文和中英平行增量预训练。项目汇总了目前公开的多语言指令数据，对中文模型进行了大规模指令跟随训练，实现了 Linly-ChatFlow 对话模型。</td>
          <td><a href="https://github.com/CVI-SZU/Linly"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CVI-SZU/Linly</a></td>
      </tr>
      <tr>
          <td>Firefly</td>
          <td>通用</td>
          <td>Firefly(流萤) 是一个开源的中文大语言模型项目，开源包括数据、微调代码、多个基于Bloom、baichuan等微调好的模型等；支持全量参数指令微调、QLoRA低成本高效指令微调、LoRA指令微调；支持绝大部分主流的开源大模型，如百川baichuan、Ziya、Bloom、LLaMA等。持lora与base model进行权重合并，推理更便捷。</td>
          <td><a href="https://github.com/yangjianxin1/Firefly"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/yangjianxin1/Firefly</a></td>
      </tr>
      <tr>
          <td>ChatYuan</td>
          <td>通用</td>
          <td>元语智能发布的一系列支持中英双语的功能型对话语言大模型，在微调数据、人类反馈强化学习、思维链等方面进行了优化。</td>
          <td><a href="https://github.com/clue-ai/ChatYuan"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/clue-ai/ChatYuan</a></td>
      </tr>
      <tr>
          <td>ChatRWKV</td>
          <td>通用</td>
          <td>开源了一系列基于RWKV架构的Chat模型（包括英文和中文），发布了包括Raven，Novel-ChnEng，Novel-Ch与Novel-ChnEng-ChnPro等模型，可以直接闲聊及进行诗歌，小说等创作，包括7B和14B等规模的模型。</td>
          <td><a href="https://github.com/BlinkDL/ChatRWKV"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/BlinkDL/ChatRWKV</a></td>
      </tr>
      <tr>
          <td>CPM-Bee</td>
          <td>通用</td>
          <td>一个完全开源、允许商用的百亿参数中英文基座模型。它采用Transformer自回归架构（auto-regressive），在超万亿（trillion）高质量语料上进行预训练，拥有强大的基础能力。开发者和研究者可以在CPM-Bee基座模型的基础上在各类场景进行适配来以创建特定领域的应用模型。</td>
          <td><a href="https://github.com/OpenBMB/CPM-Bee"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OpenBMB/CPM-Bee</a></td>
      </tr>
      <tr>
          <td>TigerBot</td>
          <td>通用</td>
          <td>一个多语言多任务的大规模语言模型(LLM)，开源了包括模型：TigerBot-7B, TigerBot-7B-base，TigerBot-180B，基本训练和推理代码，100G预训练数据，涵盖金融、法律、百科的领域数据以及API等。</td>
          <td><a href="https://github.com/TigerResearch/TigerBot"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/TigerResearch/TigerBot</a></td>
      </tr>
      <tr>
          <td>书生·浦语</td>
          <td>通用</td>
          <td>商汤科技、上海AI实验室联合香港中文大学、复旦大学和上海交通大学发布千亿级参数大语言模型“书生·浦语”（InternLM）。据悉，“书生·浦语”具有1040亿参数，基于“包含1.6万亿token的多语种高质量数据集”训练而成。</td>
          <td><a href="https://github.com/InternLM/InternLM-techreport"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/InternLM/InternLM-techreport</a></td>
      </tr>
      <tr>
          <td>Aquila</td>
          <td>通用</td>
          <td>由智源研究院发布，Aquila语言大模型在技术上继承了GPT-3、LLaMA等的架构设计优点，替换了一批更高效的底层算子实现、重新设计实现了中英双语的tokenizer，升级了BMTrain并行训练方法，是在中英文高质量语料基础上从０开始训练的，通过数据质量的控制、多种训练的优化方法，实现在更小的数据集、更短的训练时间，获得比其它开源模型更优的性能。也是首个支持中英双语知识、支持商用许可协议、符合国内数据合规需要的大规模开源语言模型。</td>
          <td><a href="https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila</a></td>
      </tr>
      <tr>
          <td>Baichuan-7B</td>
          <td>通用</td>
          <td>Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。该项目发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。</td>
          <td><a href="https://github.com/baichuan-inc/baichuan-7B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/baichuan-inc/baichuan-7B</a></td>
      </tr>
      <tr>
          <td>Baichuan-13B</td>
          <td>通用</td>
          <td>由百川智能开发的一个开源可商用的大规模预训练语言模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。</td>
          <td><a href="https://github.com/baichuan-inc/Baichuan-13B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/baichuan-inc/Baichuan-13B</a></td>
      </tr>
      <tr>
          <td>Baichuan2</td>
          <td>通用</td>
          <td>由百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练，在多个权威的中文、英文和多语言的通用、领域 benchmark上取得同尺寸最佳的效果，发布包含有7B、13B的Base和经过PPO训练的Chat版本，并提供了Chat版本的4bits量化。</td>
          <td><a href="https://github.com/baichuan-inc/Baichuan2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/baichuan-inc/Baichuan2</a></td>
      </tr>
      <tr>
          <td>Anima</td>
          <td>通用</td>
          <td>由艾写科技开发的一个开源的基于QLoRA的33B中文大语言模型，该模型基于QLoRA的Guanaco 33B模型使用Chinese-Vicuna项目开放的训练数据集guanaco_belle_merge_v1.0进行finetune训练了10000个step，基于Elo rating tournament评估效果较好。</td>
          <td><a href="https://github.com/lyogavin/Anima"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/lyogavin/Anima</a></td>
      </tr>
      <tr>
          <td>KnowLM</td>
          <td>通用</td>
          <td>KnowLM项目旨在发布开源大模型框架及相应模型权重以助力减轻知识谬误问题，包括大模型的知识难更新及存在潜在的错误和偏见等。该项目一期发布了基于Llama的抽取大模型智析，使用中英文语料对LLaMA（13B）进行进一步全量预训练，并基于知识图谱转换指令技术对知识抽取任务进行优化。</td>
          <td><a href="https://github.com/zjunlp/KnowLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zjunlp/KnowLM</a></td>
      </tr>
      <tr>
          <td>BayLing</td>
          <td>通用</td>
          <td>一个具有增强的跨语言对齐的通用大模型，由中国科学院计算技术研究所自然语言处理团队开发。百聆（BayLing）以LLaMA为基座模型，探索了以交互式翻译任务为核心进行指令微调的方法，旨在同时完成语言间对齐以及与人类意图对齐，将LLaMA的生成能力和指令跟随能力从英语迁移到其他语言（中文）。在多语言翻译、交互翻译、通用任务、标准化考试的测评中，百聆在中文/英语中均展现出更好的表现。百聆提供了在线的内测版demo，以供大家体验。</td>
          <td><a href="https://github.com/ictnlp/BayLing"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ictnlp/BayLing</a></td>
      </tr>
      <tr>
          <td>YuLan-Chat</td>
          <td>通用</td>
          <td>YuLan-Chat是中国人民大学GSAI研究人员开发的基于聊天的大语言模型。它是在LLaMA的基础上微调开发的，具有高质量的英文和中文指令。 YuLan-Chat可以与用户聊天，很好地遵循英文或中文指令，并且可以在量化后部署在GPU（A800-80G或RTX3090）上。</td>
          <td><a href="https://github.com/RUC-GSAI/YuLan-Chat"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/RUC-GSAI/YuLan-Chat</a></td>
      </tr>
      <tr>
          <td>PolyLM</td>
          <td>通用</td>
          <td>一个在6400亿个词的数据上从头训练的多语言语言模型，包括两种模型大小(1.7B和13B)。PolyLM覆盖中、英、俄、西、法、葡、德、意、荷、波、阿、土、希伯来、日、韩、泰、越、印尼等语种，特别是对亚洲语种更友好。</td>
          <td><a href="https://github.com/DAMO-NLP-MT/PolyLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DAMO-NLP-MT/PolyLM</a></td>
      </tr>
      <tr>
          <td>huozi</td>
          <td>通用</td>
          <td>由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048，同时还开源了基于RLHF训练的模型以及全人工标注的16.9K中文偏好数据集。</td>
          <td><a href="https://github.com/HIT-SCIR/huozi"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/HIT-SCIR/huozi</a></td>
      </tr>
      <tr>
          <td>YaYi</td>
          <td>通用</td>
          <td>雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，进一步提升了模型性能和安全性。已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。</td>
          <td><a href="https://github.com/wenge-research/YaYi"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/wenge-research/YaYi</a></td>
      </tr>
      <tr>
          <td>XVERSE-13B</td>
          <td>通用</td>
          <td>由深圳元象科技自主研发的支持多语言的大语言模型，使用主流 Decoder-only 的标准Transformer网络结构，支持 8K 的上下文长度（Context Length），为同尺寸模型中最长，构建了 1.4 万亿 token 的高质量、多样化的数据对模型进行充分训练，包含中、英、俄、西等 40 多种语言，通过精细化设置不同类型数据的采样比例，使得中英两种语言表现优异，也能兼顾其他语言效果；基于BPE算法使用上百GB 语料训练了一个词表大小为100,278的分词器，能够同时支持多语言，而无需额外扩展词表。</td>
          <td><a href="https://github.com/xverse-ai/XVERSE-13B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xverse-ai/XVERSE-13B</a></td>
      </tr>
      <tr>
          <td>Skywork</td>
          <td>通用</td>
          <td>该项目开源了天工系列模型，该系列模型在3.2TB高质量多语言和代码数据上进行预训练，开源了包括模型参数，训练数据，评估数据，评估方法。具体包括Skywork-13B-Base模型、Skywork-13B-Chat模型、Skywork-13B-Math模型和Skywork-13B-MM模型，以及每个模型的量化版模型，以支持用户在消费级显卡进行部署和推理。</td>
          <td><a href="https://github.com/SkyworkAI/Skywork"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SkyworkAI/Skywork</a></td>
      </tr>
      <tr>
          <td>Yi</td>
          <td>通用</td>
          <td>该项目开源了Yi-6B和Yi-34B等模型，该系列模型最长可支持200K的超长上下文窗口版本，可以处理约40万汉字超长文本输入，理解超过1000页的PDF文档。</td>
          <td><a href="https://github.com/01-ai/Yi"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/01-ai/Yi</a></td>
      </tr>
      <tr>
          <td>Yuan-2.0</td>
          <td>通用</td>
          <td>该项目开源了由浪潮信息发布的新一代基础语言大模型，具体开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且提供了预训练，微调，推理服务的相关脚本。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。</td>
          <td><a href="https://github.com/IEIT-Yuan/Yuan-2.0"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/IEIT-Yuan/Yuan-2.0</a></td>
      </tr>
      <tr>
          <td>AlpaCare</td>
          <td>专业</td>
          <td>该项目开源了医学大模型AlpaCare，在LLaMA上微调得到。</td>
          <td><a href="https://github.com/XZhang97666/AlpaCare"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/XZhang97666/AlpaCare</a></td>
      </tr>
      <tr>
          <td>Zhongjing</td>
          <td>专业</td>
          <td>该项目开源了首个包含预训练、有监督微调和 RLHF 完整训练流程的中文医学大模型，展现出了很好的泛化能力，在某些对话场景中甚至接近专业医生的专业水平。此外，还开源了一个包含 70,000 条完全来源于真实医患对话的多轮对话数据集。该数据集包含大量医生主动提问的语句，有助于提升模型的主动医疗询问能力。</td>
          <td><a href="https://github.com/SupritYoung/Zhongjing"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SupritYoung/Zhongjing</a></td>
      </tr>
      <tr>
          <td>PMC-LLaMA</td>
          <td>专业</td>
          <td>该项目开源了医疗大模型PMC-LLaMA，包括预训练版本的MedLLaMA_13B和指令微调版本的PMC_LLaMA_13B。</td>
          <td><a href="https://github.com/chaoyi-wu/PMC-LLaMA"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/chaoyi-wu/PMC-LLaMA</a></td>
      </tr>
      <tr>
          <td>ChatDoctor</td>
          <td>专业</td>
          <td>该项目开源了医疗大模型ChatDoctor，在LLaMA的基础上训练得到。</td>
          <td><a href="https://github.com/Kent0n-Li/ChatDoctor"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Kent0n-Li/ChatDoctor</a></td>
      </tr>
      <tr>
          <td>MING</td>
          <td>专业</td>
          <td>该项目开源了医疗大模型MING，基于bloomz-7b指令微调得到MING-7B，支持医疗问答、智能问诊等功能。</td>
          <td><a href="https://github.com/189569400/MedicalGPT-zh"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/189569400/MedicalGPT-zh</a></td>
      </tr>
      <tr>
          <td>IvyGPT</td>
          <td>专业</td>
          <td>该项目开源了医疗大模型IvyGPT，它在高质量的医学问答数据上进行了监督微调，并使用人类反馈的强化学习进行了训练。</td>
          <td><a href="https://github.com/WangRongsheng/IvyGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/WangRongsheng/IvyGPT</a></td>
      </tr>
      <tr>
          <td>PULSE</td>
          <td>专业</td>
          <td>该项目开源了中文医疗大模型PULSE，该模型使用约4,000,000个中文医学领域和通用领域的指令微调数据进行微调，支持医学领域的各种自然语言处理任务，包括健康教育、医师考试问题、报告解读、医疗记录结构化以及模拟诊断和治疗。</td>
          <td><a href="https://github.com/openmedlab/PULSE"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/openmedlab/PULSE</a></td>
      </tr>
      <tr>
          <td>HuangDI</td>
          <td>专业</td>
          <td>该项目开源了中医大模型HuangDI (皇帝)，该模型首先在Ziya-LLaMA-13B-V1基座模型的基础上加入中医教材、中医各类网站数据等语料库，训练出一个具有中医知识理解力的预训练模型，之后在此基础上通过海量的中医古籍指令对话数据及通用指令数据进行有监督微调，使得模型具备中医古籍知识问答能力。</td>
          <td><a href="https://github.com/Zlasejd/HuangDI"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Zlasejd/HuangDI</a></td>
      </tr>
      <tr>
          <td>CMLM-ZhongJing</td>
          <td>专业</td>
          <td>该项目开源了中医大模型ZhongJing (仲景)，该模型旨在阐明中医博大精深之知识，传承古代智慧与现代技术创新，最终为医学领域提供可信赖和专业的工具。</td>
          <td><a href="https://github.com/pariskang/CMLM-ZhongJing"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/pariskang/CMLM-ZhongJing</a></td>
      </tr>
      <tr>
          <td>TCMLLM</td>
          <td>专业</td>
          <td>该项目拟通过大模型方式实现中医临床辅助诊疗（病证诊断、处方推荐等）中医药知识问答等任务，推动中医知识问答、临床辅助诊疗等领域的快速发展。目前针对中医临床智能诊疗问题中的处方推荐任务，发布了中医处方推荐大模型TCMLLM-PR，通过整合真实世界临床病历、医学典籍与中医教科书等数据，构建了包含68k数据条目的处方推荐指令微调数据集，在ChatGLM大模型上进行微调得到。</td>
          <td><a href="https://github.com/2020MEAI/TCMLLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/2020MEAI/TCMLLM</a></td>
      </tr>
  </tbody>
</table>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">Public - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/"></a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">Theme FixIt works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="preload" href="/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-desktop":"Med-Eval 医疗大语言模型测评基准","typeit-header-title-mobile":"Med-Eval 医疗大语言模型测评基准"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-desktop":["typeit-header-desktop"],"typeit-header-title-mobile":["typeit-header-title-mobile"]},"duration":-1,"loop":false,"speed":100}};</script><script src="/js/theme.min.js" defer></script></body>
</html>
