<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>医学大模型榜单 - Med-Eval</title><meta name="author" content="">
<meta name="description" content=" 模型 所在机构 发布时间 开源地址 所用数据 Med-Flamingo 一种适用于医学领域的多模态少样本学习器 美国斯坦福大学- -基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集 BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型 美国斯坦福大学-基础模型研究中心CRFM 2022年12月 https://github.com/stanford-crfm/BioMedLM -基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合 BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型 微软 https://github.com/microsoft/BioGPT -GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类 Med-PaLM2 5400亿参数的转换器语言模型 谷歌 文心一言 百度 2023年2月 对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合 BioMedGPT-1.6B 生物医药领域基础模型 清华大学-智能产业研究院 2023年4月19日 -把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言 OpenBioMed 清华大学-智能产业研究院 2023年8月14日 https://github.com/BioFM/OpenBioMed -基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与S2ORC语料库中的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 本草Huatuo 哈尔滨工业大学 2023年3月31日 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集&#43;医学文献和GPT3.5API构建多轮问答数据 春雨慧问 基于大模型的AI在线问诊产品 春雨医生 2023年4月26日 medGPT 国内首款大模型驱动的 AI 医生 医联 2023年4月28日 -收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习 ChatGLM-6B-Med https://github.com/SCIR-HI/Med-ChatGLM -医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调 MedPalm Google -在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA ChatDoctor 德克萨斯大学 https://github.com/Kent0n-Li/ChatDoctor -基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本&#43;5KChatGPT生成数据进行指令微调 Chinese-vicuna-med https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md) Chinese-vicuna在cMedQA2数据上微调 OpenBioMed https://github.com/PharMolix/OpenBioMed 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 DoctorGLM 基于chatGLM6B模型的医学垂直领域模型 上海科技大学 https://github.com/xionghonglin/DoctorGLM ChatDoctor&#43;MedDialog&#43;CMD 多轮对话&#43;单轮指令样本微调GLM MedicalGPT-zh 上海交通大学 https://github.com/MediaBrain-SJTU/MedicalGPT-zh -基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA&#43;16个情境下SELF构建情景对话 PMC-LLaMA https://github.com/chaoyi-wu/PMC-LLaMA 医疗论文微调Llama NHS-LLM https://github.com/CogStack/OpenGPT/tree/main Chatgpt生成的医疗问答，对话，微调模型 Med-ChatGLM https://github.com/SCIR-HI/Med-ChatGLM 医学知识图谱和chatgpt构建中文医学指令数据集&#43;医学文献和chatgpt构建多轮问答数据 网新启真13B 浙江大学 https://github.com/CMKRG/QiZhenGPT -基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集 BenTsao -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果 BianQue -经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调 ChatDD 对话式药物研发助手 水木分子 https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw ChatDD-FM 100B 全球首个千亿参数多模态生物医药对话大模型 BioMedGPT-10B 16亿参数的轻量级科研版基础模型 清华智能产业研究院（AIR） https://github.com/PharMolix/OpenBioMed 太一（Taiyi） 中英双语生物医学大模型 大连理工大学 https://github.com/DUTIR-BioNLP/Taiyi-LLM Almanac：临床医学检索增强语言模型 https://arxiv.org/abs/2303.01229 MedLLaMA https://huggingface.co/chaoyi-wu/MedLLaMA_13B DISC-MedLLM 复旦大学数据智能与社会计算实验室 https://github.com/FudanDISC/DISC-MedLLM Clinical Camel Vector 研究所（加拿大） https://huggingface.co/wanglab/ClinicalCamel-70B ">
  <meta itemprop="name" content="医学大模型榜单">
  <meta itemprop="description" content="模型 所在机构 发布时间 开源地址 所用数据 Med-Flamingo 一种适用于医学领域的多模态少样本学习器 美国斯坦福大学- -基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集 BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型 美国斯坦福大学-基础模型研究中心CRFM 2022年12月 https://github.com/stanford-crfm/BioMedLM -基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合 BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型 微软 https://github.com/microsoft/BioGPT -GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类 Med-PaLM2 5400亿参数的转换器语言模型 谷歌 文心一言 百度 2023年2月 对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合 BioMedGPT-1.6B 生物医药领域基础模型 清华大学-智能产业研究院 2023年4月19日 -把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言 OpenBioMed 清华大学-智能产业研究院 2023年8月14日 https://github.com/BioFM/OpenBioMed -基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与S2ORC语料库中的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 本草Huatuo 哈尔滨工业大学 2023年3月31日 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集&#43;医学文献和GPT3.5API构建多轮问答数据 春雨慧问 基于大模型的AI在线问诊产品 春雨医生 2023年4月26日 medGPT 国内首款大模型驱动的 AI 医生 医联 2023年4月28日 -收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习 ChatGLM-6B-Med https://github.com/SCIR-HI/Med-ChatGLM -医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调 MedPalm Google -在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA ChatDoctor 德克萨斯大学 https://github.com/Kent0n-Li/ChatDoctor -基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本&#43;5KChatGPT生成数据进行指令微调 Chinese-vicuna-med https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md) Chinese-vicuna在cMedQA2数据上微调 OpenBioMed https://github.com/PharMolix/OpenBioMed 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 DoctorGLM 基于chatGLM6B模型的医学垂直领域模型 上海科技大学 https://github.com/xionghonglin/DoctorGLM ChatDoctor&#43;MedDialog&#43;CMD 多轮对话&#43;单轮指令样本微调GLM MedicalGPT-zh 上海交通大学 https://github.com/MediaBrain-SJTU/MedicalGPT-zh -基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA&#43;16个情境下SELF构建情景对话 PMC-LLaMA https://github.com/chaoyi-wu/PMC-LLaMA 医疗论文微调Llama NHS-LLM https://github.com/CogStack/OpenGPT/tree/main Chatgpt生成的医疗问答，对话，微调模型 Med-ChatGLM https://github.com/SCIR-HI/Med-ChatGLM 医学知识图谱和chatgpt构建中文医学指令数据集&#43;医学文献和chatgpt构建多轮问答数据 网新启真13B 浙江大学 https://github.com/CMKRG/QiZhenGPT -基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集 BenTsao -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果 BianQue -经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调 ChatDD 对话式药物研发助手 水木分子 https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw ChatDD-FM 100B 全球首个千亿参数多模态生物医药对话大模型 BioMedGPT-10B 16亿参数的轻量级科研版基础模型 清华智能产业研究院（AIR） https://github.com/PharMolix/OpenBioMed 太一（Taiyi） 中英双语生物医学大模型 大连理工大学 https://github.com/DUTIR-BioNLP/Taiyi-LLM Almanac：临床医学检索增强语言模型 https://arxiv.org/abs/2303.01229 MedLLaMA https://huggingface.co/chaoyi-wu/MedLLaMA_13B DISC-MedLLM 复旦大学数据智能与社会计算实验室 https://github.com/FudanDISC/DISC-MedLLM Clinical Camel Vector 研究所（加拿大） https://huggingface.co/wanglab/ClinicalCamel-70B">
  <meta itemprop="datePublished" content="2023-12-20T19:37:38+08:00">
  <meta itemprop="dateModified" content="2023-12-20T19:37:38+08:00">
  <meta itemprop="wordCount" content="166"><meta property="og:url" content="https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/">
  <meta property="og:site_name" content="Med-Eval">
  <meta property="og:title" content="医学大模型榜单">
  <meta property="og:description" content="模型 所在机构 发布时间 开源地址 所用数据 Med-Flamingo 一种适用于医学领域的多模态少样本学习器 美国斯坦福大学- -基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集 BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型 美国斯坦福大学-基础模型研究中心CRFM 2022年12月 https://github.com/stanford-crfm/BioMedLM -基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合 BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型 微软 https://github.com/microsoft/BioGPT -GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类 Med-PaLM2 5400亿参数的转换器语言模型 谷歌 文心一言 百度 2023年2月 对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合 BioMedGPT-1.6B 生物医药领域基础模型 清华大学-智能产业研究院 2023年4月19日 -把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言 OpenBioMed 清华大学-智能产业研究院 2023年8月14日 https://github.com/BioFM/OpenBioMed -基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与S2ORC语料库中的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 本草Huatuo 哈尔滨工业大学 2023年3月31日 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集&#43;医学文献和GPT3.5API构建多轮问答数据 春雨慧问 基于大模型的AI在线问诊产品 春雨医生 2023年4月26日 medGPT 国内首款大模型驱动的 AI 医生 医联 2023年4月28日 -收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习 ChatGLM-6B-Med https://github.com/SCIR-HI/Med-ChatGLM -医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调 MedPalm Google -在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA ChatDoctor 德克萨斯大学 https://github.com/Kent0n-Li/ChatDoctor -基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本&#43;5KChatGPT生成数据进行指令微调 Chinese-vicuna-med https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md) Chinese-vicuna在cMedQA2数据上微调 OpenBioMed https://github.com/PharMolix/OpenBioMed 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 DoctorGLM 基于chatGLM6B模型的医学垂直领域模型 上海科技大学 https://github.com/xionghonglin/DoctorGLM ChatDoctor&#43;MedDialog&#43;CMD 多轮对话&#43;单轮指令样本微调GLM MedicalGPT-zh 上海交通大学 https://github.com/MediaBrain-SJTU/MedicalGPT-zh -基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA&#43;16个情境下SELF构建情景对话 PMC-LLaMA https://github.com/chaoyi-wu/PMC-LLaMA 医疗论文微调Llama NHS-LLM https://github.com/CogStack/OpenGPT/tree/main Chatgpt生成的医疗问答，对话，微调模型 Med-ChatGLM https://github.com/SCIR-HI/Med-ChatGLM 医学知识图谱和chatgpt构建中文医学指令数据集&#43;医学文献和chatgpt构建多轮问答数据 网新启真13B 浙江大学 https://github.com/CMKRG/QiZhenGPT -基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集 BenTsao -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果 BianQue -经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调 ChatDD 对话式药物研发助手 水木分子 https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw ChatDD-FM 100B 全球首个千亿参数多模态生物医药对话大模型 BioMedGPT-10B 16亿参数的轻量级科研版基础模型 清华智能产业研究院（AIR） https://github.com/PharMolix/OpenBioMed 太一（Taiyi） 中英双语生物医学大模型 大连理工大学 https://github.com/DUTIR-BioNLP/Taiyi-LLM Almanac：临床医学检索增强语言模型 https://arxiv.org/abs/2303.01229 MedLLaMA https://huggingface.co/chaoyi-wu/MedLLaMA_13B DISC-MedLLM 复旦大学数据智能与社会计算实验室 https://github.com/FudanDISC/DISC-MedLLM Clinical Camel Vector 研究所（加拿大） https://huggingface.co/wanglab/ClinicalCamel-70B">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="model">
    <meta property="article:published_time" content="2023-12-20T19:37:38+08:00">
    <meta property="article:modified_time" content="2023-12-20T19:37:38+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="医学大模型榜单">
  <meta name="twitter:description" content="模型 所在机构 发布时间 开源地址 所用数据 Med-Flamingo 一种适用于医学领域的多模态少样本学习器 美国斯坦福大学- -基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集 BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型 美国斯坦福大学-基础模型研究中心CRFM 2022年12月 https://github.com/stanford-crfm/BioMedLM -基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合 BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型 微软 https://github.com/microsoft/BioGPT -GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类 Med-PaLM2 5400亿参数的转换器语言模型 谷歌 文心一言 百度 2023年2月 对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合 BioMedGPT-1.6B 生物医药领域基础模型 清华大学-智能产业研究院 2023年4月19日 -把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言 OpenBioMed 清华大学-智能产业研究院 2023年8月14日 https://github.com/BioFM/OpenBioMed -基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与S2ORC语料库中的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 本草Huatuo 哈尔滨工业大学 2023年3月31日 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集&#43;医学文献和GPT3.5API构建多轮问答数据 春雨慧问 基于大模型的AI在线问诊产品 春雨医生 2023年4月26日 medGPT 国内首款大模型驱动的 AI 医生 医联 2023年4月28日 -收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习 ChatGLM-6B-Med https://github.com/SCIR-HI/Med-ChatGLM -医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调 MedPalm Google -在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA ChatDoctor 德克萨斯大学 https://github.com/Kent0n-Li/ChatDoctor -基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本&#43;5KChatGPT生成数据进行指令微调 Chinese-vicuna-med https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md) Chinese-vicuna在cMedQA2数据上微调 OpenBioMed https://github.com/PharMolix/OpenBioMed 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 DoctorGLM 基于chatGLM6B模型的医学垂直领域模型 上海科技大学 https://github.com/xionghonglin/DoctorGLM ChatDoctor&#43;MedDialog&#43;CMD 多轮对话&#43;单轮指令样本微调GLM MedicalGPT-zh 上海交通大学 https://github.com/MediaBrain-SJTU/MedicalGPT-zh -基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA&#43;16个情境下SELF构建情景对话 PMC-LLaMA https://github.com/chaoyi-wu/PMC-LLaMA 医疗论文微调Llama NHS-LLM https://github.com/CogStack/OpenGPT/tree/main Chatgpt生成的医疗问答，对话，微调模型 Med-ChatGLM https://github.com/SCIR-HI/Med-ChatGLM 医学知识图谱和chatgpt构建中文医学指令数据集&#43;医学文献和chatgpt构建多轮问答数据 网新启真13B 浙江大学 https://github.com/CMKRG/QiZhenGPT -基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集 BenTsao -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果 BianQue -经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调 ChatDD 对话式药物研发助手 水木分子 https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw ChatDD-FM 100B 全球首个千亿参数多模态生物医药对话大模型 BioMedGPT-10B 16亿参数的轻量级科研版基础模型 清华智能产业研究院（AIR） https://github.com/PharMolix/OpenBioMed 太一（Taiyi） 中英双语生物医学大模型 大连理工大学 https://github.com/DUTIR-BioNLP/Taiyi-LLM Almanac：临床医学检索增强语言模型 https://arxiv.org/abs/2303.01229 MedLLaMA https://huggingface.co/chaoyi-wu/MedLLaMA_13B DISC-MedLLM 复旦大学数据智能与社会计算实验室 https://github.com/FudanDISC/DISC-MedLLM Clinical Camel Vector 研究所（加拿大） https://huggingface.co/wanglab/ClinicalCamel-70B">
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" type="text/html" href="https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/" title="医学大模型榜单 - Med-Eval" /><link rel="prev" type="text/html" href="https://dujh22.github.io/more/" title="加入竞技" /><link rel="next" type="text/html" href="https://dujh22.github.io/board/" title="Med-Eval 榜单" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "医学大模型榜单",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/dujh22.github.io\/model\/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95\/"
    },"genre": "model","wordcount":  166 ,
    "url": "https:\/\/dujh22.github.io\/model\/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95\/","datePublished": "2023-12-20T19:37:38+08:00","dateModified": "2023-12-20T19:37:38+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script><script src="/js/head/color-scheme.min.js"></script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="Med-Eval"><span class="header-title-text">Med-Eval</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> Archives</a></li><li class="menu-item">
              <a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> Categories</a></li><li class="menu-item">
              <a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> Tags</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="Med-Eval"><span class="header-title-text">Med-Eval</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="menu-item"><a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> Archives</a></li><li class="menu-item"><a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> Categories</a></li><li class="menu-item"><a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> Tags</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><article class="page single special">
    <div class="header"><h1 class="single-title animate__animated animate__pulse animate__faster">医学大模型榜单</h1></div><div class="content" id="content"><table>
  <thead>
      <tr>
          <th style="text-align: left">模型</th>
          <th>所在机构</th>
          <th>发布时间</th>
          <th>开源地址</th>
          <th>所用数据</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Med-Flamingo 一种适用于医学领域的多模态少样本学习器</td>
          <td>美国斯坦福大学-</td>
          <td></td>
          <td></td>
          <td>-基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集</td>
      </tr>
      <tr>
          <td style="text-align: left">BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型</td>
          <td>美国斯坦福大学-基础模型研究中心CRFM</td>
          <td>2022年12月</td>
          <td><a href="https://github.com/stanford-crfm/BioMedLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/stanford-crfm/BioMedLM</a></td>
          <td>-基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合</td>
      </tr>
      <tr>
          <td style="text-align: left">BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型</td>
          <td>微软</td>
          <td></td>
          <td><a href="https://github.com/microsoft/BioGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/microsoft/BioGPT</a></td>
          <td>-GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类</td>
      </tr>
      <tr>
          <td style="text-align: left">Med-PaLM2 5400亿参数的转换器语言模型</td>
          <td>谷歌</td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">文心一言</td>
          <td>百度</td>
          <td>2023年2月</td>
          <td></td>
          <td>对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合</td>
      </tr>
      <tr>
          <td style="text-align: left">BioMedGPT-1.6B 生物医药领域基础模型</td>
          <td>清华大学-智能产业研究院</td>
          <td>2023年4月19日</td>
          <td></td>
          <td>-把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言</td>
      </tr>
      <tr>
          <td style="text-align: left">OpenBioMed</td>
          <td>清华大学-智能产业研究院</td>
          <td>2023年8月14日</td>
          <td><a href="https://github.com/BioFM/OpenBioMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/BioFM/OpenBioMed</a></td>
          <td>-基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与<a href="https://github.com/allenai/s2orc/blob/master/README.md"target="_blank" rel="external nofollow noopener noreferrer">S2ORC语料库中</a>的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20+生物研究领域多模态预训练模型</td>
      </tr>
      <tr>
          <td style="text-align: left">本草Huatuo</td>
          <td>哈尔滨工业大学</td>
          <td>2023年3月31日</td>
          <td><a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese</a></td>
          <td>-经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集+医学文献和GPT3.5API构建多轮问答数据</td>
      </tr>
      <tr>
          <td style="text-align: left">春雨慧问 基于大模型的AI在线问诊产品</td>
          <td>春雨医生</td>
          <td>2023年4月26日</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">medGPT 国内首款大模型驱动的 AI 医生</td>
          <td>医联</td>
          <td>2023年4月28日</td>
          <td></td>
          <td>-收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习</td>
      </tr>
      <tr>
          <td style="text-align: left"><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">ChatGLM-6B-Med</a></td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM</a></td>
          <td>-医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调</td>
      </tr>
      <tr>
          <td style="text-align: left">MedPalm</td>
          <td>Google</td>
          <td></td>
          <td></td>
          <td>-在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA</td>
      </tr>
      <tr>
          <td style="text-align: left">ChatDoctor</td>
          <td>德克萨斯大学</td>
          <td></td>
          <td><a href="https://github.com/Kent0n-Li/ChatDoctor"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Kent0n-Li/ChatDoctor</a></td>
          <td>-基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本+5KChatGPT生成数据进行指令微调</td>
      </tr>
      <tr>
          <td style="text-align: left">Chinese-vicuna-med</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md</a>)</td>
          <td>Chinese-vicuna在cMedQA2数据上微调</td>
      </tr>
      <tr>
          <td style="text-align: left">OpenBioMed</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/PharMolix/OpenBioMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/PharMolix/OpenBioMed</a></td>
          <td>知识图谱&amp;20+生物研究领域多模态预训练模型</td>
      </tr>
      <tr>
          <td style="text-align: left">DoctorGLM 基于chatGLM6B模型的医学垂直领域模型</td>
          <td>上海科技大学</td>
          <td></td>
          <td><a href="https://github.com/xionghonglin/DoctorGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xionghonglin/DoctorGLM</a></td>
          <td>ChatDoctor+MedDialog+CMD 多轮对话+单轮指令样本微调GLM</td>
      </tr>
      <tr>
          <td style="text-align: left">MedicalGPT-zh</td>
          <td>上海交通大学</td>
          <td></td>
          <td><a href="https://github.com/MediaBrain-SJTU/MedicalGPT-zh"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/MediaBrain-SJTU/MedicalGPT-zh</a></td>
          <td>-基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA+16个情境下SELF构建情景对话</td>
      </tr>
      <tr>
          <td style="text-align: left">PMC-LLaMA</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/chaoyi-wu/PMC-LLaMA"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/chaoyi-wu/PMC-LLaMA</a></td>
          <td>医疗论文微调Llama</td>
      </tr>
      <tr>
          <td style="text-align: left">NHS-LLM</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/CogStack/OpenGPT/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CogStack/OpenGPT/tree/main</a></td>
          <td>Chatgpt生成的医疗问答，对话，微调模型</td>
      </tr>
      <tr>
          <td style="text-align: left">Med-ChatGLM</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM</a></td>
          <td>医学知识图谱和chatgpt构建中文医学指令数据集+医学文献和chatgpt构建多轮问答数据</td>
      </tr>
      <tr>
          <td style="text-align: left">网新启真13B</td>
          <td>浙江大学</td>
          <td></td>
          <td><a href="https://github.com/CMKRG/QiZhenGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CMKRG/QiZhenGPT</a></td>
          <td>-基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集</td>
      </tr>
      <tr>
          <td style="text-align: left">BenTsao</td>
          <td></td>
          <td></td>
          <td></td>
          <td>-经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果</td>
      </tr>
      <tr>
          <td style="text-align: left">BianQue</td>
          <td></td>
          <td></td>
          <td></td>
          <td>-经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调</td>
      </tr>
      <tr>
          <td style="text-align: left">ChatDD 对话式药物研发助手</td>
          <td>水木分子</td>
          <td></td>
          <td><a href="https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">ChatDD-FM 100B 全球首个千亿参数多模态生物医药对话大模型</td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">BioMedGPT-10B 16亿参数的轻量级科研版基础模型</td>
          <td>清华智能产业研究院（AIR）</td>
          <td></td>
          <td><a href="https://github.com/PharMolix/OpenBioMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/PharMolix/OpenBioMed</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">太一（Taiyi） 中英双语生物医学大模型</td>
          <td>大连理工大学</td>
          <td></td>
          <td><a href="https://github.com/DUTIR-BioNLP/Taiyi-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DUTIR-BioNLP/Taiyi-LLM</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Almanac：临床医学检索增强语言模型</td>
          <td></td>
          <td></td>
          <td><a href="https://arxiv.org/abs/2303.01229"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2303.01229</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">MedLLaMA</td>
          <td></td>
          <td></td>
          <td><a href="https://huggingface.co/chaoyi-wu/MedLLaMA_13B"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/chaoyi-wu/MedLLaMA_13B</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">DISC-MedLLM</td>
          <td><a href="http://fudan-disc.com/"target="_blank" rel="external nofollow noopener noreferrer">复旦大学数据智能与社会计算实验室</a></td>
          <td></td>
          <td><a href="https://github.com/FudanDISC/DISC-MedLLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FudanDISC/DISC-MedLLM</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Clinical Camel</td>
          <td>Vector 研究所（加拿大）</td>
          <td></td>
          <td><a href="https://huggingface.co/wanglab/ClinicalCamel-70B"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/wanglab/ClinicalCamel-70B</a></td>
          <td></td>
      </tr>
  </tbody>
</table>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.148.2"><img class="hugo-icon" src="/images/hugo.min.svg" alt="Hugo logo" /> Hugo</a> | Theme - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.4.0-alpha-20250721024521-a1cd700b"><img class="fixit-icon" src="/images/fixit.min.svg" alt="FixIt logo" /> FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/"></a></span></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">This website works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"version":"v0.4.0-alpha-20250721024521-a1cd700b"};</script><script src="/js/theme.min.js" defer></script></body>
</html>
