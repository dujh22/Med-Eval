<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Models on Med-Eval</title>
    <link>https://dujh22.github.io/model/</link>
    <description>Recent content in Models on Med-Eval</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 03 Jan 2024 18:14:28 +0800</lastBuildDate>
    <atom:link href="https://dujh22.github.io/model/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Med Eval测评结果</title>
      <link>https://dujh22.github.io/model/med-eval%E6%B5%8B%E8%AF%84%E7%BB%93%E6%9E%9C/</link>
      <pubDate>Wed, 03 Jan 2024 18:14:28 +0800</pubDate>
      <guid>https://dujh22.github.io/model/med-eval%E6%B5%8B%E8%AF%84%E7%BB%93%E6%9E%9C/</guid>
      <description>&lt;h2 id=&#34;med-eval-chinese-doctor&#34;&gt;Med-Eval-Chinese Doctor&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;基座&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;数据&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;得分&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Doctor&lt;/td&gt;&#xA;          &lt;td&gt;Baichuan2-13B-Chat&lt;/td&gt;&#xA;          &lt;td&gt;2.23125GB PT数据集epoch=1.0,loss=2.1348 &lt;!-- raw HTML omitted --&gt;60.06MB SFT数据epoch=2.5，loss~2.00&lt;/td&gt;&#xA;          &lt;td&gt;32.22% 31.82% 31.86%&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;AiMed&lt;/td&gt;&#xA;          &lt;td&gt;Baichuan2-13B-Chat&lt;/td&gt;&#xA;          &lt;td&gt;215.3MB PT数据集 epoch=1.0,loss=2.332  &lt;!-- raw HTML omitted --&gt;48.03MB SFT数据epoch=4.0,loss~1.900&lt;/td&gt;&#xA;          &lt;td&gt;23.43% 23.79% 23.40%&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Baichuan2-13B-Chat&lt;/td&gt;&#xA;          &lt;td&gt;-&lt;/td&gt;&#xA;          &lt;td&gt;-&lt;/td&gt;&#xA;          &lt;td&gt;13.64% 13.57% 13.53%&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;带有prompt工程后：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;指标&lt;/th&gt;&#xA;          &lt;th&gt;指标含义&lt;/th&gt;&#xA;          &lt;th&gt;Doctor&lt;/th&gt;&#xA;          &lt;th&gt;AiMed&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;total_questions&lt;/td&gt;&#xA;          &lt;td&gt;总问题数&lt;/td&gt;&#xA;          &lt;td&gt;2773&lt;/td&gt;&#xA;          &lt;td&gt;2773&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;strict_correct_answers&lt;/td&gt;&#xA;          &lt;td&gt;严格得分&lt;/td&gt;&#xA;          &lt;td&gt;1087&lt;/td&gt;&#xA;          &lt;td&gt;786&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Model Strict Accuracy&lt;/td&gt;&#xA;          &lt;td&gt;严格得分率&lt;/td&gt;&#xA;          &lt;td&gt;39.20%&lt;/td&gt;&#xA;          &lt;td&gt;28.34%&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;loose_correct_answers&lt;/td&gt;&#xA;          &lt;td&gt;绝对宽松得分&lt;/td&gt;&#xA;          &lt;td&gt;1583&lt;/td&gt;&#xA;          &lt;td&gt;1107&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Model Loose Accuracy&lt;/td&gt;&#xA;          &lt;td&gt;绝对宽松得分率&lt;/td&gt;&#xA;          &lt;td&gt;57.09%&lt;/td&gt;&#xA;          &lt;td&gt;39.92%&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;loose_correct_answers2&lt;/td&gt;&#xA;          &lt;td&gt;相对宽松得分&lt;/td&gt;&#xA;          &lt;td&gt;1594&lt;/td&gt;&#xA;          &lt;td&gt;1177&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Model Loose Accuracy2&lt;/td&gt;&#xA;          &lt;td&gt;相对宽松得分率&lt;/td&gt;&#xA;          &lt;td&gt;57.48%&lt;/td&gt;&#xA;          &lt;td&gt;42.45%&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;error_answers&lt;/td&gt;&#xA;          &lt;td&gt;错误&lt;/td&gt;&#xA;          &lt;td&gt;1179&lt;/td&gt;&#xA;          &lt;td&gt;1596&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Model Error&lt;/td&gt;&#xA;          &lt;td&gt;错误率&lt;/td&gt;&#xA;          &lt;td&gt;42.52%&lt;/td&gt;&#xA;          &lt;td&gt;57.55%&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>MedRad具体场景和代码</title>
      <link>https://dujh22.github.io/model/medrad%E5%85%B7%E4%BD%93%E5%9C%BA%E6%99%AF%E5%92%8C%E4%BB%A3%E7%A0%81/</link>
      <pubDate>Wed, 03 Jan 2024 12:22:42 +0800</pubDate>
      <guid>https://dujh22.github.io/model/medrad%E5%85%B7%E4%BD%93%E5%9C%BA%E6%99%AF%E5%92%8C%E4%BB%A3%E7%A0%81/</guid>
      <description>&lt;h2 id=&#34;低复杂度医学知识问答&#34;&gt;低复杂度：医学知识问答&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-场景示例&#34;&gt;1. 场景示例&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34; id=&#34;id-1&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;QU&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;以下都是治疗病态肥胖的手术选择，除了-&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;OP&amp;#34;&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;可调节胃束带&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;B&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;胆胰分流&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;十二指肠开关&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;D&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Roux en y十二指肠旁路&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        },&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;EXP&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ans.是“d”，即Roux en Y十二指肠旁路减肥手术程序包括：a。垂直带状腹足虫。可调节胃束带。Roux-en-Y胃旁路术（非Roux-en-Y-十二指肠旁路术）d。胆胰分流。十二指肠切除术病态肥胖的外科治疗被称为减肥手术。病态肥胖被定义为体重指数为35 kg/m2或以上，伴有肥胖相关的合并症，或BMI为40 kg/m2或更大，没有合并症。减肥手术导致体重减轻是由两个因素造成的。一是限制饮食。另一种是摄入食物的吸收不良。o胃限制性手术包括垂直带状胃成形术和可调节胃束带术o吸收不良手术包括胆胰分流，十二指肠切换器Roux-en-Y胃旁路既有限制性又有吸收不良的特点减肥手术：作用机制限制性垂直胃束带腹腔镜可调节胃束带大范围限制性/轻度吸收不良Roux-en-Y胃旁路大范围吸收不良/轻度限制性胆胰分流&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;AN&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;D&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;2-场景说明&#34;&gt;2. 场景说明&lt;/h3&gt;&#xA;&lt;p&gt;这个例子是一个医学知识问答场景，其中包含一个多项选择题，目的是识别出不属于治疗病态肥胖手术选择的选项。这种类型的问题通常要求对特定医学领域有深入了解。要处理这类场景，可以从RAG (Retrieval-Augmented Generation) 结合CoT (Chain of Thought) 和Agent的角度来考虑。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;信息检索（RAG）&lt;/strong&gt;: 首先使用信息检索技术从大量数据中找到与问题相关的信息。这一步骤的目的是快速定位与问题相关的医学知识和数据。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;思维链（CoT）&lt;/strong&gt;: 接下来，通过构建思维链来分析问题。这包括理解问题的具体要求，解析各个选项，并将其与检索到的信息相对比。这一步骤有助于深入理解问题的背景和每个选项的含义。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;智能代理（Agent）&lt;/strong&gt;: 最后，结合Agent的能力，对上述两步骤的结果进行综合分析，并得出最终答案。Agent在这里起到的是决策和验证的作用，确保答案的准确性和逻辑性。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;对于这个特定的例子，Agent首先需要识别问题是关于病态肥胖的治疗手术，然后通过RAG检索相关的医学信息，比如各种手术的类型和特点。通过CoT分析每个选项，理解它们各自的手术原理，并确定哪个选项不属于治疗病态肥胖的手术。最后，Agent将结合这些信息来验证并确定正确的答案。&lt;/p&gt;&#xA;&lt;h3 id=&#34;3-medrad实现&#34;&gt;3. MedRad实现&lt;/h3&gt;&#xA;&lt;p&gt;医学知识问答系统的工作步骤：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;问题识别&lt;/strong&gt;：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Agent接收到用户的查询（医学问题）后，首先进行问题识别。这个过程可能包括关键词提取、问题类型分类（例如症状查询、药物信息或疾病相关知识）以及紧急度评估。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;信息检索&lt;/strong&gt;：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Agent使用先进的信息检索技术在大量医学数据中找到与问题最相关的信息。这些数据可能来自医学数据库、研究论文、临床指南或其他医学知识库。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;选项解析与对比&lt;/strong&gt;：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于问题的每个可能选项（A、B、C、D），Agent分别进行解析。这个解析可能包括对医学术语的理解、病理机制、治疗方案或药物作用的解释等。&lt;/li&gt;&#xA;&lt;li&gt;Agent将每个选项与检索到的相关信息进行对比，评估其准确性、相关性和可靠性。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;综合分析&lt;/strong&gt;：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Agent对上述每个步骤的结果进行综合分析。这可能涉及比较不同选项的支持证据，考虑其医学逻辑和临床实践的一致性。&lt;/li&gt;&#xA;&lt;li&gt;分析可能还包括检查信息源的可信度、最新度以及与现有医学共识的一致性。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;得出结论&lt;/strong&gt;：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;经过综合分析后，Agent得出最终答案。这个答案是基于Agent对各个选项评估的结果，选择最佳的、最符合医学证据的选项。&lt;/li&gt;&#xA;&lt;li&gt;最终答案会反馈给用户，并提供相应的解释，解释可能包括答案的依据、相关的医学信息以及如果可行的话，进一步的建议或指示。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;这个流程涵盖了从接收用户查询到提供经过深思熟虑的、信息支持的答案的整个过程。在实际应用中，Agent还会包括额外的功能，如与用户的交互能力、处理模糊或不完整查询的能力，以及持续学习和适应新信息的能力。&lt;/p&gt;&#xA;&lt;p&gt;以下是该功能实现的伪代码：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34; id=&#34;id-2&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;function handle_medical_query(query)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    agent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; initialize_agent()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 使用信息检索技术检索相关信息&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    related_information &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; retrieve_information(query)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 解析每个选项，并与检索到的信息进行比较&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    option_A_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; agent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;analyze_option(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;, related_information)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    option_B_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; agent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;analyze_option(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;B&amp;#34;&lt;/span&gt;, related_information)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    option_C_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; agent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;analyze_option(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;, related_information)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    option_D_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; agent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;analyze_option(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;D&amp;#34;&lt;/span&gt;, related_information)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 综合分析每个选项的结果&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    final_answer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; agent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;conclude_answer(option_A_result, option_B_result, option_C_result, option_D_result)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; final_answer&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;function retrieve_information(query)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 实现具体的信息检索逻辑&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    related_information &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; search_database(query)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; related_information&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;function analyze_option(option, information)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 解析选项，并与信息进行比较&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    analysis_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; compare_option_with_information(option, information)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; analysis_result&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;function conclude_answer(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;options_results)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 对比各个选项的分析结果，得出最终答案&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    best_option &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; select_best_option_based_on_results(options_results)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; best_option&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;中复杂度门诊医患对话&#34;&gt;中复杂度：门诊医患对话&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-场景示例-1&#34;&gt;1. 场景示例&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34; id=&#34;id-3&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;input&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;医生，昨天我的右侧突然感到虚弱和麻木。它持续了几分钟，然后就消失了。我也有一段时间难以说话。你能帮我弄清楚发生了什么事吗？&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;instruction&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;如果你是一名医生，请根据患者的描述回答医学问题。&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;output&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;你可能经历了短暂的脑缺血发作。我想做一些医学检查来确认诊断结果。我们需要从放射学成像程序、血液学测试（血液测试）、全血细胞计数（CBC）、X射线计算机断层扫描（扫描CT）、心电图、肾功能测试（肾功能测试）和葡萄糖测量（葡萄糖水平）开始，以更好地了解发生了什么。我们能为您安排这些测试吗？&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;2-场景说明-1&#34;&gt;2. 场景说明&lt;/h3&gt;&#xA;&lt;p&gt;门诊医患对话场景通常涉及根据患者的描述来诊断和建议后续的医疗步骤。处理这类场景的一般范式可以从 RAG 结合 CoT 和 Agent 的角度来构建：&lt;/p&gt;</description>
    </item>
    <item>
      <title>MedRad:一个医学大模型的可靠辅助决策框架</title>
      <link>https://dujh22.github.io/model/medrad%E4%B8%80%E4%B8%AA%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E9%9D%A0%E8%BE%85%E5%8A%A9%E5%86%B3%E7%AD%96%E6%A1%86%E6%9E%B6/</link>
      <pubDate>Wed, 03 Jan 2024 12:22:31 +0800</pubDate>
      <guid>https://dujh22.github.io/model/medrad%E4%B8%80%E4%B8%AA%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E9%9D%A0%E8%BE%85%E5%8A%A9%E5%86%B3%E7%AD%96%E6%A1%86%E6%9E%B6/</guid>
      <description>&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;: 随着医学领域数据的激增和临床决策的复杂性增加，需要强大的计算工具来辅助医生作出准确、可靠的决策。虽然现有研究已经在准确性上取得进展，但大模型在可靠性和稳定性方面的不足显而易见。为此，我们提出了MedRad框架，一个结合深度知识工程、Chain of Thought (CoT) 推理、Retrieval-Augmented Generation (RAG) 技术和智能代理（Agent）的系统，以提高决策的可靠性。本研究的早期阶段集中于构建医学领域的大型语言模型和通用模型训练算法。进一步，我们将重点转向如何在医学知识问答、门诊对话和临床病历诊断等具有不同复杂度的场景中利用这些技术实现高可靠性的医学决策。实验结果显示，MedRad在上述场景中能够提供高质量的决策路径。我们的框架通过与基座模型的松耦合设计，展现了在不同医学场景中的强大适应性。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;关键词&lt;/strong&gt;: 医学大模型、决策支持系统、可靠性、Chain of Thought、Retrieval-Augmented Generation、Agent&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-引言&#34;&gt;&lt;strong&gt;1. 引言&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;在医学领域，高质量的决策是病人健康和生命的守护者。医生们经常面临着需要在短时间内处理大量信息并做出关键决策的压力。这种决策的复杂性由于医疗数据的海量增长和临床案例的多变性而不断增加。传统的决策支持系统，在处理标准化流程时效果显著，但在解读非结构化的医疗数据、处理复杂病例以及适应不断演变的医学知识面前则显得力不从心。这些系统在准确解释临床数据、适应新的治疗协议以及应对罕见病症方面，常常表现出灵活性不足和可靠性不高的问题。&lt;/p&gt;&#xA;&lt;p&gt;随着人工智能的迅速发展，机器学习和大数据分析已被引入到决策支持工具中，以增强它们处理复杂数据的能力。这些技术在提升诊断精度和治疗效率方面取得了一定的成就，尤其是在图像识别和预测建模领域。然而，仅有的准确性并不足以应对医学领域的全部挑战。现有的人工智能系统通常是针对特定任务设计的，缺乏足够的灵活性来适应医学领域不断变化的需求和复杂的临床场景。此外，这些系统在解释能力和决策透明度方面往往存在不足，这限制了它们在临床实践中的应用，因为医疗专业人员和患者都需要理解决策背后的逻辑。此外，随着医疗实践越来越依赖数据驱动的方法，医生和医疗专业人员的角色正在发生变化，他们需要能够信任并理解这些工具提供的信息和建议。&lt;/p&gt;&#xA;&lt;p&gt;正是基于这些挑战，我们开发了MedRad框架。MedRad不仅着重于提升决策的准确性，而且特别强调在保持这种准确性的同时增强决策的可靠性和稳定性。通过结合最新的人工智能技术，如深度学习、自然语言处理，和医学知识工程，MedRad旨在创建一个更加智能和适应性强的决策支持系统。该系统不仅能够处理和分析复杂的医疗数据，还能在提供决策时给出清晰、可解释的逻辑路径，使医生能够更好地理解和信任其提供的建议。通过这种方式，MedRad框架寻求成为医疗专业人员的一个真正的合作伙伴，支持他们在日益复杂的医疗环境中做出更好的决策。&lt;/p&gt;&#xA;&lt;h2 id=&#34;2-相关工作&#34;&gt;&lt;strong&gt;2. 相关工作&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;本节回顾了医学决策支持系统的发展历程，以及大模型在医学领域的应用现状。我们将探讨现有模型在处理医学数据时的局限性，尤其是在稳定性和可靠性方面的不足，并分析了Chain of Thought (CoT) 推理和Retrieval-Augmented Generation (RAG) 技术对于提高医学决策质量的潜在贡献。&lt;/p&gt;&#xA;&lt;h3 id=&#34;21-医学决策支持系统的发展&#34;&gt;2.1 &lt;strong&gt;医学决策支持系统的发展&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;医学决策支持系统（CDSS）的历史可以追溯到20世纪70年代，当时的系统主要基于规则的推理来辅助临床决策。随着时间的推移，这些系统逐渐集成了更复杂的算法，包括基于知识的系统和基于数据的机器学习方法。在21世纪初，随着计算能力的提升和电子健康记录（EHR）的普及，CDSS开始利用大数据进行更复杂的预测和分析。尽管这些系统在处理标准化数据方面取得了显著进展，但在解释非结构化医疗数据、适应新的临床实践和医学知识时，它们仍显示出局限性。&lt;/p&gt;&#xA;&lt;h3 id=&#34;22-大模型在医学领域的应用&#34;&gt;2.2 &lt;strong&gt;大模型在医学领域的应用&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;近年来，随着深度学习技术的发展，大型预训练模型，如GPT和BERT，已经开始在医学领域中发挥作用。这些模型能够处理大量非结构化的医疗文本数据，提供更深入的见解和预测。例如，它们被用于自动化病历摘要、医学文献分析、疾病预测以及提供个性化的患者护理建议。然而，这些大型模型在处理高度专业化的医学知识时仍然面临挑战，尤其是在理解复杂医学术语和临床指南方面。&lt;/p&gt;&#xA;&lt;h3 id=&#34;23-医学决策的稳定性和可靠性&#34;&gt;2.3 &lt;strong&gt;医学决策的稳定性和可靠性&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;尽管大型模型在处理医学数据方面显示出潜力，但其在确保决策稳定性和可靠性方面存在不足。这主要是由于这些模型在训练过程中缺乏对医学领域特定知识的深入理解。此外，这些模型通常缺乏足够的透明度，使得医疗专业人员难以理解和信任其决策过程和结果。因此，需要开发更加先进的方法来提高这些系统在医学决策中的稳定性和可靠性。&lt;/p&gt;&#xA;&lt;h3 id=&#34;24-chain-of-thought-cot-推理和retrieval-augmented-generation-rag-技术&#34;&gt;2.4 &lt;strong&gt;Chain of Thought (CoT) 推理和Retrieval-Augmented Generation (RAG) 技术&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;CoT推理技术通过模拟医生的思考过程来增强决策的逻辑性和透明度。通过将决策过程分解为一系列逻辑步骤，CoT提供了一种更加直观和可解释的方式来解释模型的推理过程。另一方面，RAG技术通过结合强大的检索能力和生成能力来增强模型对医学知识的理解。这种方法允许模型在做出决策前，先从大量医学数据中检索相关信息，从而提高其决策的准确性和深度。结合这两种技术，可以显著提高医学决策支持系统的性能，尤其是在处理复杂和非标准化的临床情况时。&lt;/p&gt;&#xA;&lt;p&gt;综上所述，尽管当前的医学大模型在处理特定类型的医疗数据方面取得了进展，但在提高医学决策的稳定性和可靠性方面还有很长的路要走。&lt;/p&gt;&#xA;&lt;h2 id=&#34;3-medrad框架&#34;&gt;&lt;strong&gt;3. MedRad框架&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;本节详细介绍MedRad框架的架构和工作原理。我们将解释如何结合CoT、RAG和Agent技术来挖掘大型语言模型的潜力，并描述这一多模块系统是如何协同工作以提升决策质量的。&lt;/p&gt;&#xA;&lt;p&gt;框架图：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://gitee.com/dujh22/pic/raw/master/MedRad.png&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://gitee.com/dujh22/pic/raw/master/MedRad.png&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;详细见本目录下的：“MedRad具体场景和代码”&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;注意：由于本项目前一版本的论文还未完成：MedRad: 一个医学大模型的可靠辅助决策框架&lt;/p&gt;&#xA;&lt;p&gt;谷歌云盘链接： &lt;a href=&#34;https://drive.google.com/file/d/1fqFGu4y3uhspXWeRlqiCsBHsl_NjNaen/view?usp=sharing&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://drive.google.com/file/d/1fqFGu4y3uhspXWeRlqiCsBHsl_NjNaen/view?usp=sharing&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;百度网盘链接: &lt;a href=&#34;https://pan.baidu.com/s/1qeWx2RrwiErLc8vVXS0pBA?pwd=2024&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://pan.baidu.com/s/1qeWx2RrwiErLc8vVXS0pBA?pwd=2024&lt;/a&gt; 提取码: 2024&lt;/p&gt;</description>
    </item>
    <item>
      <title>医学大模型榜单V2</title>
      <link>https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95v2/</link>
      <pubDate>Wed, 03 Jan 2024 12:18:27 +0800</pubDate>
      <guid>https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95v2/</guid>
      <description>&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;模型名称&lt;/th&gt;&#xA;          &lt;th&gt;模型类型&lt;/th&gt;&#xA;          &lt;th&gt;支持语言&lt;/th&gt;&#xA;          &lt;th&gt;参数规模&lt;/th&gt;&#xA;          &lt;th&gt;发布机构&lt;/th&gt;&#xA;          &lt;th&gt;模型最后一次更新时间&lt;/th&gt;&#xA;          &lt;th&gt;简介&lt;/th&gt;&#xA;          &lt;th&gt;GitHub地址&lt;/th&gt;&#xA;          &lt;th&gt;Hugging Face 地址&lt;/th&gt;&#xA;          &lt;th&gt;论文地址&lt;/th&gt;&#xA;          &lt;th&gt;官网地址&lt;/th&gt;&#xA;          &lt;th&gt;其它信息来源&lt;/th&gt;&#xA;          &lt;th&gt;API 是否可用&lt;/th&gt;&#xA;          &lt;th&gt;API 使用地址&lt;/th&gt;&#xA;          &lt;th&gt;是否可以私有化部署&lt;/th&gt;&#xA;          &lt;th&gt;Demo 地址&lt;/th&gt;&#xA;          &lt;th&gt;上下文长度&lt;/th&gt;&#xA;          &lt;th&gt;训练用数据&lt;/th&gt;&#xA;          &lt;th&gt;训练基座&lt;/th&gt;&#xA;          &lt;th&gt;是否可商用&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;gpt-4-1106-preview&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;多语言&lt;/td&gt;&#xA;          &lt;td&gt;1.8T (未证实)&lt;/td&gt;&#xA;          &lt;td&gt;OpenAI&lt;/td&gt;&#xA;          &lt;td&gt;2023.11.6&lt;/td&gt;&#xA;          &lt;td&gt;GPT-4，或称Generative Pre-trained Transformer 4，是由OpenAI开发的最先进的语言模型。它的参数量大概是其前身GPT-3的10倍。这一进步使GPT-4能够以惊人的准确性和细微差别理解和生成类似人类的文本。它擅长各种任务，如回答问题、写文章、总结文本等等。它在不同数据集上进行大规模训练，能够深入理解多种语言和科目，使其成为人工智能驱动应用程序中的通用工具。&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://platform.openai.com/docs/overview&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://platform.openai.com/docs/overview&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;到处都有&lt;/td&gt;&#xA;          &lt;td&gt;付费，输入/输出 1k token 花费 0.01$/0.03$&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://platform.openai.com/docs/api-reference&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://chat.openai.com/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://chat.openai.com/&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;128K&lt;/td&gt;&#xA;          &lt;td&gt;未公布&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;gpt-3.5-turbo-1106&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;多语言&lt;/td&gt;&#xA;          &lt;td&gt;175B (未证实)&lt;/td&gt;&#xA;          &lt;td&gt;OpenAI&lt;/td&gt;&#xA;          &lt;td&gt;2023.11.6&lt;/td&gt;&#xA;          &lt;td&gt;GPT-3.5是OpenAI开发的先进自然语言处理模型，作为GPT-3系列的改进版本。它继承了GPT-3的基本架构，拥有数以百亿计的参数，能够更有效地处理和生成自然语言文本。GPT-3.5在理解复杂语境、回答问题、撰写文章和文本摘要方面表现出色。通过广泛的数据训练，GPT-3.5能够处理多种语言和领域的内容，是人工智能应用中的多功能工具。&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://platform.openai.com/docs/overview&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://platform.openai.com/docs/overview&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;到处都有&lt;/td&gt;&#xA;          &lt;td&gt;付费，输入/输出 1k token 花费 0.001$/0.002$&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://platform.openai.com/docs/api-reference&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://chat.openai.com/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://chat.openai.com/&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;16K&lt;/td&gt;&#xA;          &lt;td&gt;未公布&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatGLM3-6B&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;双语&lt;/td&gt;&#xA;          &lt;td&gt;6.2B&lt;/td&gt;&#xA;          &lt;td&gt;清华大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.10&lt;/td&gt;&#xA;          &lt;td&gt;ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性：更强大的基础模型： ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略；更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景；更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、长文本对话模型 ChatGLM3-6B-32K。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM3&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/THUDM/ChatGLM3&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM/chatglm3-6b&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/THUDM/chatglm3-6b&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2210.02414.pdf&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2210.02414.pdf&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://www.chatglm.cn/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://www.chatglm.cn/&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;私有部署免费，官网调用收费&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://zhipu-ai.feishu.cn/wiki/FelEwysrFiM81ekrRqfcWN24nXb&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;API 开发文档&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;是，32GB内存，13GB显存&lt;/td&gt;&#xA;          &lt;td&gt;无(可以本地安装)&lt;/td&gt;&#xA;          &lt;td&gt;8K/32K&lt;/td&gt;&#xA;          &lt;td&gt;The pile； Wudaocorpora&lt;/td&gt;&#xA;          &lt;td&gt;ChatGLM3-6B-Base&lt;/td&gt;&#xA;          &lt;td&gt;填写问卷后可以免费商用&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;BenTsao (本草)&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;哈尔滨工业大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.8.3&lt;/td&gt;&#xA;          &lt;td&gt;开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.06975.pdf&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2304.06975.pdf&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;不可用&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;github仓库里提到了若干个微调用数据集，之后可以调研&lt;/td&gt;&#xA;          &lt;td&gt;活字1.0/Bloom-7B/Alpaca-Chinese-7B/LLaMA-7B&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;DoctorGLM&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;双语&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;上海交通大学，复旦大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.6&lt;/td&gt;&#xA;          &lt;td&gt;基于 ChatGLM-6B的中文问诊模型，通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/xionghonglin/DoctorGLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/xionghonglin/DoctorGLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.01097.pdf&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2304.01097.pdf&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://xionghonglin.github.io/DoctorGLM/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://xionghonglin.github.io/DoctorGLM/&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/622649076&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://zhuanlan.zhihu.com/p/622649076&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;不可用&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;见仓库&lt;/td&gt;&#xA;          &lt;td&gt;ChatGLM-6B&lt;/td&gt;&#xA;          &lt;td&gt;不清楚&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;BianQue&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;华南理工大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.6.6&lt;/td&gt;&#xA;          &lt;td&gt;一个经过指令与多轮问询对话联合微调的医疗对话大模型，基于ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/scutcyr/BianQue&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/scutcyr/BianQue&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/scutcyr/BianQue-2&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/scutcyr/BianQue-2&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.15896.pdf&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2310.15896.pdf&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可以私有部署api&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;我们结合当前开源的中文医疗问答数据集（&lt;a href=&#34;https://github.com/UCSD-AI4H/Medical-Dialogue-System&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;MedDialog-CN&lt;/a&gt;、&lt;a href=&#34;https://github.com/lemuria-wchen/imcs21&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;IMCS-V2&lt;/a&gt;、&lt;a href=&#34;https://tianchi.aliyun.com/dataset/95414&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;CHIP-MDCFNPC&lt;/a&gt;、&lt;a href=&#34;https://tianchi.aliyun.com/dataset/95414&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;MedDG&lt;/a&gt;、&lt;a href=&#34;https://github.com/zhangsheng93/cMedQA2&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;cMedQA2&lt;/a&gt;、&lt;a href=&#34;https://github.com/Toyhom/Chinese-medical-dialogue-data&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Chinese-medical-dialogue-data&lt;/a&gt;），分析其中的单轮/多轮特性以及医生问询特性，结合实验室长期自建的生活空间健康对话大数据，构建了千万级别规模的扁鹊健康大数据BianQueCorpus。&lt;/td&gt;&#xA;          &lt;td&gt;ChatGLM-6B&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;HuatuoGPT-II&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;双语&lt;/td&gt;&#xA;          &lt;td&gt;7B/13B/34B&lt;/td&gt;&#xA;          &lt;td&gt;深圳大数据研究院，香港中文大学(深圳)&lt;/td&gt;&#xA;          &lt;td&gt;2023.11.23&lt;/td&gt;&#xA;          &lt;td&gt;开源了经过中文医学指令精调/指令微调(Instruct-tuning)的一个GPT-like模型&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/FreedomIntelligence/HuatuoGPT-II&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/FreedomIntelligence/HuatuoGPT-II&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;好多个，详见github&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.09774.pdf&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2311.09774.pdf&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;私有部署，自己开发&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://www.huatuogpt.cn/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://www.huatuogpt.cn/&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/king-yyf/CMeKG_tools&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/king-yyf/CMeKG_tools&lt;/a&gt;，还用ChatGPT构造了一些，和本草相同&lt;/td&gt;&#xA;          &lt;td&gt;Baichuan2-7B-Base/Baichuan2-13B-Base/Yi-34B&lt;/td&gt;&#xA;          &lt;td&gt;不清楚&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Med-ChatGLM&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;哈尔滨工业大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.3&lt;/td&gt;&#xA;          &lt;td&gt;基于中文医学知识的ChatGLM模型微调，微调数据与BenTsao相同。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Med-ChatGLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/SCIR-HI/Med-ChatGLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;或许可用&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;是，下载地址：https://pan.baidu.com/share/init?surl=Sfi1bRwV741GIChIEOUW0A&amp;amp;pwd=i73e，https://drive.google.com/drive/folders/1ZQSN56DloRGQ-Qj7IwzY4jV3ZHKMe9Bc&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/king-yyf/CMeKG_tools&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/king-yyf/CMeKG_tools&lt;/a&gt;，还用&lt;/td&gt;&#xA;          &lt;td&gt;ChatGLM-6B&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;QiZhenGPT&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;7B/6B/13B&lt;/td&gt;&#xA;          &lt;td&gt;浙江大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.6.27&lt;/td&gt;&#xA;          &lt;td&gt;该项目利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/CMKRG/QiZhenGPT&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/CMKRG/QiZhenGPT&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可以私有部署&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;是，见github&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;见仓库&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Chinese-LLaMA-Plus-7B&lt;/a&gt;/&lt;a href=&#34;https://github.com/zjunlp/CaMA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;CaMA-13B&lt;/a&gt;/&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;ChatGLM-6B&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatMed&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;7B（同基座模型Llama-7b）&lt;/td&gt;&#xA;          &lt;td&gt;华东师范大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.5.5&lt;/td&gt;&#xA;          &lt;td&gt;该项目推出ChatMed系列中文医疗大规模语言模型，模型主干为LlaMA-7b并采用LoRA微调，具体包括ChatMed-Consult : 基于中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w+在线问诊+ChatGPT回复作为训练集；ChatMed-TCM : 基于中医药指令数据集ChatMed_TCM_Dataset，以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到2.6w+的围绕中医药的指令数据训练得到。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/michael-wzhu/ChatMed&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/michael-wzhu/ChatMed&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/michaelwzhu/ChatMed-Consult&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/michaelwzhu/ChatMed-Consult&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可以私有部署api，需要下载模型后自己开发api&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;是，.bin文件80M&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Llama-7b&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;XrayGLM&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;双语&lt;/td&gt;&#xA;          &lt;td&gt;6B，同基座&lt;/td&gt;&#xA;          &lt;td&gt;澳门理工大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.5&lt;/td&gt;&#xA;          &lt;td&gt;该项目为促进中文领域医学多模态大模型的研究发展，发布了XrayGLM数据集及模型，其在医学影像诊断和多轮交互对话上显示出了非凡的潜力。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/WangRongsheng/XrayGLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/WangRongsheng/XrayGLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可以用webUI&lt;/td&gt;&#xA;          &lt;td&gt;详见github中webUI一部分&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://physionet.org/content/mimic-cxr-jpg/2.0.0/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://physionet.org/content/mimic-cxr-jpg/2.0.0/&lt;/a&gt;   &lt;a href=&#34;https://openi.nlm.nih.gov/faq#collection&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://openi.nlm.nih.gov/faq#collection&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;VisualGLM-6B&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;MeChat&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;6B&lt;/td&gt;&#xA;          &lt;td&gt;浙江大学，西湖大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.12&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源的中文心理健康支持通用模型由 ChatGLM-6B LoRA 16-bit 指令微调得到。数据集通过调用gpt-3.5-turbo API扩展真实的心理互助 QA为多轮的心理健康支持多轮对话，提高了通用语言大模型在心理健康支持领域的表现，更加符合在长程多轮对话的应用场景。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/qiuhuachuan/smile&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/qiuhuachuan/smile&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/qiuhuachuan/MeChat&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/qiuhuachuan/MeChat&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可以，有交互文件可运行&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;http://47.97.220.53:8080/ （似乎不能用了）&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;smileChat，见仓库&lt;/td&gt;&#xA;          &lt;td&gt;ChatGLM2-6B&lt;/td&gt;&#xA;          &lt;td&gt;不清楚&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;MedicalGPT&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;双语&lt;/td&gt;&#xA;          &lt;td&gt;13B&lt;/td&gt;&#xA;          &lt;td&gt;个人（应该是公司里的，百度/腾讯）&lt;/td&gt;&#xA;          &lt;td&gt;2023.10.23&lt;/td&gt;&#xA;          &lt;td&gt;训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。发布中文医疗LoRA模型shibing624/ziya-llama-13b-medical-lora，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/shibing624/MedicalGPT&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/shibing624/vicuna-baichuan-13b-chat&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/shibing624/vicuna-baichuan-13b-chat&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可以，demo的api&lt;/td&gt;&#xA;          &lt;td&gt;详见github Demo&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;详见github Demo部分&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;240w条中文医疗数据：https://huggingface.co/datasets/shibing624/medical  22万条中文医疗对话数据集(华佗项目)：&lt;a href=&#34;https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;FreedomIntelligence/HuatuoGPT-sft-data-v1&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;BaiChuan-13B&lt;/td&gt;&#xA;          &lt;td&gt;不清楚&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Sunsimiao&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;7B&lt;/td&gt;&#xA;          &lt;td&gt;华东理工大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.6&lt;/td&gt;&#xA;          &lt;td&gt;Sunsimiao是一个开源的中文医疗大模型，该模型基于baichuan-7B和ChatGLM-6B底座模型在十万级高质量的中文医疗数据中微调而得。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/thomas-yanxin/Sunsimiao&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/thomas-yanxin/Sunsimiao&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;有单纯的类似命令行输入输出，一段使用代码&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;没有提供10W中文医疗数据&lt;/td&gt;&#xA;          &lt;td&gt;BaiChuan-7B&lt;/td&gt;&#xA;          &lt;td&gt;不清楚&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ShenNong-TCM-LLM&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;7B&lt;/td&gt;&#xA;          &lt;td&gt;华东师范大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.6&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了ShenNong中医药大规模语言模型，该模型以LlaMA为底座，采用LoRA (rank=16)微调得到。微调代码与ChatMed代码库相同。此外该项目还开源了中医药指令微调数据集。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/michael-wzhu/ShenNong-TCM-LLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/michael-wzhu/ShenNong-TCM-LLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset&lt;/a&gt;   中医药知识图谱：https://github.com/ywjawmw/TCM_KG&lt;/td&gt;&#xA;          &lt;td&gt;Llama-7B&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;SoulChat&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;中文&lt;/td&gt;&#xA;          &lt;td&gt;6B&lt;/td&gt;&#xA;          &lt;td&gt;华南理工大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.7&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了经过百万规模心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调的心理健康大模型灵心（SoulChat），该模型以ChatGLM-6B作为初始化模型，进行了全量参数的指令微调。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/scutcyr/SoulChat&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/scutcyr/SoulChat&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;扁鹊：https://huggingface.co/scutcyr/BianQue-2  灵心：https://huggingface.co/scutcyr/SoulChat&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2023.findings-emnlp.83/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://aclanthology.org/2023.findings-emnlp.83/&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可以，有Demo&lt;/td&gt;&#xA;          &lt;td&gt;详见github启动服务&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;提供&lt;a href=&#34;https://github.com/scutcyr/SoulChat/blob/main/soulchat_app.py&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;soulchat_app.py&lt;/a&gt;代码&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;数据集即将发布&lt;/td&gt;&#xA;          &lt;td&gt;ChatGLM-6B&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;CareGPT&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;双语&lt;/td&gt;&#xA;          &lt;td&gt;有多种，包括7b、13B、14B、20B，同基座&lt;/td&gt;&#xA;          &lt;td&gt;澳门理工大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.8&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了数十个公开可用的医疗微调数据集和开放可用的医疗大语言模型，包含LLM的训练、测评、部署等以促进医疗LLM快速发展。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/WangRongsheng/CareGPT&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/WangRongsheng/CareGPT&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/wangrongsheng/carellm/tree/main&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/wangrongsheng/carellm/tree/main&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可用&lt;/td&gt;&#xA;          &lt;td&gt;github上有介绍，在部署的部分&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/wangrongsheng/CareLlama&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/spaces/wangrongsheng/CareLlama&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;在github数据部分&lt;/td&gt;&#xA;          &lt;td&gt;llama-7b、baichuan&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;DISC-MedLLM&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;双语&lt;/td&gt;&#xA;          &lt;td&gt;13B&lt;/td&gt;&#xA;          &lt;td&gt;复旦大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.8&lt;/td&gt;&#xA;          &lt;td&gt;该项目是由复旦大学发布的针对医疗健康对话式场景而设计的医疗领域大模型与数据集，该模型由DISC-Med-SFT数据集基于Baichuan-13B-Base指令微调得到。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/FudanDISC/DISC-MedLLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/FudanDISC/DISC-MedLLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Flmc/DISC-MedLLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/Flmc/DISC-MedLLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.14346&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/abs/2308.14346&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;可用，cli_demo.py&lt;/td&gt;&#xA;          &lt;td&gt;详见github demo&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;web_demo.py&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/Flmc/DISC-Med-SFT&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/datasets/Flmc/DISC-Med-SFT&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;baichuan-13B-base&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Taiyi-LLM&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;双语&lt;/td&gt;&#xA;          &lt;td&gt;7BB&lt;/td&gt;&#xA;          &lt;td&gt;大连理工大学&lt;/td&gt;&#xA;          &lt;td&gt;2023.10&lt;/td&gt;&#xA;          &lt;td&gt;该项目由大连理工大学信息检索研究室开发的中英双语医学大模型&amp;quot;太一&amp;quot;，收集整理了丰富的中英双语生物医学自然语言处理（BioNLP）训练语料，总共包含38个中文数据集，通过丰富的中英双语任务指令数据（超过100W条样本）进行大模型（Qwen-7B-base）指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种BioNLP能力。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/DUTIR-BioNLP/Taiyi-LLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/DUTIR-BioNLP/Taiyi-LLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.11608&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/abs/2311.11608&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;无&lt;/td&gt;&#xA;          &lt;td&gt;似乎可用，dialog开头的python文件&lt;/td&gt;&#xA;          &lt;td&gt;如前&lt;/td&gt;&#xA;          &lt;td&gt;是&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;同基座&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Qwen-7B -base&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;下述模型仍然在信息更新中&#34;&gt;下述模型仍然在信息更新中……&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;模型名称&lt;/th&gt;&#xA;          &lt;th&gt;模型类型&lt;/th&gt;&#xA;          &lt;th&gt;简介&lt;/th&gt;&#xA;          &lt;th&gt;GitHub地址&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;WiNGPT&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;WiNGPT是一个基于GPT的医疗垂直领域大模型，基于Qwen-7b1作为基础预训练模型，在此技术上进行了继续预训练，指令微调等，该项目具体开源了WiNGPT2-7B-Base与WiNGPT2-7B-Chat模型。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/winninghealth/WiNGPT2&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/winninghealth/WiNGPT2&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChiMed-GPT&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;ChiMed-GPT是一个开源中文医学大语言模型，通过在中文医学数据上持续训练 Ziya-v2 构建而成，其中涵盖了预训练、有监督微调 (SFT) 和来自人类反馈的强化学习 (RLHF) 等训练过程。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/synlp/ChiMed-GPT&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/synlp/ChiMed-GPT&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatGLM&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/THUDM/ChatGLM-6B&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatGLM2-6B&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;基于开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练；基座模型的上下文长度扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练；基于 Multi-Query Attention 技术实现更高效的推理速度和更低的显存占用；允许商业使用。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/THUDM/ChatGLM2-6B&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Chinese-LLaMA-Alpaca&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;中文LLaMA&amp;amp;Alpaca大语言模型+本地CPU/GPU部署，在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Chinese-LLaMA-Alpaca-2&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;该项目将发布中文LLaMA-2 &amp;amp; Alpaca-2大语言模型，基于可商用的LLaMA-2进行二次开发&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Chinese-LlaMA2&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;该项目基于可商用的LLaMA-2进行二次开发决定在次开展Llama 2的中文汉化工作，包括Chinese-LlaMA2: 对Llama 2进行中文预训练；第一步：先在42G中文预料上进行训练；后续将会加大训练规模；Chinese-LlaMA2-chat: 对Chinese-LlaMA2进行指令微调和多轮对话微调，以适应各种应用场景和多轮对话交互。同时我们也考虑更为快速的中文适配方案：Chinese-LlaMA2-sft-v0: 采用现有的开源中文指令微调或者是对话数据，对LlaMA-2进行直接微调 (将于近期开源)。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/michael-wzhu/Chinese-LlaMA2&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/michael-wzhu/Chinese-LlaMA2&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Llama2-Chinese&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;该项目专注于Llama2模型在中文方面的优化和上层建设，基于大规模中文数据，从预训练开始对Llama2模型进行中文能力的持续迭代升级。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/FlagAlpha/Llama2-Chinese&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/FlagAlpha/Llama2-Chinese&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Qwen&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;通义千问 是阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。各个规模的模型包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。数据集包括文本和代码等多种数据类型，覆盖通用领域和专业领域，能支持8K的上下文长度，针对插件调用相关的对齐数据做了特定优化，当前模型能有效调用插件以及升级为Agent。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/QwenLM/Qwen&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;OpenChineseLLaMA&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;基于 LLaMA-7B 经过中文数据集增量预训练产生的中文大语言模型基座，对比原版 LLaMA，该模型在中文理解能力和生成能力方面均获得较大提升，在众多下游任务中均取得了突出的成绩。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/OpenLMLab/OpenChineseLLaMA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/OpenLMLab/OpenChineseLLaMA&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;BELLE&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;开源了基于BLOOMZ和LLaMA优化后的一系列模型，同时包括训练数据、相关模型、训练代码、应用场景等，也会持续评估不同训练数据、训练算法等对模型表现的影响。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/LianjiaTech/BELLE&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Panda&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;开源了基于LLaMA-7B, -13B, -33B, -65B 进行中文领域上的持续预训练的语言模型, 使用了接近 15M 条数据进行二次预训练。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/dandelionsllm/pandallm&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/dandelionsllm/pandallm&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Robin (罗宾)&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;Robin (罗宾)是香港科技大学LMFlow团队开发的中英双语大语言模型。仅使用180K条数据微调得到的Robin第二代模型，在Huggingface榜单上达到了第一名的成绩。LMFlow支持用户快速训练个性化模型，仅需单张3090和5个小时即可微调70亿参数定制化模型。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/OptimalScale/LMFlow&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Fengshenbang-LM&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，该项目开源了姜子牙通用大模型V1，是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。除姜子牙系列模型之外，该项目还开源了太乙、二郎神系列等模型。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-CCNL/Fengshenbang-LM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/IDEA-CCNL/Fengshenbang-LM&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;BiLLa&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了推理能力增强的中英双语LLaMA模型。模型的主要特性有：较大提升LLaMA的中文理解能力，并尽可能减少对原始LLaMA英文能力的损伤；训练过程增加较多的任务型数据，利用ChatGPT生成解析，强化模型理解任务求解逻辑；全量参数更新，追求更好的生成效果。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/Neutralzz/BiLLa&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/Neutralzz/BiLLa&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Moss&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;支持中英双语和多种插件的开源对话语言模型，MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/OpenLMLab/MOSS&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/OpenLMLab/MOSS&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Luotuo-Chinese-LLM&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;囊括了一系列中文大语言模型开源项目，包含了一系列基于已有开源模型（ChatGLM, MOSS, LLaMA）进行二次微调的语言模型，指令微调数据集等。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/LC1332/Luotuo-Chinese-LLM&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Linly&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;提供中文对话模型 Linly-ChatFlow 、中文基础模型 Linly-Chinese-LLaMA 及其训练数据。 中文基础模型以 LLaMA 为底座，利用中文和中英平行增量预训练。项目汇总了目前公开的多语言指令数据，对中文模型进行了大规模指令跟随训练，实现了 Linly-ChatFlow 对话模型。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/CVI-SZU/Linly&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/CVI-SZU/Linly&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Firefly&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;Firefly(流萤) 是一个开源的中文大语言模型项目，开源包括数据、微调代码、多个基于Bloom、baichuan等微调好的模型等；支持全量参数指令微调、QLoRA低成本高效指令微调、LoRA指令微调；支持绝大部分主流的开源大模型，如百川baichuan、Ziya、Bloom、LLaMA等。持lora与base model进行权重合并，推理更便捷。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/yangjianxin1/Firefly&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/yangjianxin1/Firefly&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatYuan&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;元语智能发布的一系列支持中英双语的功能型对话语言大模型，在微调数据、人类反馈强化学习、思维链等方面进行了优化。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/clue-ai/ChatYuan&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/clue-ai/ChatYuan&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatRWKV&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;开源了一系列基于RWKV架构的Chat模型（包括英文和中文），发布了包括Raven，Novel-ChnEng，Novel-Ch与Novel-ChnEng-ChnPro等模型，可以直接闲聊及进行诗歌，小说等创作，包括7B和14B等规模的模型。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/BlinkDL/ChatRWKV&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;CPM-Bee&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;一个完全开源、允许商用的百亿参数中英文基座模型。它采用Transformer自回归架构（auto-regressive），在超万亿（trillion）高质量语料上进行预训练，拥有强大的基础能力。开发者和研究者可以在CPM-Bee基座模型的基础上在各类场景进行适配来以创建特定领域的应用模型。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/CPM-Bee&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/OpenBMB/CPM-Bee&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;TigerBot&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;一个多语言多任务的大规模语言模型(LLM)，开源了包括模型：TigerBot-7B, TigerBot-7B-base，TigerBot-180B，基本训练和推理代码，100G预训练数据，涵盖金融、法律、百科的领域数据以及API等。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/TigerResearch/TigerBot&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/TigerResearch/TigerBot&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;书生·浦语&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;商汤科技、上海AI实验室联合香港中文大学、复旦大学和上海交通大学发布千亿级参数大语言模型“书生·浦语”（InternLM）。据悉，“书生·浦语”具有1040亿参数，基于“包含1.6万亿token的多语种高质量数据集”训练而成。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-techreport&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/InternLM/InternLM-techreport&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Aquila&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;由智源研究院发布，Aquila语言大模型在技术上继承了GPT-3、LLaMA等的架构设计优点，替换了一批更高效的底层算子实现、重新设计实现了中英双语的tokenizer，升级了BMTrain并行训练方法，是在中英文高质量语料基础上从０开始训练的，通过数据质量的控制、多种训练的优化方法，实现在更小的数据集、更短的训练时间，获得比其它开源模型更优的性能。也是首个支持中英双语知识、支持商用许可协议、符合国内数据合规需要的大规模开源语言模型。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Baichuan-7B&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。该项目发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/baichuan-inc/baichuan-7B&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Baichuan-13B&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;由百川智能开发的一个开源可商用的大规模预训练语言模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-13B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/baichuan-inc/Baichuan-13B&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Baichuan2&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;由百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练，在多个权威的中文、英文和多语言的通用、领域 benchmark上取得同尺寸最佳的效果，发布包含有7B、13B的Base和经过PPO训练的Chat版本，并提供了Chat版本的4bits量化。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan2&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/baichuan-inc/Baichuan2&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Anima&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;由艾写科技开发的一个开源的基于QLoRA的33B中文大语言模型，该模型基于QLoRA的Guanaco 33B模型使用Chinese-Vicuna项目开放的训练数据集guanaco_belle_merge_v1.0进行finetune训练了10000个step，基于Elo rating tournament评估效果较好。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/lyogavin/Anima&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/lyogavin/Anima&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;KnowLM&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;KnowLM项目旨在发布开源大模型框架及相应模型权重以助力减轻知识谬误问题，包括大模型的知识难更新及存在潜在的错误和偏见等。该项目一期发布了基于Llama的抽取大模型智析，使用中英文语料对LLaMA（13B）进行进一步全量预训练，并基于知识图谱转换指令技术对知识抽取任务进行优化。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/zjunlp/KnowLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/zjunlp/KnowLM&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;BayLing&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;一个具有增强的跨语言对齐的通用大模型，由中国科学院计算技术研究所自然语言处理团队开发。百聆（BayLing）以LLaMA为基座模型，探索了以交互式翻译任务为核心进行指令微调的方法，旨在同时完成语言间对齐以及与人类意图对齐，将LLaMA的生成能力和指令跟随能力从英语迁移到其他语言（中文）。在多语言翻译、交互翻译、通用任务、标准化考试的测评中，百聆在中文/英语中均展现出更好的表现。百聆提供了在线的内测版demo，以供大家体验。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/ictnlp/BayLing&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/ictnlp/BayLing&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;YuLan-Chat&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;YuLan-Chat是中国人民大学GSAI研究人员开发的基于聊天的大语言模型。它是在LLaMA的基础上微调开发的，具有高质量的英文和中文指令。 YuLan-Chat可以与用户聊天，很好地遵循英文或中文指令，并且可以在量化后部署在GPU（A800-80G或RTX3090）上。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/RUC-GSAI/YuLan-Chat&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/RUC-GSAI/YuLan-Chat&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;PolyLM&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;一个在6400亿个词的数据上从头训练的多语言语言模型，包括两种模型大小(1.7B和13B)。PolyLM覆盖中、英、俄、西、法、葡、德、意、荷、波、阿、土、希伯来、日、韩、泰、越、印尼等语种，特别是对亚洲语种更友好。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/DAMO-NLP-MT/PolyLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/DAMO-NLP-MT/PolyLM&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;huozi&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048，同时还开源了基于RLHF训练的模型以及全人工标注的16.9K中文偏好数据集。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/HIT-SCIR/huozi&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/HIT-SCIR/huozi&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;YaYi&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，进一步提升了模型性能和安全性。已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/wenge-research/YaYi&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/wenge-research/YaYi&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;XVERSE-13B&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;由深圳元象科技自主研发的支持多语言的大语言模型，使用主流 Decoder-only 的标准Transformer网络结构，支持 8K 的上下文长度（Context Length），为同尺寸模型中最长，构建了 1.4 万亿 token 的高质量、多样化的数据对模型进行充分训练，包含中、英、俄、西等 40 多种语言，通过精细化设置不同类型数据的采样比例，使得中英两种语言表现优异，也能兼顾其他语言效果；基于BPE算法使用上百GB 语料训练了一个词表大小为100,278的分词器，能够同时支持多语言，而无需额外扩展词表。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/xverse-ai/XVERSE-13B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/xverse-ai/XVERSE-13B&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Skywork&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了天工系列模型，该系列模型在3.2TB高质量多语言和代码数据上进行预训练，开源了包括模型参数，训练数据，评估数据，评估方法。具体包括Skywork-13B-Base模型、Skywork-13B-Chat模型、Skywork-13B-Math模型和Skywork-13B-MM模型，以及每个模型的量化版模型，以支持用户在消费级显卡进行部署和推理。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/SkyworkAI/Skywork&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/SkyworkAI/Skywork&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Yi&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了Yi-6B和Yi-34B等模型，该系列模型最长可支持200K的超长上下文窗口版本，可以处理约40万汉字超长文本输入，理解超过1000页的PDF文档。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/01-ai/Yi&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/01-ai/Yi&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Yuan-2.0&lt;/td&gt;&#xA;          &lt;td&gt;通用&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了由浪潮信息发布的新一代基础语言大模型，具体开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且提供了预训练，微调，推理服务的相关脚本。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/IEIT-Yuan/Yuan-2.0&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/IEIT-Yuan/Yuan-2.0&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;AlpaCare&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了医学大模型AlpaCare，在LLaMA上微调得到。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/XZhang97666/AlpaCare&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/XZhang97666/AlpaCare&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Zhongjing&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了首个包含预训练、有监督微调和 RLHF 完整训练流程的中文医学大模型，展现出了很好的泛化能力，在某些对话场景中甚至接近专业医生的专业水平。此外，还开源了一个包含 70,000 条完全来源于真实医患对话的多轮对话数据集。该数据集包含大量医生主动提问的语句，有助于提升模型的主动医疗询问能力。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/SupritYoung/Zhongjing&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/SupritYoung/Zhongjing&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;PMC-LLaMA&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了医疗大模型PMC-LLaMA，包括预训练版本的MedLLaMA_13B和指令微调版本的PMC_LLaMA_13B。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/chaoyi-wu/PMC-LLaMA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/chaoyi-wu/PMC-LLaMA&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatDoctor&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了医疗大模型ChatDoctor，在LLaMA的基础上训练得到。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/Kent0n-Li/ChatDoctor&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/Kent0n-Li/ChatDoctor&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;MING&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了医疗大模型MING，基于bloomz-7b指令微调得到MING-7B，支持医疗问答、智能问诊等功能。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/189569400/MedicalGPT-zh&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/189569400/MedicalGPT-zh&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;IvyGPT&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了医疗大模型IvyGPT，它在高质量的医学问答数据上进行了监督微调，并使用人类反馈的强化学习进行了训练。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/WangRongsheng/IvyGPT&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/WangRongsheng/IvyGPT&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;PULSE&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了中文医疗大模型PULSE，该模型使用约4,000,000个中文医学领域和通用领域的指令微调数据进行微调，支持医学领域的各种自然语言处理任务，包括健康教育、医师考试问题、报告解读、医疗记录结构化以及模拟诊断和治疗。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/openmedlab/PULSE&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/openmedlab/PULSE&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;HuangDI&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了中医大模型HuangDI (皇帝)，该模型首先在Ziya-LLaMA-13B-V1基座模型的基础上加入中医教材、中医各类网站数据等语料库，训练出一个具有中医知识理解力的预训练模型，之后在此基础上通过海量的中医古籍指令对话数据及通用指令数据进行有监督微调，使得模型具备中医古籍知识问答能力。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/Zlasejd/HuangDI&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/Zlasejd/HuangDI&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;CMLM-ZhongJing&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目开源了中医大模型ZhongJing (仲景)，该模型旨在阐明中医博大精深之知识，传承古代智慧与现代技术创新，最终为医学领域提供可信赖和专业的工具。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/pariskang/CMLM-ZhongJing&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/pariskang/CMLM-ZhongJing&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;TCMLLM&lt;/td&gt;&#xA;          &lt;td&gt;专业&lt;/td&gt;&#xA;          &lt;td&gt;该项目拟通过大模型方式实现中医临床辅助诊疗（病证诊断、处方推荐等）中医药知识问答等任务，推动中医知识问答、临床辅助诊疗等领域的快速发展。目前针对中医临床智能诊疗问题中的处方推荐任务，发布了中医处方推荐大模型TCMLLM-PR，通过整合真实世界临床病历、医学典籍与中医教科书等数据，构建了包含68k数据条目的处方推荐指令微调数据集，在ChatGLM大模型上进行微调得到。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/2020MEAI/TCMLLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/2020MEAI/TCMLLM&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>医学大模型榜单</title>
      <link>https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/</link>
      <pubDate>Wed, 20 Dec 2023 19:37:38 +0800</pubDate>
      <guid>https://dujh22.github.io/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/</guid>
      <description>&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;模型&lt;/th&gt;&#xA;          &lt;th&gt;所在机构&lt;/th&gt;&#xA;          &lt;th&gt;发布时间&lt;/th&gt;&#xA;          &lt;th&gt;开源地址&lt;/th&gt;&#xA;          &lt;th&gt;所用数据&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Med-Flamingo 一种适用于医学领域的多模态少样本学习器&lt;/td&gt;&#xA;          &lt;td&gt;美国斯坦福大学-&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型&lt;/td&gt;&#xA;          &lt;td&gt;美国斯坦福大学-基础模型研究中心CRFM&lt;/td&gt;&#xA;          &lt;td&gt;2022年12月&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/stanford-crfm/BioMedLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/stanford-crfm/BioMedLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型&lt;/td&gt;&#xA;          &lt;td&gt;微软&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/BioGPT&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/microsoft/BioGPT&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Med-PaLM2 5400亿参数的转换器语言模型&lt;/td&gt;&#xA;          &lt;td&gt;谷歌&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;文心一言&lt;/td&gt;&#xA;          &lt;td&gt;百度&lt;/td&gt;&#xA;          &lt;td&gt;2023年2月&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;BioMedGPT-1.6B 生物医药领域基础模型&lt;/td&gt;&#xA;          &lt;td&gt;清华大学-智能产业研究院&lt;/td&gt;&#xA;          &lt;td&gt;2023年4月19日&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;OpenBioMed&lt;/td&gt;&#xA;          &lt;td&gt;清华大学-智能产业研究院&lt;/td&gt;&#xA;          &lt;td&gt;2023年8月14日&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/BioFM/OpenBioMed&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/BioFM/OpenBioMed&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与&lt;a href=&#34;https://github.com/allenai/s2orc/blob/master/README.md&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;S2ORC语料库中&lt;/a&gt;的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;amp;20+生物研究领域多模态预训练模型&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;本草Huatuo&lt;/td&gt;&#xA;          &lt;td&gt;哈尔滨工业大学&lt;/td&gt;&#xA;          &lt;td&gt;2023年3月31日&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集+医学文献和GPT3.5API构建多轮问答数据&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;春雨慧问 基于大模型的AI在线问诊产品&lt;/td&gt;&#xA;          &lt;td&gt;春雨医生&lt;/td&gt;&#xA;          &lt;td&gt;2023年4月26日&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;medGPT 国内首款大模型驱动的 AI 医生&lt;/td&gt;&#xA;          &lt;td&gt;医联&lt;/td&gt;&#xA;          &lt;td&gt;2023年4月28日&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Med-ChatGLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;ChatGLM-6B-Med&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Med-ChatGLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/SCIR-HI/Med-ChatGLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MedPalm&lt;/td&gt;&#xA;          &lt;td&gt;Google&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;ChatDoctor&lt;/td&gt;&#xA;          &lt;td&gt;德克萨斯大学&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/Kent0n-Li/ChatDoctor&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/Kent0n-Li/ChatDoctor&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本+5KChatGPT生成数据进行指令微调&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Chinese-vicuna-med&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md&lt;/a&gt;)&lt;/td&gt;&#xA;          &lt;td&gt;Chinese-vicuna在cMedQA2数据上微调&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;OpenBioMed&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/PharMolix/OpenBioMed&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/PharMolix/OpenBioMed&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;知识图谱&amp;amp;20+生物研究领域多模态预训练模型&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;DoctorGLM 基于chatGLM6B模型的医学垂直领域模型&lt;/td&gt;&#xA;          &lt;td&gt;上海科技大学&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/xionghonglin/DoctorGLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/xionghonglin/DoctorGLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;ChatDoctor+MedDialog+CMD 多轮对话+单轮指令样本微调GLM&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MedicalGPT-zh&lt;/td&gt;&#xA;          &lt;td&gt;上海交通大学&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/MediaBrain-SJTU/MedicalGPT-zh&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/MediaBrain-SJTU/MedicalGPT-zh&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA+16个情境下SELF构建情景对话&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;PMC-LLaMA&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/chaoyi-wu/PMC-LLaMA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/chaoyi-wu/PMC-LLaMA&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;医疗论文微调Llama&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;NHS-LLM&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/CogStack/OpenGPT/tree/main&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/CogStack/OpenGPT/tree/main&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Chatgpt生成的医疗问答，对话，微调模型&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Med-ChatGLM&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Med-ChatGLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/SCIR-HI/Med-ChatGLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;医学知识图谱和chatgpt构建中文医学指令数据集+医学文献和chatgpt构建多轮问答数据&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;网新启真13B&lt;/td&gt;&#xA;          &lt;td&gt;浙江大学&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/CMKRG/QiZhenGPT&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/CMKRG/QiZhenGPT&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;BenTsao&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;BianQue&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;-经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;ChatDD 对话式药物研发助手&lt;/td&gt;&#xA;          &lt;td&gt;水木分子&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;ChatDD-FM 100B 全球首个千亿参数多模态生物医药对话大模型&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;BioMedGPT-10B 16亿参数的轻量级科研版基础模型&lt;/td&gt;&#xA;          &lt;td&gt;清华智能产业研究院（AIR）&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/PharMolix/OpenBioMed&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/PharMolix/OpenBioMed&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;太一（Taiyi） 中英双语生物医学大模型&lt;/td&gt;&#xA;          &lt;td&gt;大连理工大学&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/DUTIR-BioNLP/Taiyi-LLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/DUTIR-BioNLP/Taiyi-LLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Almanac：临床医学检索增强语言模型&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.01229&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/abs/2303.01229&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MedLLaMA&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/chaoyi-wu/MedLLaMA_13B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/chaoyi-wu/MedLLaMA_13B&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;DISC-MedLLM&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;http://fudan-disc.com/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;复旦大学数据智能与社会计算实验室&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/FudanDISC/DISC-MedLLM&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/FudanDISC/DISC-MedLLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Clinical Camel&lt;/td&gt;&#xA;          &lt;td&gt;Vector 研究所（加拿大）&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://huggingface.co/wanglab/ClinicalCamel-70B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/wanglab/ClinicalCamel-70B&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>通用大模型榜单</title>
      <link>https://dujh22.github.io/model/%E9%80%9A%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/</link>
      <pubDate>Wed, 20 Dec 2023 18:47:46 +0800</pubDate>
      <guid>https://dujh22.github.io/model/%E9%80%9A%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/</guid>
      <description>&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;模型&lt;/th&gt;&#xA;          &lt;th&gt;开源地址&lt;/th&gt;&#xA;          &lt;th&gt;参数规模&lt;/th&gt;&#xA;          &lt;th&gt;所用数据&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatGLM-6B&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/THUDM/ChatGLM-6B&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;62亿&lt;/td&gt;&#xA;          &lt;td&gt;1T 标识符的中英双语训练&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatGLM2-6B&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/THUDM/ChatGLM2-6B&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.4万亿中英文tokens数据集上训练，并做了模型对齐+SFT&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;GLM-130B&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/GLM-130B&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/THUDM/GLM-130B&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1300亿&lt;/td&gt;&#xA;          &lt;td&gt;4000 亿个文本token训练+SFT&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Chinese-LLaMA-Alpaca&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Moss&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/OpenLMLab/MOSS&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/OpenLMLab/MOSS&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;七千亿中英文以及代码单词上预训练&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;baichuan-7B&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;AquilaChat-7B&lt;/td&gt;&#xA;          &lt;td&gt;{{https://mp.weixin.qq.com/s/XkoLnFycG1jPWrNT3w_p-g}}&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;CPM-Bee&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/639459740&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://zhuanlan.zhihu.com/p/639459740&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;MPT-30b&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1UW4y1D7N9/?share_source=copy_web&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://www.bilibili.com/video/BV1UW4y1D7N9/?share_source=copy_web&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>全球开源开放大模型发展情况</title>
      <link>https://dujh22.github.io/model/%E5%85%A8%E7%90%83%E5%BC%80%E6%BA%90%E5%BC%80%E6%94%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E6%83%85%E5%86%B5/</link>
      <pubDate>Tue, 19 Dec 2023 14:43:38 +0800</pubDate>
      <guid>https://dujh22.github.io/model/%E5%85%A8%E7%90%83%E5%BC%80%E6%BA%90%E5%BC%80%E6%94%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E6%83%85%E5%86%B5/</guid>
      <description>&lt;h1 id=&#34;开源开放的大模型&#34;&gt;开源开放的大模型&lt;/h1&gt;&#xA;&lt;p&gt;旨在记录全球开源开放大模型发展情况&lt;/p&gt;&#xA;&lt;h2 id=&#34;基础大模型&#34;&gt;基础大模型&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;序号&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;名称&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;参数规模&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;数据规模&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;说明&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;Open-LLMs/llama2.md&#34;&gt;LLaMA-2&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B,13B,34B,70B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;2T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;可商用&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;2&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;Open-LLMs/falcon.md&#34;&gt;Falcon&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B,40B,180B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3.5T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;数据集&lt;a href=&#34;https://huggingface.co/datasets/tiiuae/falcon-refinedweb&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt; RefinedWeb&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;Open-LLMs/baichuan2.md&#34;&gt;baichuan-2&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B,13B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;2.6T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;开放，商用需授权，&lt;a href=&#34;Open-LLMs/baichuan.md&#34;&gt;baichuan-1&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;4&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;Open-LLMs/internlm.md&#34;&gt;InternLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B,20B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;2.3T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;开放，商用需授权&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;5&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;Open-LLMs/bloom.md&#34;&gt;BLOOM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3B,7.1B,176B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;366B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;可商用，最为宽松，&lt;a href=&#34;https://mp.weixin.qq.com/s/ia-yrmXbnlooRA3K1hoTwQ&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;详细介绍&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;6&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;GALACTICA&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;6.7B,30B,120B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;106B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;开放的科学文本和数据&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;Open-LLMs/llama.md&#34;&gt;LLaMA&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B,13B,30B,65B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1.4T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Meta，代码开源，模型“泄露”,不可商用，&lt;a href=&#34;https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;详细介绍&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;8&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MOSS-moon&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;16B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;700B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;6.67x1022 FLOPs&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;9&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;ChatGLM2&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;6B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1.4T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;10&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;StableLM&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3B,7B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;800B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;11&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;RedPajama-INCITE&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3B,7B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;12&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;GPT-NeoX&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;20B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3.15M&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;800GB的&lt;a href=&#34;https://arxiv.org/abs/2101.00027&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;The Pile&lt;/a&gt;数据集&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;13&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;OpenLLaMA&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3B,7B,13B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;14&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MPT&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B,30B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;15&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Pythia&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;2.8B,6.9B,12B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;300B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;16&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;XGen&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1.5T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;17&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;OPT&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;6.7B,13B,30B,66B,175B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;180B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;18&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;Open-LLMs/qwen.md&#34;&gt;Qwen&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B,14B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;2.4T,3.0T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;19&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;XVERSE&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;13B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1.4T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;20&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://github.com/FlagAI-Open/Aquila2&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Aquila2&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B,34B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;21&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Prithvi&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;IBM+NASA,地理空间，100M（图片）&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;22&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;Open-LLMs/skywork.md&#34;&gt;Skywork&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;13B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3.2T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;昆仑万维·天工&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;23&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-Coder&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Deepseek Coder&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1.3B,6.7B,33B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;2T&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Deepseek Coder comprises a series of code language models trained on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;24&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Aquila&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;7B&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;悟道·天鹰&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;非基础大模型&#34;&gt;非基础大模型&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;WizardLM，WizardMath，WizardCoder&lt;/li&gt;&#xA;&lt;li&gt;Alpaca&lt;/li&gt;&#xA;&lt;li&gt;Vicuna&lt;/li&gt;&#xA;&lt;li&gt;Guanaco&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;Open-LLMs/codellama.md&#34;&gt;CodeLLaMA&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;7B,13B,34B&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型架构&#34;&gt;模型架构&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;GPTQ&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
