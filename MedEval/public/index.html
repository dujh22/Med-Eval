<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
	<meta name="generator" content="Hugo 0.148.2">
    
      <meta name="theme" content='FixIt v0.4.0-alpha-20250721024521-a1cd700b'>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>Med-Eval</title><meta name="author" content="">
<meta name="description" content="">
  <meta itemprop="name" content="Med-Eval">
  <meta itemprop="datePublished" content="2024-01-03T18:14:28+08:00">
  <meta itemprop="dateModified" content="2024-01-03T18:14:28+08:00"><meta property="og:url" content="https://dujh22.github.io/">
  <meta property="og:site_name" content="Med-Eval">
  <meta property="og:title" content="Med-Eval">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="website">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Med-Eval">
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" type="text/html" href="https://dujh22.github.io/" title="Med-Eval" /><link rel="alternate" type="application/rss+xml" href="https://dujh22.github.io/index.xml" title="Med-Eval"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "WebSite",
    "url": "https:\/\/dujh22.github.io\/","inLanguage": "zh-CN","name": "Med-Eval"
  }
  </script><script src="/js/head/color-scheme.min.js"></script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="Med-Eval"><span class="header-title-text">Med-Eval</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> Archives</a></li><li class="menu-item">
              <a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> Categories</a></li><li class="menu-item">
              <a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> Tags</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="Med-Eval"><span class="header-title-text">Med-Eval</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="menu-item"><a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> Archives</a></li><li class="menu-item"><a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> Categories</a></li><li class="menu-item"><a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> Tags</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><div class="page home posts"><article class="single summary" itemscope itemtype="http://schema.org/Article"><h2 class="single-title" itemprop="name headline"><a href="/model/med-eval%E6%B5%8B%E8%AF%84%E7%BB%93%E6%9E%9C/">Med Eval测评结果</a>
  </h2><div class="post-meta"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      Anonymous</span></span>&nbsp;<span class="post-publish" title='2024-01-03 18:14:28'>published on <time datetime="2024-01-03">2024-01-03</time></span></div><div class="content"><h3 class="heading-element" id="med-eval-chinese-doctor"><span>Med-Eval-Chinese Doctor</span>
  <a href="#med-eval-chinese-doctor" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><table>
  <thead>
      <tr>
          <th><strong>模型</strong></th>
          <th><strong>基座</strong></th>
          <th><strong>数据</strong></th>
          <th><strong>得分</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Doctor</td>
          <td>Baichuan2-13B-Chat</td>
          <td>2.23125GB PT数据集epoch=1.0,loss=2.1348 <!-- raw HTML omitted -->60.06MB SFT数据epoch=2.5，loss~2.00</td>
          <td>32.22% 31.82% 31.86%</td>
      </tr>
      <tr>
          <td>AiMed</td>
          <td>Baichuan2-13B-Chat</td>
          <td>215.3MB PT数据集 epoch=1.0,loss=2.332  <!-- raw HTML omitted -->48.03MB SFT数据epoch=4.0,loss~1.900</td>
          <td>23.43% 23.79% 23.40%</td>
      </tr>
      <tr>
          <td>Baichuan2-13B-Chat</td>
          <td>-</td>
          <td>-</td>
          <td>13.64% 13.57% 13.53%</td>
      </tr>
  </tbody>
</table>
<p>带有prompt工程后：</p></div><div class="post-footer">
    <a href="/model/med-eval%E6%B5%8B%E8%AF%84%E7%BB%93%E6%9E%9C/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h2 class="single-title" itemprop="name headline"><a href="/model/medrad%E5%85%B7%E4%BD%93%E5%9C%BA%E6%99%AF%E5%92%8C%E4%BB%A3%E7%A0%81/">MedRad具体场景和代码</a>
  </h2><div class="post-meta"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      Anonymous</span></span>&nbsp;<span class="post-publish" title='2024-01-03 12:22:42'>published on <time datetime="2024-01-03">2024-01-03</time></span></div><div class="content"><h3 class="heading-element" id="低复杂度医学知识问答"><span>低复杂度：医学知识问答</span>
  <a href="#%e4%bd%8e%e5%a4%8d%e6%9d%82%e5%ba%a6%e5%8c%bb%e5%ad%a6%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><h4 class="heading-element" id="1-场景示例"><span>1. 场景示例</span>
  <a href="#1-%e5%9c%ba%e6%99%af%e7%a4%ba%e4%be%8b" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;QU&#34;</span>: <span style="color:#e6db74">&#34;以下都是治疗病态肥胖的手术选择，除了-&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;OP&#34;</span>:
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;A&#34;</span>: <span style="color:#e6db74">&#34;可调节胃束带&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;B&#34;</span>: <span style="color:#e6db74">&#34;胆胰分流&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;C&#34;</span>: <span style="color:#e6db74">&#34;十二指肠开关&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;D&#34;</span>: <span style="color:#e6db74">&#34;Roux en y十二指肠旁路&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;EXP&#34;</span>: <span style="color:#e6db74">&#34;Ans.是“d”，即Roux en Y十二指肠旁路减肥手术程序包括：a。垂直带状腹足虫。可调节胃束带。Roux-en-Y胃旁路术（非Roux-en-Y-十二指肠旁路术）d。胆胰分流。十二指肠切除术病态肥胖的外科治疗被称为减肥手术。病态肥胖被定义为体重指数为35 kg/m2或以上，伴有肥胖相关的合并症，或BMI为40 kg/m2或更大，没有合并症。减肥手术导致体重减轻是由两个因素造成的。一是限制饮食。另一种是摄入食物的吸收不良。o胃限制性手术包括垂直带状胃成形术和可调节胃束带术o吸收不良手术包括胆胰分流，十二指肠切换器Roux-en-Y胃旁路既有限制性又有吸收不良的特点减肥手术：作用机制限制性垂直胃束带腹腔镜可调节胃束带大范围限制性/轻度吸收不良Roux-en-Y胃旁路大范围吸收不良/轻度限制性胆胰分流&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;AN&#34;</span>: <span style="color:#e6db74">&#34;D&#34;</span>
</span></span><span style="display:flex;"><span>}<span style="color:#960050;background-color:#1e0010">,</span></span></span></code></pre></div><h4 class="heading-element" id="2-场景说明"><span>2. 场景说明</span>
  <a href="#2-%e5%9c%ba%e6%99%af%e8%af%b4%e6%98%8e" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>这个例子是一个医学知识问答场景，其中包含一个多项选择题，目的是识别出不属于治疗病态肥胖手术选择的选项。这种类型的问题通常要求对特定医学领域有深入了解。要处理这类场景，可以从RAG (Retrieval-Augmented Generation) 结合CoT (Chain of Thought) 和Agent的角度来考虑。</p></div><div class="post-footer">
    <a href="/model/medrad%E5%85%B7%E4%BD%93%E5%9C%BA%E6%99%AF%E5%92%8C%E4%BB%A3%E7%A0%81/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h2 class="single-title" itemprop="name headline"><a href="/model/medrad%E4%B8%80%E4%B8%AA%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E9%9D%A0%E8%BE%85%E5%8A%A9%E5%86%B3%E7%AD%96%E6%A1%86%E6%9E%B6/">MedRad:一个医学大模型的可靠辅助决策框架</a>
  </h2><div class="post-meta"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      Anonymous</span></span>&nbsp;<span class="post-publish" title='2024-01-03 12:22:31'>published on <time datetime="2024-01-03">2024-01-03</time></span></div><div class="content"><p><strong>摘要</strong>: 随着医学领域数据的激增和临床决策的复杂性增加，需要强大的计算工具来辅助医生作出准确、可靠的决策。虽然现有研究已经在准确性上取得进展，但大模型在可靠性和稳定性方面的不足显而易见。为此，我们提出了MedRad框架，一个结合深度知识工程、Chain of Thought (CoT) 推理、Retrieval-Augmented Generation (RAG) 技术和智能代理（Agent）的系统，以提高决策的可靠性。本研究的早期阶段集中于构建医学领域的大型语言模型和通用模型训练算法。进一步，我们将重点转向如何在医学知识问答、门诊对话和临床病历诊断等具有不同复杂度的场景中利用这些技术实现高可靠性的医学决策。实验结果显示，MedRad在上述场景中能够提供高质量的决策路径。我们的框架通过与基座模型的松耦合设计，展现了在不同医学场景中的强大适应性。</p>
<p><strong>关键词</strong>: 医学大模型、决策支持系统、可靠性、Chain of Thought、Retrieval-Augmented Generation、Agent</p>
<hr>
<h3 class="heading-element" id="1-引言"><span><strong>1. 引言</strong></span>
  <a href="#1-%e5%bc%95%e8%a8%80" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>在医学领域，高质量的决策是病人健康和生命的守护者。医生们经常面临着需要在短时间内处理大量信息并做出关键决策的压力。这种决策的复杂性由于医疗数据的海量增长和临床案例的多变性而不断增加。传统的决策支持系统，在处理标准化流程时效果显著，但在解读非结构化的医疗数据、处理复杂病例以及适应不断演变的医学知识面前则显得力不从心。这些系统在准确解释临床数据、适应新的治疗协议以及应对罕见病症方面，常常表现出灵活性不足和可靠性不高的问题。</p></div><div class="post-footer">
    <a href="/model/medrad%E4%B8%80%E4%B8%AA%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E9%9D%A0%E8%BE%85%E5%8A%A9%E5%86%B3%E7%AD%96%E6%A1%86%E6%9E%B6/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h2 class="single-title" itemprop="name headline"><a href="/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95v2/">医学大模型榜单V2</a>
  </h2><div class="post-meta"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      Anonymous</span></span>&nbsp;<span class="post-publish" title='2024-01-03 12:18:27'>published on <time datetime="2024-01-03">2024-01-03</time></span></div><div class="content"><table>
  <thead>
      <tr>
          <th>模型名称</th>
          <th>模型类型</th>
          <th>支持语言</th>
          <th>参数规模</th>
          <th>发布机构</th>
          <th>模型最后一次更新时间</th>
          <th>简介</th>
          <th>GitHub地址</th>
          <th>Hugging Face 地址</th>
          <th>论文地址</th>
          <th>官网地址</th>
          <th>其它信息来源</th>
          <th>API 是否可用</th>
          <th>API 使用地址</th>
          <th>是否可以私有化部署</th>
          <th>Demo 地址</th>
          <th>上下文长度</th>
          <th>训练用数据</th>
          <th>训练基座</th>
          <th>是否可商用</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>gpt-4-1106-preview</td>
          <td>通用</td>
          <td>多语言</td>
          <td>1.8T (未证实)</td>
          <td>OpenAI</td>
          <td>2023.11.6</td>
          <td>GPT-4，或称Generative Pre-trained Transformer 4，是由OpenAI开发的最先进的语言模型。它的参数量大概是其前身GPT-3的10倍。这一进步使GPT-4能够以惊人的准确性和细微差别理解和生成类似人类的文本。它擅长各种任务，如回答问题、写文章、总结文本等等。它在不同数据集上进行大规模训练，能够深入理解多种语言和科目，使其成为人工智能驱动应用程序中的通用工具。</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td><a href="https://platform.openai.com/docs/overview"target="_blank" rel="external nofollow noopener noreferrer">https://platform.openai.com/docs/overview</a></td>
          <td>到处都有</td>
          <td>付费，输入/输出 1k token 花费 0.01$/0.03$</td>
          <td><a href="https://platform.openai.com/docs/api-reference"target="_blank" rel="external nofollow noopener noreferrer">https://platform.openai.com/docs/api-reference</a></td>
          <td>否</td>
          <td><a href="https://chat.openai.com/"target="_blank" rel="external nofollow noopener noreferrer">https://chat.openai.com/</a></td>
          <td>128K</td>
          <td>未公布</td>
          <td>无</td>
          <td>是</td>
      </tr>
      <tr>
          <td>gpt-3.5-turbo-1106</td>
          <td>通用</td>
          <td>多语言</td>
          <td>175B (未证实)</td>
          <td>OpenAI</td>
          <td>2023.11.6</td>
          <td>GPT-3.5是OpenAI开发的先进自然语言处理模型，作为GPT-3系列的改进版本。它继承了GPT-3的基本架构，拥有数以百亿计的参数，能够更有效地处理和生成自然语言文本。GPT-3.5在理解复杂语境、回答问题、撰写文章和文本摘要方面表现出色。通过广泛的数据训练，GPT-3.5能够处理多种语言和领域的内容，是人工智能应用中的多功能工具。</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td><a href="https://platform.openai.com/docs/overview"target="_blank" rel="external nofollow noopener noreferrer">https://platform.openai.com/docs/overview</a></td>
          <td>到处都有</td>
          <td>付费，输入/输出 1k token 花费 0.001$/0.002$</td>
          <td><a href="https://platform.openai.com/docs/api-reference"target="_blank" rel="external nofollow noopener noreferrer">https://platform.openai.com/docs/api-reference</a></td>
          <td>否</td>
          <td><a href="https://chat.openai.com/"target="_blank" rel="external nofollow noopener noreferrer">https://chat.openai.com/</a></td>
          <td>16K</td>
          <td>未公布</td>
          <td>无</td>
          <td>是</td>
      </tr>
      <tr>
          <td>ChatGLM3-6B</td>
          <td>通用</td>
          <td>双语</td>
          <td>6.2B</td>
          <td>清华大学</td>
          <td>2023.10</td>
          <td>ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性：更强大的基础模型： ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略；更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景；更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、长文本对话模型 ChatGLM3-6B-32K。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。</td>
          <td><a href="https://github.com/THUDM/ChatGLM3"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/ChatGLM3</a></td>
          <td><a href="https://huggingface.co/THUDM/chatglm3-6b"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/THUDM/chatglm3-6b</a></td>
          <td><a href="https://arxiv.org/pdf/2210.02414.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2210.02414.pdf</a></td>
          <td><a href="https://www.chatglm.cn/"target="_blank" rel="external nofollow noopener noreferrer">https://www.chatglm.cn/</a></td>
          <td>无</td>
          <td>私有部署免费，官网调用收费</td>
          <td><a href="https://zhipu-ai.feishu.cn/wiki/FelEwysrFiM81ekrRqfcWN24nXb"target="_blank" rel="external nofollow noopener noreferrer">API 开发文档</a></td>
          <td>是，32GB内存，13GB显存</td>
          <td>无(可以本地安装)</td>
          <td>8K/32K</td>
          <td>The pile； Wudaocorpora</td>
          <td>ChatGLM3-6B-Base</td>
          <td>填写问卷后可以免费商用</td>
      </tr>
      <tr>
          <td>BenTsao (本草)</td>
          <td>专业</td>
          <td>中文</td>
          <td>同基座</td>
          <td>哈尔滨工业大学</td>
          <td>2023.8.3</td>
          <td>开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。</td>
          <td><a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese</a></td>
          <td>无</td>
          <td><a href="https://arxiv.org/pdf/2304.06975.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2304.06975.pdf</a></td>
          <td>无</td>
          <td>无</td>
          <td>不可用</td>
          <td>无</td>
          <td>否</td>
          <td>无</td>
          <td>同基座</td>
          <td>github仓库里提到了若干个微调用数据集，之后可以调研</td>
          <td>活字1.0/Bloom-7B/Alpaca-Chinese-7B/LLaMA-7B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>DoctorGLM</td>
          <td>专业</td>
          <td>双语</td>
          <td>同基座</td>
          <td>上海交通大学，复旦大学</td>
          <td>2023.6</td>
          <td>基于 ChatGLM-6B的中文问诊模型，通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署</td>
          <td><a href="https://github.com/xionghonglin/DoctorGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xionghonglin/DoctorGLM</a></td>
          <td>无</td>
          <td><a href="https://arxiv.org/pdf/2304.01097.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2304.01097.pdf</a></td>
          <td><a href="https://xionghonglin.github.io/DoctorGLM/"target="_blank" rel="external nofollow noopener noreferrer">https://xionghonglin.github.io/DoctorGLM/</a></td>
          <td><a href="https://zhuanlan.zhihu.com/p/622649076"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/622649076</a></td>
          <td>不可用</td>
          <td>无</td>
          <td>否</td>
          <td>无</td>
          <td>同基座</td>
          <td>见仓库</td>
          <td>ChatGLM-6B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>BianQue</td>
          <td>专业</td>
          <td>中文</td>
          <td>同基座</td>
          <td>华南理工大学</td>
          <td>2023.6.6</td>
          <td>一个经过指令与多轮问询对话联合微调的医疗对话大模型，基于ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。</td>
          <td><a href="https://github.com/scutcyr/BianQue"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/scutcyr/BianQue</a></td>
          <td><a href="https://huggingface.co/scutcyr/BianQue-2"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/scutcyr/BianQue-2</a></td>
          <td><a href="https://arxiv.org/pdf/2310.15896.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2310.15896.pdf</a></td>
          <td>无</td>
          <td>无</td>
          <td>可以私有部署api</td>
          <td><a href="https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/scutcyr/BianQue/blob/main/bianque_v2_app.py</a></td>
          <td>是</td>
          <td>无</td>
          <td>同基座</td>
          <td>我们结合当前开源的中文医疗问答数据集（<a href="https://github.com/UCSD-AI4H/Medical-Dialogue-System"target="_blank" rel="external nofollow noopener noreferrer">MedDialog-CN</a>、<a href="https://github.com/lemuria-wchen/imcs21"target="_blank" rel="external nofollow noopener noreferrer">IMCS-V2</a>、<a href="https://tianchi.aliyun.com/dataset/95414"target="_blank" rel="external nofollow noopener noreferrer">CHIP-MDCFNPC</a>、<a href="https://tianchi.aliyun.com/dataset/95414"target="_blank" rel="external nofollow noopener noreferrer">MedDG</a>、<a href="https://github.com/zhangsheng93/cMedQA2"target="_blank" rel="external nofollow noopener noreferrer">cMedQA2</a>、<a href="https://github.com/Toyhom/Chinese-medical-dialogue-data"target="_blank" rel="external nofollow noopener noreferrer">Chinese-medical-dialogue-data</a>），分析其中的单轮/多轮特性以及医生问询特性，结合实验室长期自建的生活空间健康对话大数据，构建了千万级别规模的扁鹊健康大数据BianQueCorpus。</td>
          <td>ChatGLM-6B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>HuatuoGPT-II</td>
          <td>专业</td>
          <td>双语</td>
          <td>7B/13B/34B</td>
          <td>深圳大数据研究院，香港中文大学(深圳)</td>
          <td>2023.11.23</td>
          <td>开源了经过中文医学指令精调/指令微调(Instruct-tuning)的一个GPT-like模型</td>
          <td><a href="https://github.com/FreedomIntelligence/HuatuoGPT-II"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FreedomIntelligence/HuatuoGPT-II</a></td>
          <td>好多个，详见github</td>
          <td><a href="https://arxiv.org/pdf/2311.09774.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2311.09774.pdf</a></td>
          <td>无</td>
          <td>无</td>
          <td>私有部署，自己开发</td>
          <td>无</td>
          <td>是</td>
          <td><a href="https://www.huatuogpt.cn/"target="_blank" rel="external nofollow noopener noreferrer">https://www.huatuogpt.cn/</a></td>
          <td>同基座</td>
          <td><a href="https://github.com/king-yyf/CMeKG_tools"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/king-yyf/CMeKG_tools</a>，还用ChatGPT构造了一些，和本草相同</td>
          <td>Baichuan2-7B-Base/Baichuan2-13B-Base/Yi-34B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>Med-ChatGLM</td>
          <td>专业</td>
          <td>中文</td>
          <td>同基座</td>
          <td>哈尔滨工业大学</td>
          <td>2023.3</td>
          <td>基于中文医学知识的ChatGLM模型微调，微调数据与BenTsao相同。</td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>或许可用</td>
          <td>无</td>
          <td>是，下载地址：https://pan.baidu.com/share/init?surl=Sfi1bRwV741GIChIEOUW0A&amp;pwd=i73e，https://drive.google.com/drive/folders/1ZQSN56DloRGQ-Qj7IwzY4jV3ZHKMe9Bc</td>
          <td>无</td>
          <td>同基座</td>
          <td><a href="https://github.com/king-yyf/CMeKG_tools"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/king-yyf/CMeKG_tools</a>，还用</td>
          <td>ChatGLM-6B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>QiZhenGPT</td>
          <td>专业</td>
          <td>中文</td>
          <td>7B/6B/13B</td>
          <td>浙江大学</td>
          <td>2023.6.27</td>
          <td>该项目利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。</td>
          <td><a href="https://github.com/CMKRG/QiZhenGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CMKRG/QiZhenGPT</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可以私有部署</td>
          <td>无</td>
          <td>是，见github</td>
          <td>无</td>
          <td>同基座</td>
          <td>见仓库</td>
          <td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca"target="_blank" rel="external nofollow noopener noreferrer">Chinese-LLaMA-Plus-7B</a>/<a href="https://github.com/zjunlp/CaMA"target="_blank" rel="external nofollow noopener noreferrer">CaMA-13B</a>/<a href="https://github.com/THUDM/ChatGLM-6B"target="_blank" rel="external nofollow noopener noreferrer">ChatGLM-6B</a></td>
          <td>否</td>
      </tr>
      <tr>
          <td>ChatMed</td>
          <td>专业</td>
          <td>中文</td>
          <td>7B（同基座模型Llama-7b）</td>
          <td>华东师范大学</td>
          <td>2023.5.5</td>
          <td>该项目推出ChatMed系列中文医疗大规模语言模型，模型主干为LlaMA-7b并采用LoRA微调，具体包括ChatMed-Consult : 基于中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w+在线问诊+ChatGPT回复作为训练集；ChatMed-TCM : 基于中医药指令数据集ChatMed_TCM_Dataset，以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到2.6w+的围绕中医药的指令数据训练得到。</td>
          <td><a href="https://github.com/michael-wzhu/ChatMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/michael-wzhu/ChatMed</a></td>
          <td><a href="https://huggingface.co/michaelwzhu/ChatMed-Consult"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/michaelwzhu/ChatMed-Consult</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可以私有部署api，需要下载模型后自己开发api</td>
          <td>无</td>
          <td>是，.bin文件80M</td>
          <td>无</td>
          <td>同基座</td>
          <td><a href="https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset</a></td>
          <td>Llama-7b</td>
          <td>否</td>
      </tr>
      <tr>
          <td>XrayGLM</td>
          <td>专业</td>
          <td>双语</td>
          <td>6B，同基座</td>
          <td>澳门理工大学</td>
          <td>2023.5</td>
          <td>该项目为促进中文领域医学多模态大模型的研究发展，发布了XrayGLM数据集及模型，其在医学影像诊断和多轮交互对话上显示出了非凡的潜力。</td>
          <td><a href="https://github.com/WangRongsheng/XrayGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/WangRongsheng/XrayGLM</a></td>
          <td><a href="https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/wangrongsheng/XrayGLM-3000/tree/main</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可以用webUI</td>
          <td>详见github中webUI一部分</td>
          <td>是</td>
          <td>无</td>
          <td>同基座</td>
          <td><a href="https://physionet.org/content/mimic-cxr-jpg/2.0.0/"target="_blank" rel="external nofollow noopener noreferrer">https://physionet.org/content/mimic-cxr-jpg/2.0.0/</a>   <a href="https://openi.nlm.nih.gov/faq#collection"target="_blank" rel="external nofollow noopener noreferrer">https://openi.nlm.nih.gov/faq#collection</a></td>
          <td>VisualGLM-6B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>MeChat</td>
          <td>专业</td>
          <td>中文</td>
          <td>6B</td>
          <td>浙江大学，西湖大学</td>
          <td>2023.12</td>
          <td>该项目开源的中文心理健康支持通用模型由 ChatGLM-6B LoRA 16-bit 指令微调得到。数据集通过调用gpt-3.5-turbo API扩展真实的心理互助 QA为多轮的心理健康支持多轮对话，提高了通用语言大模型在心理健康支持领域的表现，更加符合在长程多轮对话的应用场景。</td>
          <td><a href="https://github.com/qiuhuachuan/smile"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/qiuhuachuan/smile</a></td>
          <td><a href="https://huggingface.co/qiuhuachuan/MeChat"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/qiuhuachuan/MeChat</a></td>
          <td><a href="https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/qiuhuachuan/smile/blob/main/paper/SMILE_Single_turn_to_Multi_turn_Inclusive_Language_Expansion_via_ChatGPT_for_Mental_Health_Support.pdf</a></td>
          <td>无</td>
          <td>无</td>
          <td>可以，有交互文件可运行</td>
          <td>无</td>
          <td>是</td>
          <td>http://47.97.220.53:8080/ （似乎不能用了）</td>
          <td>同基座</td>
          <td>smileChat，见仓库</td>
          <td>ChatGLM2-6B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>MedicalGPT</td>
          <td>专业</td>
          <td>双语</td>
          <td>13B</td>
          <td>个人（应该是公司里的，百度/腾讯）</td>
          <td>2023.10.23</td>
          <td>训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。发布中文医疗LoRA模型shibing624/ziya-llama-13b-medical-lora，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重。</td>
          <td><a href="https://github.com/shibing624/MedicalGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/shibing624/MedicalGPT</a></td>
          <td><a href="https://huggingface.co/shibing624/vicuna-baichuan-13b-chat"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/shibing624/vicuna-baichuan-13b-chat</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可以，demo的api</td>
          <td>详见github Demo</td>
          <td>是</td>
          <td>详见github Demo部分</td>
          <td>同基座</td>
          <td>240w条中文医疗数据：https://huggingface.co/datasets/shibing624/medical  22万条中文医疗对话数据集(华佗项目)：<a href="https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1"target="_blank" rel="external nofollow noopener noreferrer">FreedomIntelligence/HuatuoGPT-sft-data-v1</a></td>
          <td>BaiChuan-13B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>Sunsimiao</td>
          <td>专业</td>
          <td>中文</td>
          <td>7B</td>
          <td>华东理工大学</td>
          <td>2023.6</td>
          <td>Sunsimiao是一个开源的中文医疗大模型，该模型基于baichuan-7B和ChatGLM-6B底座模型在十万级高质量的中文医疗数据中微调而得。</td>
          <td><a href="https://github.com/thomas-yanxin/Sunsimiao"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/thomas-yanxin/Sunsimiao</a></td>
          <td><a href="https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary"target="_blank" rel="external nofollow noopener noreferrer">https://modelscope.cn/models/thomas/Sunsimiao-01M-Chat/summary</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>有单纯的类似命令行输入输出，一段使用代码</td>
          <td>无</td>
          <td>是</td>
          <td>无</td>
          <td>同基座</td>
          <td>没有提供10W中文医疗数据</td>
          <td>BaiChuan-7B</td>
          <td>不清楚</td>
      </tr>
      <tr>
          <td>ShenNong-TCM-LLM</td>
          <td>专业</td>
          <td>中文</td>
          <td>7B</td>
          <td>华东师范大学</td>
          <td>2023.6</td>
          <td>该项目开源了ShenNong中医药大规模语言模型，该模型以LlaMA为底座，采用LoRA (rank=16)微调得到。微调代码与ChatMed代码库相同。此外该项目还开源了中医药指令微调数据集。</td>
          <td><a href="https://github.com/michael-wzhu/ShenNong-TCM-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/michael-wzhu/ShenNong-TCM-LLM</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>否</td>
          <td>无</td>
          <td>同基座</td>
          <td><a href="https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset</a>   中医药知识图谱：https://github.com/ywjawmw/TCM_KG</td>
          <td>Llama-7B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>SoulChat</td>
          <td>专业</td>
          <td>中文</td>
          <td>6B</td>
          <td>华南理工大学</td>
          <td>2023.7</td>
          <td>该项目开源了经过百万规模心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调的心理健康大模型灵心（SoulChat），该模型以ChatGLM-6B作为初始化模型，进行了全量参数的指令微调。</td>
          <td><a href="https://github.com/scutcyr/SoulChat"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/scutcyr/SoulChat</a></td>
          <td>扁鹊：https://huggingface.co/scutcyr/BianQue-2  灵心：https://huggingface.co/scutcyr/SoulChat</td>
          <td><a href="https://aclanthology.org/2023.findings-emnlp.83/"target="_blank" rel="external nofollow noopener noreferrer">https://aclanthology.org/2023.findings-emnlp.83/</a></td>
          <td>无</td>
          <td>无</td>
          <td>可以，有Demo</td>
          <td>详见github启动服务</td>
          <td>是</td>
          <td>提供<a href="https://github.com/scutcyr/SoulChat/blob/main/soulchat_app.py"target="_blank" rel="external nofollow noopener noreferrer">soulchat_app.py</a>代码</td>
          <td>同基座</td>
          <td>数据集即将发布</td>
          <td>ChatGLM-6B</td>
          <td>否</td>
      </tr>
      <tr>
          <td>CareGPT</td>
          <td>专业</td>
          <td>双语</td>
          <td>有多种，包括7b、13B、14B、20B，同基座</td>
          <td>澳门理工大学</td>
          <td>2023.8</td>
          <td>该项目开源了数十个公开可用的医疗微调数据集和开放可用的医疗大语言模型，包含LLM的训练、测评、部署等以促进医疗LLM快速发展。</td>
          <td><a href="https://github.com/WangRongsheng/CareGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/WangRongsheng/CareGPT</a></td>
          <td><a href="https://huggingface.co/wangrongsheng/carellm/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/wangrongsheng/carellm/tree/main</a></td>
          <td>无</td>
          <td>无</td>
          <td>无</td>
          <td>可用</td>
          <td>github上有介绍，在部署的部分</td>
          <td>是</td>
          <td><a href="https://huggingface.co/spaces/wangrongsheng/CareLlama"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/spaces/wangrongsheng/CareLlama</a></td>
          <td>同基座</td>
          <td>在github数据部分</td>
          <td>llama-7b、baichuan</td>
          <td>是</td>
      </tr>
      <tr>
          <td>DISC-MedLLM</td>
          <td>专业</td>
          <td>双语</td>
          <td>13B</td>
          <td>复旦大学</td>
          <td>2023.8</td>
          <td>该项目是由复旦大学发布的针对医疗健康对话式场景而设计的医疗领域大模型与数据集，该模型由DISC-Med-SFT数据集基于Baichuan-13B-Base指令微调得到。</td>
          <td><a href="https://github.com/FudanDISC/DISC-MedLLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FudanDISC/DISC-MedLLM</a></td>
          <td><a href="https://huggingface.co/Flmc/DISC-MedLLM"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/Flmc/DISC-MedLLM</a></td>
          <td><a href="https://arxiv.org/abs/2308.14346"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2308.14346</a></td>
          <td>无</td>
          <td>无</td>
          <td>可用，cli_demo.py</td>
          <td>详见github demo</td>
          <td>是</td>
          <td>web_demo.py</td>
          <td>同基座</td>
          <td><a href="https://huggingface.co/datasets/Flmc/DISC-Med-SFT"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/datasets/Flmc/DISC-Med-SFT</a></td>
          <td>baichuan-13B-base</td>
          <td>否</td>
      </tr>
      <tr>
          <td>Taiyi-LLM</td>
          <td>专业</td>
          <td>双语</td>
          <td>7BB</td>
          <td>大连理工大学</td>
          <td>2023.10</td>
          <td>该项目由大连理工大学信息检索研究室开发的中英双语医学大模型&quot;太一&quot;，收集整理了丰富的中英双语生物医学自然语言处理（BioNLP）训练语料，总共包含38个中文数据集，通过丰富的中英双语任务指令数据（超过100W条样本）进行大模型（Qwen-7B-base）指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种BioNLP能力。</td>
          <td><a href="https://github.com/DUTIR-BioNLP/Taiyi-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DUTIR-BioNLP/Taiyi-LLM</a></td>
          <td><a href="https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/DUTIR-BioNLP/Taiyi-LLM</a></td>
          <td><a href="https://arxiv.org/abs/2311.11608"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2311.11608</a></td>
          <td></td>
          <td>无</td>
          <td>似乎可用，dialog开头的python文件</td>
          <td>如前</td>
          <td>是</td>
          <td><a href="https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/"target="_blank" rel="external nofollow noopener noreferrer">https://u230271-8d67-862a10ff.westb.seetacloud.com:8443/</a></td>
          <td>同基座</td>
          <td><a href="https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DUTIR-BioNLP/Taiyi-LLM/blob/main/data_file/dataset_inf.md</a></td>
          <td>Qwen-7B -base</td>
          <td>否</td>
      </tr>
  </tbody>
</table>
<h3 class="heading-element" id="下述模型仍然在信息更新中"><span>下述模型仍然在信息更新中……</span>
  <a href="#%e4%b8%8b%e8%bf%b0%e6%a8%a1%e5%9e%8b%e4%bb%8d%e7%84%b6%e5%9c%a8%e4%bf%a1%e6%81%af%e6%9b%b4%e6%96%b0%e4%b8%ad" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><table>
  <thead>
      <tr>
          <th>模型名称</th>
          <th>模型类型</th>
          <th>简介</th>
          <th>GitHub地址</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>WiNGPT</td>
          <td>专业</td>
          <td>WiNGPT是一个基于GPT的医疗垂直领域大模型，基于Qwen-7b1作为基础预训练模型，在此技术上进行了继续预训练，指令微调等，该项目具体开源了WiNGPT2-7B-Base与WiNGPT2-7B-Chat模型。</td>
          <td><a href="https://github.com/winninghealth/WiNGPT2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/winninghealth/WiNGPT2</a></td>
      </tr>
      <tr>
          <td>ChiMed-GPT</td>
          <td>专业</td>
          <td>ChiMed-GPT是一个开源中文医学大语言模型，通过在中文医学数据上持续训练 Ziya-v2 构建而成，其中涵盖了预训练、有监督微调 (SFT) 和来自人类反馈的强化学习 (RLHF) 等训练过程。</td>
          <td><a href="https://github.com/synlp/ChiMed-GPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/synlp/ChiMed-GPT</a></td>
      </tr>
      <tr>
          <td>ChatGLM</td>
          <td>通用</td>
          <td>中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持</td>
          <td><a href="https://github.com/THUDM/ChatGLM-6B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/ChatGLM-6B</a></td>
      </tr>
      <tr>
          <td>ChatGLM2-6B</td>
          <td>通用</td>
          <td>基于开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练；基座模型的上下文长度扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练；基于 Multi-Query Attention 技术实现更高效的推理速度和更低的显存占用；允许商业使用。</td>
          <td><a href="https://github.com/THUDM/ChatGLM2-6B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/ChatGLM2-6B</a></td>
      </tr>
      <tr>
          <td>Chinese-LLaMA-Alpaca</td>
          <td>通用</td>
          <td>中文LLaMA&amp;Alpaca大语言模型+本地CPU/GPU部署，在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练</td>
          <td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></td>
      </tr>
      <tr>
          <td>Chinese-LLaMA-Alpaca-2</td>
          <td>通用</td>
          <td>该项目将发布中文LLaMA-2 &amp; Alpaca-2大语言模型，基于可商用的LLaMA-2进行二次开发</td>
          <td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ymcui/Chinese-LLaMA-Alpaca-2</a></td>
      </tr>
      <tr>
          <td>Chinese-LlaMA2</td>
          <td>通用</td>
          <td>该项目基于可商用的LLaMA-2进行二次开发决定在次开展Llama 2的中文汉化工作，包括Chinese-LlaMA2: 对Llama 2进行中文预训练；第一步：先在42G中文预料上进行训练；后续将会加大训练规模；Chinese-LlaMA2-chat: 对Chinese-LlaMA2进行指令微调和多轮对话微调，以适应各种应用场景和多轮对话交互。同时我们也考虑更为快速的中文适配方案：Chinese-LlaMA2-sft-v0: 采用现有的开源中文指令微调或者是对话数据，对LlaMA-2进行直接微调 (将于近期开源)。</td>
          <td><a href="https://github.com/michael-wzhu/Chinese-LlaMA2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/michael-wzhu/Chinese-LlaMA2</a></td>
      </tr>
      <tr>
          <td>Llama2-Chinese</td>
          <td>通用</td>
          <td>该项目专注于Llama2模型在中文方面的优化和上层建设，基于大规模中文数据，从预训练开始对Llama2模型进行中文能力的持续迭代升级。</td>
          <td><a href="https://github.com/FlagAlpha/Llama2-Chinese"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FlagAlpha/Llama2-Chinese</a></td>
      </tr>
      <tr>
          <td>Qwen</td>
          <td>通用</td>
          <td>通义千问 是阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。各个规模的模型包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。数据集包括文本和代码等多种数据类型，覆盖通用领域和专业领域，能支持8K的上下文长度，针对插件调用相关的对齐数据做了特定优化，当前模型能有效调用插件以及升级为Agent。</td>
          <td><a href="https://github.com/QwenLM/Qwen"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/QwenLM/Qwen</a></td>
      </tr>
      <tr>
          <td>OpenChineseLLaMA</td>
          <td>通用</td>
          <td>基于 LLaMA-7B 经过中文数据集增量预训练产生的中文大语言模型基座，对比原版 LLaMA，该模型在中文理解能力和生成能力方面均获得较大提升，在众多下游任务中均取得了突出的成绩。</td>
          <td><a href="https://github.com/OpenLMLab/OpenChineseLLaMA"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OpenLMLab/OpenChineseLLaMA</a></td>
      </tr>
      <tr>
          <td>BELLE</td>
          <td>通用</td>
          <td>开源了基于BLOOMZ和LLaMA优化后的一系列模型，同时包括训练数据、相关模型、训练代码、应用场景等，也会持续评估不同训练数据、训练算法等对模型表现的影响。</td>
          <td><a href="https://github.com/LianjiaTech/BELLE"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/LianjiaTech/BELLE</a></td>
      </tr>
      <tr>
          <td>Panda</td>
          <td>通用</td>
          <td>开源了基于LLaMA-7B, -13B, -33B, -65B 进行中文领域上的持续预训练的语言模型, 使用了接近 15M 条数据进行二次预训练。</td>
          <td><a href="https://github.com/dandelionsllm/pandallm"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/dandelionsllm/pandallm</a></td>
      </tr>
      <tr>
          <td>Robin (罗宾)</td>
          <td>通用</td>
          <td>Robin (罗宾)是香港科技大学LMFlow团队开发的中英双语大语言模型。仅使用180K条数据微调得到的Robin第二代模型，在Huggingface榜单上达到了第一名的成绩。LMFlow支持用户快速训练个性化模型，仅需单张3090和5个小时即可微调70亿参数定制化模型。</td>
          <td><a href="https://github.com/OptimalScale/LMFlow"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OptimalScale/LMFlow</a></td>
      </tr>
      <tr>
          <td>Fengshenbang-LM</td>
          <td>通用</td>
          <td>Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，该项目开源了姜子牙通用大模型V1，是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。除姜子牙系列模型之外，该项目还开源了太乙、二郎神系列等模型。</td>
          <td><a href="https://github.com/IDEA-CCNL/Fengshenbang-LM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/IDEA-CCNL/Fengshenbang-LM</a></td>
      </tr>
      <tr>
          <td>BiLLa</td>
          <td>通用</td>
          <td>该项目开源了推理能力增强的中英双语LLaMA模型。模型的主要特性有：较大提升LLaMA的中文理解能力，并尽可能减少对原始LLaMA英文能力的损伤；训练过程增加较多的任务型数据，利用ChatGPT生成解析，强化模型理解任务求解逻辑；全量参数更新，追求更好的生成效果。</td>
          <td><a href="https://github.com/Neutralzz/BiLLa"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Neutralzz/BiLLa</a></td>
      </tr>
      <tr>
          <td>Moss</td>
          <td>通用</td>
          <td>支持中英双语和多种插件的开源对话语言模型，MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。</td>
          <td><a href="https://github.com/OpenLMLab/MOSS"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OpenLMLab/MOSS</a></td>
      </tr>
      <tr>
          <td>Luotuo-Chinese-LLM</td>
          <td>通用</td>
          <td>囊括了一系列中文大语言模型开源项目，包含了一系列基于已有开源模型（ChatGLM, MOSS, LLaMA）进行二次微调的语言模型，指令微调数据集等。</td>
          <td><a href="https://github.com/LC1332/Luotuo-Chinese-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/LC1332/Luotuo-Chinese-LLM</a></td>
      </tr>
      <tr>
          <td>Linly</td>
          <td>通用</td>
          <td>提供中文对话模型 Linly-ChatFlow 、中文基础模型 Linly-Chinese-LLaMA 及其训练数据。 中文基础模型以 LLaMA 为底座，利用中文和中英平行增量预训练。项目汇总了目前公开的多语言指令数据，对中文模型进行了大规模指令跟随训练，实现了 Linly-ChatFlow 对话模型。</td>
          <td><a href="https://github.com/CVI-SZU/Linly"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CVI-SZU/Linly</a></td>
      </tr>
      <tr>
          <td>Firefly</td>
          <td>通用</td>
          <td>Firefly(流萤) 是一个开源的中文大语言模型项目，开源包括数据、微调代码、多个基于Bloom、baichuan等微调好的模型等；支持全量参数指令微调、QLoRA低成本高效指令微调、LoRA指令微调；支持绝大部分主流的开源大模型，如百川baichuan、Ziya、Bloom、LLaMA等。持lora与base model进行权重合并，推理更便捷。</td>
          <td><a href="https://github.com/yangjianxin1/Firefly"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/yangjianxin1/Firefly</a></td>
      </tr>
      <tr>
          <td>ChatYuan</td>
          <td>通用</td>
          <td>元语智能发布的一系列支持中英双语的功能型对话语言大模型，在微调数据、人类反馈强化学习、思维链等方面进行了优化。</td>
          <td><a href="https://github.com/clue-ai/ChatYuan"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/clue-ai/ChatYuan</a></td>
      </tr>
      <tr>
          <td>ChatRWKV</td>
          <td>通用</td>
          <td>开源了一系列基于RWKV架构的Chat模型（包括英文和中文），发布了包括Raven，Novel-ChnEng，Novel-Ch与Novel-ChnEng-ChnPro等模型，可以直接闲聊及进行诗歌，小说等创作，包括7B和14B等规模的模型。</td>
          <td><a href="https://github.com/BlinkDL/ChatRWKV"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/BlinkDL/ChatRWKV</a></td>
      </tr>
      <tr>
          <td>CPM-Bee</td>
          <td>通用</td>
          <td>一个完全开源、允许商用的百亿参数中英文基座模型。它采用Transformer自回归架构（auto-regressive），在超万亿（trillion）高质量语料上进行预训练，拥有强大的基础能力。开发者和研究者可以在CPM-Bee基座模型的基础上在各类场景进行适配来以创建特定领域的应用模型。</td>
          <td><a href="https://github.com/OpenBMB/CPM-Bee"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OpenBMB/CPM-Bee</a></td>
      </tr>
      <tr>
          <td>TigerBot</td>
          <td>通用</td>
          <td>一个多语言多任务的大规模语言模型(LLM)，开源了包括模型：TigerBot-7B, TigerBot-7B-base，TigerBot-180B，基本训练和推理代码，100G预训练数据，涵盖金融、法律、百科的领域数据以及API等。</td>
          <td><a href="https://github.com/TigerResearch/TigerBot"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/TigerResearch/TigerBot</a></td>
      </tr>
      <tr>
          <td>书生·浦语</td>
          <td>通用</td>
          <td>商汤科技、上海AI实验室联合香港中文大学、复旦大学和上海交通大学发布千亿级参数大语言模型“书生·浦语”（InternLM）。据悉，“书生·浦语”具有1040亿参数，基于“包含1.6万亿token的多语种高质量数据集”训练而成。</td>
          <td><a href="https://github.com/InternLM/InternLM-techreport"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/InternLM/InternLM-techreport</a></td>
      </tr>
      <tr>
          <td>Aquila</td>
          <td>通用</td>
          <td>由智源研究院发布，Aquila语言大模型在技术上继承了GPT-3、LLaMA等的架构设计优点，替换了一批更高效的底层算子实现、重新设计实现了中英双语的tokenizer，升级了BMTrain并行训练方法，是在中英文高质量语料基础上从０开始训练的，通过数据质量的控制、多种训练的优化方法，实现在更小的数据集、更短的训练时间，获得比其它开源模型更优的性能。也是首个支持中英双语知识、支持商用许可协议、符合国内数据合规需要的大规模开源语言模型。</td>
          <td><a href="https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila</a></td>
      </tr>
      <tr>
          <td>Baichuan-7B</td>
          <td>通用</td>
          <td>Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。该项目发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。</td>
          <td><a href="https://github.com/baichuan-inc/baichuan-7B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/baichuan-inc/baichuan-7B</a></td>
      </tr>
      <tr>
          <td>Baichuan-13B</td>
          <td>通用</td>
          <td>由百川智能开发的一个开源可商用的大规模预训练语言模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。</td>
          <td><a href="https://github.com/baichuan-inc/Baichuan-13B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/baichuan-inc/Baichuan-13B</a></td>
      </tr>
      <tr>
          <td>Baichuan2</td>
          <td>通用</td>
          <td>由百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练，在多个权威的中文、英文和多语言的通用、领域 benchmark上取得同尺寸最佳的效果，发布包含有7B、13B的Base和经过PPO训练的Chat版本，并提供了Chat版本的4bits量化。</td>
          <td><a href="https://github.com/baichuan-inc/Baichuan2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/baichuan-inc/Baichuan2</a></td>
      </tr>
      <tr>
          <td>Anima</td>
          <td>通用</td>
          <td>由艾写科技开发的一个开源的基于QLoRA的33B中文大语言模型，该模型基于QLoRA的Guanaco 33B模型使用Chinese-Vicuna项目开放的训练数据集guanaco_belle_merge_v1.0进行finetune训练了10000个step，基于Elo rating tournament评估效果较好。</td>
          <td><a href="https://github.com/lyogavin/Anima"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/lyogavin/Anima</a></td>
      </tr>
      <tr>
          <td>KnowLM</td>
          <td>通用</td>
          <td>KnowLM项目旨在发布开源大模型框架及相应模型权重以助力减轻知识谬误问题，包括大模型的知识难更新及存在潜在的错误和偏见等。该项目一期发布了基于Llama的抽取大模型智析，使用中英文语料对LLaMA（13B）进行进一步全量预训练，并基于知识图谱转换指令技术对知识抽取任务进行优化。</td>
          <td><a href="https://github.com/zjunlp/KnowLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zjunlp/KnowLM</a></td>
      </tr>
      <tr>
          <td>BayLing</td>
          <td>通用</td>
          <td>一个具有增强的跨语言对齐的通用大模型，由中国科学院计算技术研究所自然语言处理团队开发。百聆（BayLing）以LLaMA为基座模型，探索了以交互式翻译任务为核心进行指令微调的方法，旨在同时完成语言间对齐以及与人类意图对齐，将LLaMA的生成能力和指令跟随能力从英语迁移到其他语言（中文）。在多语言翻译、交互翻译、通用任务、标准化考试的测评中，百聆在中文/英语中均展现出更好的表现。百聆提供了在线的内测版demo，以供大家体验。</td>
          <td><a href="https://github.com/ictnlp/BayLing"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ictnlp/BayLing</a></td>
      </tr>
      <tr>
          <td>YuLan-Chat</td>
          <td>通用</td>
          <td>YuLan-Chat是中国人民大学GSAI研究人员开发的基于聊天的大语言模型。它是在LLaMA的基础上微调开发的，具有高质量的英文和中文指令。 YuLan-Chat可以与用户聊天，很好地遵循英文或中文指令，并且可以在量化后部署在GPU（A800-80G或RTX3090）上。</td>
          <td><a href="https://github.com/RUC-GSAI/YuLan-Chat"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/RUC-GSAI/YuLan-Chat</a></td>
      </tr>
      <tr>
          <td>PolyLM</td>
          <td>通用</td>
          <td>一个在6400亿个词的数据上从头训练的多语言语言模型，包括两种模型大小(1.7B和13B)。PolyLM覆盖中、英、俄、西、法、葡、德、意、荷、波、阿、土、希伯来、日、韩、泰、越、印尼等语种，特别是对亚洲语种更友好。</td>
          <td><a href="https://github.com/DAMO-NLP-MT/PolyLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DAMO-NLP-MT/PolyLM</a></td>
      </tr>
      <tr>
          <td>huozi</td>
          <td>通用</td>
          <td>由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048，同时还开源了基于RLHF训练的模型以及全人工标注的16.9K中文偏好数据集。</td>
          <td><a href="https://github.com/HIT-SCIR/huozi"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/HIT-SCIR/huozi</a></td>
      </tr>
      <tr>
          <td>YaYi</td>
          <td>通用</td>
          <td>雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，进一步提升了模型性能和安全性。已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。</td>
          <td><a href="https://github.com/wenge-research/YaYi"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/wenge-research/YaYi</a></td>
      </tr>
      <tr>
          <td>XVERSE-13B</td>
          <td>通用</td>
          <td>由深圳元象科技自主研发的支持多语言的大语言模型，使用主流 Decoder-only 的标准Transformer网络结构，支持 8K 的上下文长度（Context Length），为同尺寸模型中最长，构建了 1.4 万亿 token 的高质量、多样化的数据对模型进行充分训练，包含中、英、俄、西等 40 多种语言，通过精细化设置不同类型数据的采样比例，使得中英两种语言表现优异，也能兼顾其他语言效果；基于BPE算法使用上百GB 语料训练了一个词表大小为100,278的分词器，能够同时支持多语言，而无需额外扩展词表。</td>
          <td><a href="https://github.com/xverse-ai/XVERSE-13B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xverse-ai/XVERSE-13B</a></td>
      </tr>
      <tr>
          <td>Skywork</td>
          <td>通用</td>
          <td>该项目开源了天工系列模型，该系列模型在3.2TB高质量多语言和代码数据上进行预训练，开源了包括模型参数，训练数据，评估数据，评估方法。具体包括Skywork-13B-Base模型、Skywork-13B-Chat模型、Skywork-13B-Math模型和Skywork-13B-MM模型，以及每个模型的量化版模型，以支持用户在消费级显卡进行部署和推理。</td>
          <td><a href="https://github.com/SkyworkAI/Skywork"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SkyworkAI/Skywork</a></td>
      </tr>
      <tr>
          <td>Yi</td>
          <td>通用</td>
          <td>该项目开源了Yi-6B和Yi-34B等模型，该系列模型最长可支持200K的超长上下文窗口版本，可以处理约40万汉字超长文本输入，理解超过1000页的PDF文档。</td>
          <td><a href="https://github.com/01-ai/Yi"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/01-ai/Yi</a></td>
      </tr>
      <tr>
          <td>Yuan-2.0</td>
          <td>通用</td>
          <td>该项目开源了由浪潮信息发布的新一代基础语言大模型，具体开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且提供了预训练，微调，推理服务的相关脚本。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。</td>
          <td><a href="https://github.com/IEIT-Yuan/Yuan-2.0"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/IEIT-Yuan/Yuan-2.0</a></td>
      </tr>
      <tr>
          <td>AlpaCare</td>
          <td>专业</td>
          <td>该项目开源了医学大模型AlpaCare，在LLaMA上微调得到。</td>
          <td><a href="https://github.com/XZhang97666/AlpaCare"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/XZhang97666/AlpaCare</a></td>
      </tr>
      <tr>
          <td>Zhongjing</td>
          <td>专业</td>
          <td>该项目开源了首个包含预训练、有监督微调和 RLHF 完整训练流程的中文医学大模型，展现出了很好的泛化能力，在某些对话场景中甚至接近专业医生的专业水平。此外，还开源了一个包含 70,000 条完全来源于真实医患对话的多轮对话数据集。该数据集包含大量医生主动提问的语句，有助于提升模型的主动医疗询问能力。</td>
          <td><a href="https://github.com/SupritYoung/Zhongjing"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SupritYoung/Zhongjing</a></td>
      </tr>
      <tr>
          <td>PMC-LLaMA</td>
          <td>专业</td>
          <td>该项目开源了医疗大模型PMC-LLaMA，包括预训练版本的MedLLaMA_13B和指令微调版本的PMC_LLaMA_13B。</td>
          <td><a href="https://github.com/chaoyi-wu/PMC-LLaMA"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/chaoyi-wu/PMC-LLaMA</a></td>
      </tr>
      <tr>
          <td>ChatDoctor</td>
          <td>专业</td>
          <td>该项目开源了医疗大模型ChatDoctor，在LLaMA的基础上训练得到。</td>
          <td><a href="https://github.com/Kent0n-Li/ChatDoctor"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Kent0n-Li/ChatDoctor</a></td>
      </tr>
      <tr>
          <td>MING</td>
          <td>专业</td>
          <td>该项目开源了医疗大模型MING，基于bloomz-7b指令微调得到MING-7B，支持医疗问答、智能问诊等功能。</td>
          <td><a href="https://github.com/189569400/MedicalGPT-zh"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/189569400/MedicalGPT-zh</a></td>
      </tr>
      <tr>
          <td>IvyGPT</td>
          <td>专业</td>
          <td>该项目开源了医疗大模型IvyGPT，它在高质量的医学问答数据上进行了监督微调，并使用人类反馈的强化学习进行了训练。</td>
          <td><a href="https://github.com/WangRongsheng/IvyGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/WangRongsheng/IvyGPT</a></td>
      </tr>
      <tr>
          <td>PULSE</td>
          <td>专业</td>
          <td>该项目开源了中文医疗大模型PULSE，该模型使用约4,000,000个中文医学领域和通用领域的指令微调数据进行微调，支持医学领域的各种自然语言处理任务，包括健康教育、医师考试问题、报告解读、医疗记录结构化以及模拟诊断和治疗。</td>
          <td><a href="https://github.com/openmedlab/PULSE"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/openmedlab/PULSE</a></td>
      </tr>
      <tr>
          <td>HuangDI</td>
          <td>专业</td>
          <td>该项目开源了中医大模型HuangDI (皇帝)，该模型首先在Ziya-LLaMA-13B-V1基座模型的基础上加入中医教材、中医各类网站数据等语料库，训练出一个具有中医知识理解力的预训练模型，之后在此基础上通过海量的中医古籍指令对话数据及通用指令数据进行有监督微调，使得模型具备中医古籍知识问答能力。</td>
          <td><a href="https://github.com/Zlasejd/HuangDI"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Zlasejd/HuangDI</a></td>
      </tr>
      <tr>
          <td>CMLM-ZhongJing</td>
          <td>专业</td>
          <td>该项目开源了中医大模型ZhongJing (仲景)，该模型旨在阐明中医博大精深之知识，传承古代智慧与现代技术创新，最终为医学领域提供可信赖和专业的工具。</td>
          <td><a href="https://github.com/pariskang/CMLM-ZhongJing"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/pariskang/CMLM-ZhongJing</a></td>
      </tr>
      <tr>
          <td>TCMLLM</td>
          <td>专业</td>
          <td>该项目拟通过大模型方式实现中医临床辅助诊疗（病证诊断、处方推荐等）中医药知识问答等任务，推动中医知识问答、临床辅助诊疗等领域的快速发展。目前针对中医临床智能诊疗问题中的处方推荐任务，发布了中医处方推荐大模型TCMLLM-PR，通过整合真实世界临床病历、医学典籍与中医教科书等数据，构建了包含68k数据条目的处方推荐指令微调数据集，在ChatGLM大模型上进行微调得到。</td>
          <td><a href="https://github.com/2020MEAI/TCMLLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/2020MEAI/TCMLLM</a></td>
      </tr>
  </tbody>
</table></div><div class="post-footer">
    <a href="/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95v2/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h2 class="single-title" itemprop="name headline"><a href="/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/">医学大模型榜单</a>
  </h2><div class="post-meta"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      Anonymous</span></span>&nbsp;<span class="post-publish" title='2023-12-20 19:37:38'>published on <time datetime="2023-12-20">2023-12-20</time></span></div><div class="content"><table>
  <thead>
      <tr>
          <th style="text-align: left">模型</th>
          <th>所在机构</th>
          <th>发布时间</th>
          <th>开源地址</th>
          <th>所用数据</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Med-Flamingo 一种适用于医学领域的多模态少样本学习器</td>
          <td>美国斯坦福大学-</td>
          <td></td>
          <td></td>
          <td>-基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集</td>
      </tr>
      <tr>
          <td style="text-align: left">BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型</td>
          <td>美国斯坦福大学-基础模型研究中心CRFM</td>
          <td>2022年12月</td>
          <td><a href="https://github.com/stanford-crfm/BioMedLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/stanford-crfm/BioMedLM</a></td>
          <td>-基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合</td>
      </tr>
      <tr>
          <td style="text-align: left">BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型</td>
          <td>微软</td>
          <td></td>
          <td><a href="https://github.com/microsoft/BioGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/microsoft/BioGPT</a></td>
          <td>-GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类</td>
      </tr>
      <tr>
          <td style="text-align: left">Med-PaLM2 5400亿参数的转换器语言模型</td>
          <td>谷歌</td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">文心一言</td>
          <td>百度</td>
          <td>2023年2月</td>
          <td></td>
          <td>对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合</td>
      </tr>
      <tr>
          <td style="text-align: left">BioMedGPT-1.6B 生物医药领域基础模型</td>
          <td>清华大学-智能产业研究院</td>
          <td>2023年4月19日</td>
          <td></td>
          <td>-把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言</td>
      </tr>
      <tr>
          <td style="text-align: left">OpenBioMed</td>
          <td>清华大学-智能产业研究院</td>
          <td>2023年8月14日</td>
          <td><a href="https://github.com/BioFM/OpenBioMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/BioFM/OpenBioMed</a></td>
          <td>-基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与<a href="https://github.com/allenai/s2orc/blob/master/README.md"target="_blank" rel="external nofollow noopener noreferrer">S2ORC语料库中</a>的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20+生物研究领域多模态预训练模型</td>
      </tr>
      <tr>
          <td style="text-align: left">本草Huatuo</td>
          <td>哈尔滨工业大学</td>
          <td>2023年3月31日</td>
          <td><a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese</a></td>
          <td>-经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集+医学文献和GPT3.5API构建多轮问答数据</td>
      </tr>
      <tr>
          <td style="text-align: left">春雨慧问 基于大模型的AI在线问诊产品</td>
          <td>春雨医生</td>
          <td>2023年4月26日</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">medGPT 国内首款大模型驱动的 AI 医生</td>
          <td>医联</td>
          <td>2023年4月28日</td>
          <td></td>
          <td>-收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习</td>
      </tr>
      <tr>
          <td style="text-align: left"><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">ChatGLM-6B-Med</a></td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM</a></td>
          <td>-医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调</td>
      </tr>
      <tr>
          <td style="text-align: left">MedPalm</td>
          <td>Google</td>
          <td></td>
          <td></td>
          <td>-在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA</td>
      </tr>
      <tr>
          <td style="text-align: left">ChatDoctor</td>
          <td>德克萨斯大学</td>
          <td></td>
          <td><a href="https://github.com/Kent0n-Li/ChatDoctor"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Kent0n-Li/ChatDoctor</a></td>
          <td>-基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本+5KChatGPT生成数据进行指令微调</td>
      </tr>
      <tr>
          <td style="text-align: left">Chinese-vicuna-med</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md</a>)</td>
          <td>Chinese-vicuna在cMedQA2数据上微调</td>
      </tr>
      <tr>
          <td style="text-align: left">OpenBioMed</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/PharMolix/OpenBioMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/PharMolix/OpenBioMed</a></td>
          <td>知识图谱&amp;20+生物研究领域多模态预训练模型</td>
      </tr>
      <tr>
          <td style="text-align: left">DoctorGLM 基于chatGLM6B模型的医学垂直领域模型</td>
          <td>上海科技大学</td>
          <td></td>
          <td><a href="https://github.com/xionghonglin/DoctorGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xionghonglin/DoctorGLM</a></td>
          <td>ChatDoctor+MedDialog+CMD 多轮对话+单轮指令样本微调GLM</td>
      </tr>
      <tr>
          <td style="text-align: left">MedicalGPT-zh</td>
          <td>上海交通大学</td>
          <td></td>
          <td><a href="https://github.com/MediaBrain-SJTU/MedicalGPT-zh"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/MediaBrain-SJTU/MedicalGPT-zh</a></td>
          <td>-基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA+16个情境下SELF构建情景对话</td>
      </tr>
      <tr>
          <td style="text-align: left">PMC-LLaMA</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/chaoyi-wu/PMC-LLaMA"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/chaoyi-wu/PMC-LLaMA</a></td>
          <td>医疗论文微调Llama</td>
      </tr>
      <tr>
          <td style="text-align: left">NHS-LLM</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/CogStack/OpenGPT/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CogStack/OpenGPT/tree/main</a></td>
          <td>Chatgpt生成的医疗问答，对话，微调模型</td>
      </tr>
      <tr>
          <td style="text-align: left">Med-ChatGLM</td>
          <td></td>
          <td></td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM</a></td>
          <td>医学知识图谱和chatgpt构建中文医学指令数据集+医学文献和chatgpt构建多轮问答数据</td>
      </tr>
      <tr>
          <td style="text-align: left">网新启真13B</td>
          <td>浙江大学</td>
          <td></td>
          <td><a href="https://github.com/CMKRG/QiZhenGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CMKRG/QiZhenGPT</a></td>
          <td>-基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集</td>
      </tr>
      <tr>
          <td style="text-align: left">BenTsao</td>
          <td></td>
          <td></td>
          <td></td>
          <td>-经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果</td>
      </tr>
      <tr>
          <td style="text-align: left">BianQue</td>
          <td></td>
          <td></td>
          <td></td>
          <td>-经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调</td>
      </tr>
      <tr>
          <td style="text-align: left">ChatDD 对话式药物研发助手</td>
          <td>水木分子</td>
          <td></td>
          <td><a href="https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/l5iE9NCA2v0fC_wg3S5DTw</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">ChatDD-FM 100B 全球首个千亿参数多模态生物医药对话大模型</td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">BioMedGPT-10B 16亿参数的轻量级科研版基础模型</td>
          <td>清华智能产业研究院（AIR）</td>
          <td></td>
          <td><a href="https://github.com/PharMolix/OpenBioMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/PharMolix/OpenBioMed</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">太一（Taiyi） 中英双语生物医学大模型</td>
          <td>大连理工大学</td>
          <td></td>
          <td><a href="https://github.com/DUTIR-BioNLP/Taiyi-LLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DUTIR-BioNLP/Taiyi-LLM</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Almanac：临床医学检索增强语言模型</td>
          <td></td>
          <td></td>
          <td><a href="https://arxiv.org/abs/2303.01229"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2303.01229</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">MedLLaMA</td>
          <td></td>
          <td></td>
          <td><a href="https://huggingface.co/chaoyi-wu/MedLLaMA_13B"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/chaoyi-wu/MedLLaMA_13B</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">DISC-MedLLM</td>
          <td><a href="http://fudan-disc.com/"target="_blank" rel="external nofollow noopener noreferrer">复旦大学数据智能与社会计算实验室</a></td>
          <td></td>
          <td><a href="https://github.com/FudanDISC/DISC-MedLLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/FudanDISC/DISC-MedLLM</a></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Clinical Camel</td>
          <td>Vector 研究所（加拿大）</td>
          <td></td>
          <td><a href="https://huggingface.co/wanglab/ClinicalCamel-70B"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/wanglab/ClinicalCamel-70B</a></td>
          <td></td>
      </tr>
  </tbody>
</table></div><div class="post-footer">
    <a href="/model/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/">Read More</a></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h2 class="single-title" itemprop="name headline"><a href="/model/%E9%80%9A%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/">通用大模型榜单</a>
  </h2><div class="post-meta"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      Anonymous</span></span>&nbsp;<span class="post-publish" title='2023-12-20 18:47:46'>published on <time datetime="2023-12-20">2023-12-20</time></span></div><div class="content"><table>
  <thead>
      <tr>
          <th>模型</th>
          <th>开源地址</th>
          <th>参数规模</th>
          <th>所用数据</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ChatGLM-6B</td>
          <td><a href="https://github.com/THUDM/ChatGLM-6B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/ChatGLM-6B</a></td>
          <td>62亿</td>
          <td>1T 标识符的中英双语训练</td>
      </tr>
      <tr>
          <td>ChatGLM2-6B</td>
          <td><a href="https://github.com/THUDM/ChatGLM2-6B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/ChatGLM2-6B</a></td>
          <td></td>
          <td>1.4万亿中英文tokens数据集上训练，并做了模型对齐+SFT</td>
      </tr>
      <tr>
          <td>GLM-130B</td>
          <td><a href="https://github.com/THUDM/GLM-130B"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/THUDM/GLM-130B</a></td>
          <td>1300亿</td>
          <td>4000 亿个文本token训练+SFT</td>
      </tr>
      <tr>
          <td>Chinese-LLaMA-Alpaca</td>
          <td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></td>
          <td></td>
          <td>在原版LLaMA的基础上扩充了中文词表并使用了中文数据进行二次预训练</td>
      </tr>
      <tr>
          <td>Moss</td>
          <td><a href="https://github.com/OpenLMLab/MOSS"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/OpenLMLab/MOSS</a></td>
          <td></td>
          <td>七千亿中英文以及代码单词上预训练</td>
      </tr>
      <tr>
          <td>baichuan-7B</td>
          <td></td>
          <td></td>
          <td>1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096</td>
      </tr>
      <tr>
          <td>AquilaChat-7B</td>
          <td>{{https://mp.weixin.qq.com/s/XkoLnFycG1jPWrNT3w_p-g}}</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>CPM-Bee</td>
          <td></td>
          <td></td>
          <td><a href="https://zhuanlan.zhihu.com/p/639459740"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/639459740</a></td>
      </tr>
      <tr>
          <td>MPT-30b</td>
          <td></td>
          <td></td>
          <td><a href="https://www.bilibili.com/video/BV1UW4y1D7N9/?share_source=copy_web"target="_blank" rel="external nofollow noopener noreferrer">https://www.bilibili.com/video/BV1UW4y1D7N9/?share_source=copy_web</a></td>
      </tr>
  </tbody>
</table></div><div class="post-footer">
    <a href="/model/%E9%80%9A%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A6%9C%E5%8D%95/">Read More</a></div>
</article><ul class="pagination"><li class="page-item active">
          <span class="page-link">
            <a href="/">1</a>
          </span>
        </li><li class="page-item">
          <span class="page-link">
            <a href="/page/2/">2</a>
          </span>
        </li></ul></div></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.148.2"><img class="hugo-icon" src="/images/hugo.min.svg" alt="Hugo logo" /> Hugo</a> | Theme - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.4.0-alpha-20250721024521-a1cd700b"><img class="fixit-icon" src="/images/fixit.min.svg" alt="FixIt logo" /> FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/"></a></span></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">This website works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"version":"v0.4.0-alpha-20250721024521-a1cd700b"};</script><script src="/js/theme.min.js" defer></script></body>
</html>
