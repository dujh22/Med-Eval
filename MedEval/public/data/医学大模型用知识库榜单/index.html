<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>医学大模型用知识库榜单 - Med-Eval</title><meta name="author" content="">
<meta name="description" content=" 数据集名称 大小 出处 下载链接 C4 来自互联网上超过3.65亿个域的超过1560亿个token T5的训练语料：巨型爬虫数据 Common Crawl 做清洗后得到的语料库 https://github.com/allenai/allennlp/discussions/5056 ROOTS 1.6TB的数据集跨越了59种语言(46种自然语言，13种编程语言) BLOOM 的训练语料：62%的文本来自社区选择和记录的语言数据源列表，另外38％的文本来自经过预处理的网络爬取数据集OSCAR, 并通过母语人士的帮助进行了过滤 https://huggingface.co/bigscience-data Pile 825G的语料 22个多样化的高质量子集构成，包括现有的和新构建的子集，许多子集来自学术或专业来 https://github.com/EleutherAI/the-pile WuDaoCorpora 3TB training data and 1.08T trillion Chinese characters，包含有 822 million Web pages 采用20多种规则从100TB原始网页数据中清洗得出最终数据集，注重隐私数据信息的去除，源头上避免GPT-3存在的隐私泄露风险；包含教育、科技等50&#43;个行业数据标签，可以支持多领域预训练模型的训练。 https://data.baai.ac.cn/details/WuDaoCorporaText THUCNews 是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。 中文文本分类数据集：我们在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。使用THUCTC工具包在此数据集上进行评测，准确率可以达到88.6%。 https://github.com/thunlp/THUCTC CLUECorpus2020 通过对Common Crawl的中文部分进行语料清洗，最终得到100GB的高质量中文预训练语料。 https://github.com/CLUEbenchmark/CLUECorpus2020 CDial-GPT 多轮对话数据 本项目提供了一个大规模中文对话数据集，并提供了在此数据集上的中文对话预训练模型（中文GPT模型），更多信息可参考我们的论文。 https://github.com/thu-coai/CDial-GPT RedPajama-Data RedPajama 数据的可重现数据接收，1.2万亿 https://github.com/togethercomputer/RedPajama-Data ">
  <meta itemprop="name" content="医学大模型用知识库榜单">
  <meta itemprop="description" content="数据集名称 大小 出处 下载链接 C4 来自互联网上超过3.65亿个域的超过1560亿个token T5的训练语料：巨型爬虫数据 Common Crawl 做清洗后得到的语料库 https://github.com/allenai/allennlp/discussions/5056 ROOTS 1.6TB的数据集跨越了59种语言(46种自然语言，13种编程语言) BLOOM 的训练语料：62%的文本来自社区选择和记录的语言数据源列表，另外38％的文本来自经过预处理的网络爬取数据集OSCAR, 并通过母语人士的帮助进行了过滤 https://huggingface.co/bigscience-data Pile 825G的语料 22个多样化的高质量子集构成，包括现有的和新构建的子集，许多子集来自学术或专业来 https://github.com/EleutherAI/the-pile WuDaoCorpora 3TB training data and 1.08T trillion Chinese characters，包含有 822 million Web pages 采用20多种规则从100TB原始网页数据中清洗得出最终数据集，注重隐私数据信息的去除，源头上避免GPT-3存在的隐私泄露风险；包含教育、科技等50&#43;个行业数据标签，可以支持多领域预训练模型的训练。 https://data.baai.ac.cn/details/WuDaoCorporaText THUCNews 是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。 中文文本分类数据集：我们在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。使用THUCTC工具包在此数据集上进行评测，准确率可以达到88.6%。 https://github.com/thunlp/THUCTC CLUECorpus2020 通过对Common Crawl的中文部分进行语料清洗，最终得到100GB的高质量中文预训练语料。 https://github.com/CLUEbenchmark/CLUECorpus2020 CDial-GPT 多轮对话数据 本项目提供了一个大规模中文对话数据集，并提供了在此数据集上的中文对话预训练模型（中文GPT模型），更多信息可参考我们的论文。 https://github.com/thu-coai/CDial-GPT RedPajama-Data RedPajama 数据的可重现数据接收，1.2万亿 https://github.com/togethercomputer/RedPajama-Data">
  <meta itemprop="datePublished" content="2023-12-20T19:48:51+08:00">
  <meta itemprop="dateModified" content="2023-12-20T19:48:51+08:00">
  <meta itemprop="wordCount" content="53"><meta property="og:url" content="https://dujh22.github.io/data/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%94%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%A6%9C%E5%8D%95/">
  <meta property="og:site_name" content="Med-Eval">
  <meta property="og:title" content="医学大模型用知识库榜单">
  <meta property="og:description" content="数据集名称 大小 出处 下载链接 C4 来自互联网上超过3.65亿个域的超过1560亿个token T5的训练语料：巨型爬虫数据 Common Crawl 做清洗后得到的语料库 https://github.com/allenai/allennlp/discussions/5056 ROOTS 1.6TB的数据集跨越了59种语言(46种自然语言，13种编程语言) BLOOM 的训练语料：62%的文本来自社区选择和记录的语言数据源列表，另外38％的文本来自经过预处理的网络爬取数据集OSCAR, 并通过母语人士的帮助进行了过滤 https://huggingface.co/bigscience-data Pile 825G的语料 22个多样化的高质量子集构成，包括现有的和新构建的子集，许多子集来自学术或专业来 https://github.com/EleutherAI/the-pile WuDaoCorpora 3TB training data and 1.08T trillion Chinese characters，包含有 822 million Web pages 采用20多种规则从100TB原始网页数据中清洗得出最终数据集，注重隐私数据信息的去除，源头上避免GPT-3存在的隐私泄露风险；包含教育、科技等50&#43;个行业数据标签，可以支持多领域预训练模型的训练。 https://data.baai.ac.cn/details/WuDaoCorporaText THUCNews 是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。 中文文本分类数据集：我们在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。使用THUCTC工具包在此数据集上进行评测，准确率可以达到88.6%。 https://github.com/thunlp/THUCTC CLUECorpus2020 通过对Common Crawl的中文部分进行语料清洗，最终得到100GB的高质量中文预训练语料。 https://github.com/CLUEbenchmark/CLUECorpus2020 CDial-GPT 多轮对话数据 本项目提供了一个大规模中文对话数据集，并提供了在此数据集上的中文对话预训练模型（中文GPT模型），更多信息可参考我们的论文。 https://github.com/thu-coai/CDial-GPT RedPajama-Data RedPajama 数据的可重现数据接收，1.2万亿 https://github.com/togethercomputer/RedPajama-Data">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="data">
    <meta property="article:published_time" content="2023-12-20T19:48:51+08:00">
    <meta property="article:modified_time" content="2023-12-20T19:48:51+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="医学大模型用知识库榜单">
  <meta name="twitter:description" content="数据集名称 大小 出处 下载链接 C4 来自互联网上超过3.65亿个域的超过1560亿个token T5的训练语料：巨型爬虫数据 Common Crawl 做清洗后得到的语料库 https://github.com/allenai/allennlp/discussions/5056 ROOTS 1.6TB的数据集跨越了59种语言(46种自然语言，13种编程语言) BLOOM 的训练语料：62%的文本来自社区选择和记录的语言数据源列表，另外38％的文本来自经过预处理的网络爬取数据集OSCAR, 并通过母语人士的帮助进行了过滤 https://huggingface.co/bigscience-data Pile 825G的语料 22个多样化的高质量子集构成，包括现有的和新构建的子集，许多子集来自学术或专业来 https://github.com/EleutherAI/the-pile WuDaoCorpora 3TB training data and 1.08T trillion Chinese characters，包含有 822 million Web pages 采用20多种规则从100TB原始网页数据中清洗得出最终数据集，注重隐私数据信息的去除，源头上避免GPT-3存在的隐私泄露风险；包含教育、科技等50&#43;个行业数据标签，可以支持多领域预训练模型的训练。 https://data.baai.ac.cn/details/WuDaoCorporaText THUCNews 是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。 中文文本分类数据集：我们在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。使用THUCTC工具包在此数据集上进行评测，准确率可以达到88.6%。 https://github.com/thunlp/THUCTC CLUECorpus2020 通过对Common Crawl的中文部分进行语料清洗，最终得到100GB的高质量中文预训练语料。 https://github.com/CLUEbenchmark/CLUECorpus2020 CDial-GPT 多轮对话数据 本项目提供了一个大规模中文对话数据集，并提供了在此数据集上的中文对话预训练模型（中文GPT模型），更多信息可参考我们的论文。 https://github.com/thu-coai/CDial-GPT RedPajama-Data RedPajama 数据的可重现数据接收，1.2万亿 https://github.com/togethercomputer/RedPajama-Data">
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" type="text/html" href="https://dujh22.github.io/data/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%94%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%A6%9C%E5%8D%95/" title="医学大模型用知识库榜单 - Med-Eval" /><link rel="prev" type="text/html" href="https://dujh22.github.io/board/" title="Med-Eval 榜单" /><link rel="next" type="text/html" href="https://dujh22.github.io/data/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%94%A8%E8%AF%AD%E6%96%99%E5%BA%93%E6%A6%9C%E5%8D%95/" title="医学大模型训练用语料库榜单" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "医学大模型用知识库榜单",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/dujh22.github.io\/data\/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%94%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%A6%9C%E5%8D%95\/"
    },"genre": "data","wordcount":  53 ,
    "url": "https:\/\/dujh22.github.io\/data\/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%94%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%A6%9C%E5%8D%95\/","datePublished": "2023-12-20T19:48:51+08:00","dateModified": "2023-12-20T19:48:51+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script><script src="/js/head/color-scheme.min.js"></script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="Med-Eval"><span class="header-title-text">Med-Eval</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> Archives</a></li><li class="menu-item">
              <a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> Categories</a></li><li class="menu-item">
              <a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> Tags</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="Med-Eval"><span class="header-title-text">Med-Eval</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="menu-item"><a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> Archives</a></li><li class="menu-item"><a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> Categories</a></li><li class="menu-item"><a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> Tags</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><article class="page single special">
    <div class="header"><h1 class="single-title animate__animated animate__pulse animate__faster">医学大模型用知识库榜单</h1></div><div class="content" id="content"><table>
  <thead>
      <tr>
          <th>数据集名称</th>
          <th>大小</th>
          <th>出处</th>
          <th>下载链接</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>C4</td>
          <td>来自互联网上超过3.65亿个域的超过1560亿个token</td>
          <td>T5的训练语料：巨型爬虫数据 Common Crawl 做清洗后得到的语料库</td>
          <td><a href="https://github.com/allenai/allennlp/discussions/5056"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/allenai/allennlp/discussions/5056</a></td>
      </tr>
      <tr>
          <td>ROOTS</td>
          <td>1.6TB的数据集跨越了59种语言(46种自然语言，13种编程语言)</td>
          <td>BLOOM 的训练语料：62%的文本来自社区选择和记录的语言数据源列表，另外38％的文本来自经过预处理的网络爬取数据集OSCAR, 并通过母语人士的帮助进行了过滤</td>
          <td><a href="https://huggingface.co/bigscience-data"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/bigscience-data</a></td>
      </tr>
      <tr>
          <td>Pile</td>
          <td>825G的语料</td>
          <td>22个多样化的高质量子集构成，包括现有的和新构建的子集，许多子集来自学术或专业来</td>
          <td><a href="https://github.com/EleutherAI/the-pile"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/EleutherAI/the-pile</a></td>
      </tr>
      <tr>
          <td>WuDaoCorpora</td>
          <td>3TB training data and 1.08T trillion Chinese characters，包含有 822 million Web pages</td>
          <td>采用20多种规则从100TB原始网页数据中清洗得出最终数据集，注重隐私数据信息的去除，源头上避免GPT-3存在的隐私泄露风险；包含教育、科技等50+个行业数据标签，可以支持多领域预训练模型的训练。</td>
          <td><a href="https://data.baai.ac.cn/details/WuDaoCorporaText"target="_blank" rel="external nofollow noopener noreferrer">https://data.baai.ac.cn/details/WuDaoCorporaText</a></td>
      </tr>
      <tr>
          <td>THUCNews</td>
          <td>是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。</td>
          <td>中文文本分类数据集：我们在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。使用THUCTC工具包在此数据集上进行评测，准确率可以达到88.6%。</td>
          <td><a href="https://github.com/thunlp/THUCTC"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/thunlp/THUCTC</a></td>
      </tr>
      <tr>
          <td>CLUECorpus2020</td>
          <td>通过对<a href="http://commoncrawl.org/"target="_blank" rel="external nofollow noopener noreferrer">Common Crawl</a>的中文部分进行语料清洗，最终得到100GB的高质量中文预训练语料。</td>
          <td></td>
          <td><a href="https://github.com/CLUEbenchmark/CLUECorpus2020"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CLUEbenchmark/CLUECorpus2020</a></td>
      </tr>
      <tr>
          <td>CDial-GPT</td>
          <td>多轮对话数据</td>
          <td>本项目提供了一个大规模中文对话数据集，并提供了在此数据集上的中文对话预训练模型（中文GPT模型），更多信息可参考我们的<a href="https://arxiv.org/abs/2008.03946"target="_blank" rel="external nofollow noopener noreferrer">论文</a>。</td>
          <td><a href="https://github.com/thu-coai/CDial-GPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/thu-coai/CDial-GPT</a></td>
      </tr>
      <tr>
          <td>RedPajama-Data</td>
          <td>RedPajama 数据的可重现数据接收，1.2万亿</td>
          <td></td>
          <td><a href="https://github.com/togethercomputer/RedPajama-Data"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/togethercomputer/RedPajama-Data</a></td>
      </tr>
  </tbody>
</table>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.148.2"><img class="hugo-icon" src="/images/hugo.min.svg" alt="Hugo logo" /> Hugo</a> | Theme - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.4.0-alpha-20250721024521-a1cd700b"><img class="fixit-icon" src="/images/fixit.min.svg" alt="FixIt logo" /> FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/"></a></span></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">This website works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"version":"v0.4.0-alpha-20250721024521-a1cd700b"};</script><script src="/js/theme.min.js" defer></script></body>
</html>
