<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>医学大模型训练用语料库榜单 - Med-Eval</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="
  
      
          模型
          开源地址
          所用数据
          数据下载地址
      
  
  
      
          Med-Flamingo 一种适用于医学领域的多模态少样本学习器
          
          -基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集
          https://huggingface.co/datasets/axiong/pmc_oa
      
      
          BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型
          https://github.com/stanford-crfm/BioMedLM
          -基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合
          https://github.com/thoppe/The-Pile-PubMed
      
      
          BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型
          https://github.com/microsoft/BioGPT
          -GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类
          
      
      
          Med-PaLM2 5400亿参数的转换器语言模型
          
          
          
      
      
          文心一言
          
          对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合
          
      
      
          BioMedGPT-1.6B 生物医药领域基础模型
          
          -把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言
          
      
      
          OpenBioMed
          https://github.com/BioFM/OpenBioMed
          -基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与S2ORC语料库中的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20&#43;生物研究领域多模态预训练模型
          
      
      
          本草Huatuo
          https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese
          -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集&#43;医学文献和GPT3.5API构建多轮问答数据
          https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data-literature https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data
      
      
          春雨慧问 基于大模型的AI在线问诊产品
          
          
          
      
      
          medGPT 国内首款大模型驱动的 AI 医生
          
          -收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习
          
      
      
          ChatGLM-6B-Med
          https://github.com/SCIR-HI/Med-ChatGLM
          -医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调
          
      
      
          MedPalm
          
          -在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA
          https://github.com/SCIR-HI/Med-ChatGLM/tree/main/data
      
      
          ChatDoctor
          https://github.com/Kent0n-Li/ChatDoctor
          -基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本&#43;5KChatGPT生成数据进行指令微调
          https://github.com/Kent0n-Li/ChatDoctor
      
      
          Chinese-vicuna-med
          https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md)
          Chinese-vicuna在cMedQA2数据上微调
          https://github.com/zhangsheng93/cMedQA2
      
      
          OpenBioMed
          https://github.com/PharMolix/OpenBioMed
          知识图谱&amp;20&#43;生物研究领域多模态预训练模型
          CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools
      
      
          DoctorGLM 基于chatGLM6B模型的医学垂直领域模型
          https://github.com/xionghonglin/DoctorGLM
          ChatDoctor&#43;MedDialog&#43;CMD 多轮对话&#43;单轮指令样本微调GLM
          CMD.中文医疗对话数据集 https://tianchi.aliyun.com/dataset/90163 MedDialog中文医疗对话数据集https://tianchi.aliyun.com/dataset/92110 HearlthcareMagic
      
      
          MedicalGPT-zh
          https://github.com/MediaBrain-SJTU/MedicalGPT-zh
          -基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA&#43;16个情境下SELF构建情景对话
          
      
      
          PMC-LLaMA
          https://github.com/chaoyi-wu/PMC-LLaMA
          医疗论文微调Llama
          https://github.com/allenai/s2orc
      
      
          NHS-LLM
          https://github.com/CogStack/OpenGPT/tree/main
          Chatgpt生成的医疗问答，对话，微调模型
          https://github.com/CogStack/OpenGPT/tree/main
      
      
          Med-ChatGLM
          https://github.com/SCIR-HI/Med-ChatGLM
          医学知识图谱和chatgpt构建中文医学指令数据集&#43;医学文献和chatgpt构建多轮问答数据
          CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools
      
      
          网新启真13B
          https://github.com/CMKRG/QiZhenGPT
          -基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集
          https://github.com/CMKRG/QiZhenGPT/blob/main/data/train/sft-20k.json
      
      
          BenTsao
          
          -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果
          
      
      
          BianQue
          https://blog.csdn.net/m0_37201243/article/details/130334076
          -经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调
          hinese medical dialogue data 中文医疗问答数据集 https://github.com/Toyhom/Chinese-medical-dialogue-data cMedQA2 https://github.com/zhangsheng93/cMedQA2 IMCS-21 https://github.com/lemuria-wchen/imcs21 Medical-Dialogue-System https://github.com/UCSD-AI4H/Medical-Dialogue-System
      
  
" /><meta name="keywords" content='Hugo, FixIt' />
  <meta itemprop="name" content="医学大模型训练用语料库榜单">
  <meta itemprop="description" content="模型 开源地址 所用数据 数据下载地址 Med-Flamingo 一种适用于医学领域的多模态少样本学习器 -基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集 https://huggingface.co/datasets/axiong/pmc_oa BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型 https://github.com/stanford-crfm/BioMedLM -基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合 https://github.com/thoppe/The-Pile-PubMed BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型 https://github.com/microsoft/BioGPT -GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类 Med-PaLM2 5400亿参数的转换器语言模型 文心一言 对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合 BioMedGPT-1.6B 生物医药领域基础模型 -把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言 OpenBioMed https://github.com/BioFM/OpenBioMed -基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与S2ORC语料库中的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 本草Huatuo https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集&#43;医学文献和GPT3.5API构建多轮问答数据 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data-literature https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data 春雨慧问 基于大模型的AI在线问诊产品 medGPT 国内首款大模型驱动的 AI 医生 -收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习 ChatGLM-6B-Med https://github.com/SCIR-HI/Med-ChatGLM -医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调 MedPalm -在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA https://github.com/SCIR-HI/Med-ChatGLM/tree/main/data ChatDoctor https://github.com/Kent0n-Li/ChatDoctor -基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本&#43;5KChatGPT生成数据进行指令微调 https://github.com/Kent0n-Li/ChatDoctor Chinese-vicuna-med https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md) Chinese-vicuna在cMedQA2数据上微调 https://github.com/zhangsheng93/cMedQA2 OpenBioMed https://github.com/PharMolix/OpenBioMed 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools DoctorGLM 基于chatGLM6B模型的医学垂直领域模型 https://github.com/xionghonglin/DoctorGLM ChatDoctor&#43;MedDialog&#43;CMD 多轮对话&#43;单轮指令样本微调GLM CMD.中文医疗对话数据集 https://tianchi.aliyun.com/dataset/90163 MedDialog中文医疗对话数据集https://tianchi.aliyun.com/dataset/92110 HearlthcareMagic MedicalGPT-zh https://github.com/MediaBrain-SJTU/MedicalGPT-zh -基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA&#43;16个情境下SELF构建情景对话 PMC-LLaMA https://github.com/chaoyi-wu/PMC-LLaMA 医疗论文微调Llama https://github.com/allenai/s2orc NHS-LLM https://github.com/CogStack/OpenGPT/tree/main Chatgpt生成的医疗问答，对话，微调模型 https://github.com/CogStack/OpenGPT/tree/main Med-ChatGLM https://github.com/SCIR-HI/Med-ChatGLM 医学知识图谱和chatgpt构建中文医学指令数据集&#43;医学文献和chatgpt构建多轮问答数据 CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools 网新启真13B https://github.com/CMKRG/QiZhenGPT -基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集 https://github.com/CMKRG/QiZhenGPT/blob/main/data/train/sft-20k.json BenTsao -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果 BianQue https://blog.csdn.net/m0_37201243/article/details/130334076 -经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调 hinese medical dialogue data 中文医疗问答数据集 https://github.com/Toyhom/Chinese-medical-dialogue-data cMedQA2 https://github.com/zhangsheng93/cMedQA2 IMCS-21 https://github.com/lemuria-wchen/imcs21 Medical-Dialogue-System https://github.com/UCSD-AI4H/Medical-Dialogue-System">
  <meta itemprop="datePublished" content="2023-12-20T19:49:10+08:00">
  <meta itemprop="dateModified" content="2023-12-20T19:49:10+08:00">
  <meta itemprop="wordCount" content="145"><meta property="og:url" content="https://dujh22.github.io/data/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%94%A8%E8%AF%AD%E6%96%99%E5%BA%93%E6%A6%9C%E5%8D%95/">
  <meta property="og:site_name" content="Med-Eval">
  <meta property="og:title" content="医学大模型训练用语料库榜单">
  <meta property="og:description" content="模型 开源地址 所用数据 数据下载地址 Med-Flamingo 一种适用于医学领域的多模态少样本学习器 -基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集 https://huggingface.co/datasets/axiong/pmc_oa BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型 https://github.com/stanford-crfm/BioMedLM -基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合 https://github.com/thoppe/The-Pile-PubMed BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型 https://github.com/microsoft/BioGPT -GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类 Med-PaLM2 5400亿参数的转换器语言模型 文心一言 对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合 BioMedGPT-1.6B 生物医药领域基础模型 -把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言 OpenBioMed https://github.com/BioFM/OpenBioMed -基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与S2ORC语料库中的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 本草Huatuo https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集&#43;医学文献和GPT3.5API构建多轮问答数据 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data-literature https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data 春雨慧问 基于大模型的AI在线问诊产品 medGPT 国内首款大模型驱动的 AI 医生 -收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习 ChatGLM-6B-Med https://github.com/SCIR-HI/Med-ChatGLM -医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调 MedPalm -在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA https://github.com/SCIR-HI/Med-ChatGLM/tree/main/data ChatDoctor https://github.com/Kent0n-Li/ChatDoctor -基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本&#43;5KChatGPT生成数据进行指令微调 https://github.com/Kent0n-Li/ChatDoctor Chinese-vicuna-med https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md) Chinese-vicuna在cMedQA2数据上微调 https://github.com/zhangsheng93/cMedQA2 OpenBioMed https://github.com/PharMolix/OpenBioMed 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools DoctorGLM 基于chatGLM6B模型的医学垂直领域模型 https://github.com/xionghonglin/DoctorGLM ChatDoctor&#43;MedDialog&#43;CMD 多轮对话&#43;单轮指令样本微调GLM CMD.中文医疗对话数据集 https://tianchi.aliyun.com/dataset/90163 MedDialog中文医疗对话数据集https://tianchi.aliyun.com/dataset/92110 HearlthcareMagic MedicalGPT-zh https://github.com/MediaBrain-SJTU/MedicalGPT-zh -基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA&#43;16个情境下SELF构建情景对话 PMC-LLaMA https://github.com/chaoyi-wu/PMC-LLaMA 医疗论文微调Llama https://github.com/allenai/s2orc NHS-LLM https://github.com/CogStack/OpenGPT/tree/main Chatgpt生成的医疗问答，对话，微调模型 https://github.com/CogStack/OpenGPT/tree/main Med-ChatGLM https://github.com/SCIR-HI/Med-ChatGLM 医学知识图谱和chatgpt构建中文医学指令数据集&#43;医学文献和chatgpt构建多轮问答数据 CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools 网新启真13B https://github.com/CMKRG/QiZhenGPT -基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集 https://github.com/CMKRG/QiZhenGPT/blob/main/data/train/sft-20k.json BenTsao -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果 BianQue https://blog.csdn.net/m0_37201243/article/details/130334076 -经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调 hinese medical dialogue data 中文医疗问答数据集 https://github.com/Toyhom/Chinese-medical-dialogue-data cMedQA2 https://github.com/zhangsheng93/cMedQA2 IMCS-21 https://github.com/lemuria-wchen/imcs21 Medical-Dialogue-System https://github.com/UCSD-AI4H/Medical-Dialogue-System">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="data">
    <meta property="article:published_time" content="2023-12-20T19:49:10+08:00">
    <meta property="article:modified_time" content="2023-12-20T19:49:10+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="医学大模型训练用语料库榜单">
  <meta name="twitter:description" content="模型 开源地址 所用数据 数据下载地址 Med-Flamingo 一种适用于医学领域的多模态少样本学习器 -基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集 https://huggingface.co/datasets/axiong/pmc_oa BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型 https://github.com/stanford-crfm/BioMedLM -基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合 https://github.com/thoppe/The-Pile-PubMed BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型 https://github.com/microsoft/BioGPT -GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类 Med-PaLM2 5400亿参数的转换器语言模型 文心一言 对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合 BioMedGPT-1.6B 生物医药领域基础模型 -把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言 OpenBioMed https://github.com/BioFM/OpenBioMed -基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与S2ORC语料库中的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 本草Huatuo https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集&#43;医学文献和GPT3.5API构建多轮问答数据 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data-literature https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data 春雨慧问 基于大模型的AI在线问诊产品 medGPT 国内首款大模型驱动的 AI 医生 -收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习 ChatGLM-6B-Med https://github.com/SCIR-HI/Med-ChatGLM -医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调 MedPalm -在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA https://github.com/SCIR-HI/Med-ChatGLM/tree/main/data ChatDoctor https://github.com/Kent0n-Li/ChatDoctor -基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本&#43;5KChatGPT生成数据进行指令微调 https://github.com/Kent0n-Li/ChatDoctor Chinese-vicuna-med https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md) Chinese-vicuna在cMedQA2数据上微调 https://github.com/zhangsheng93/cMedQA2 OpenBioMed https://github.com/PharMolix/OpenBioMed 知识图谱&amp;20&#43;生物研究领域多模态预训练模型 CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools DoctorGLM 基于chatGLM6B模型的医学垂直领域模型 https://github.com/xionghonglin/DoctorGLM ChatDoctor&#43;MedDialog&#43;CMD 多轮对话&#43;单轮指令样本微调GLM CMD.中文医疗对话数据集 https://tianchi.aliyun.com/dataset/90163 MedDialog中文医疗对话数据集https://tianchi.aliyun.com/dataset/92110 HearlthcareMagic MedicalGPT-zh https://github.com/MediaBrain-SJTU/MedicalGPT-zh -基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA&#43;16个情境下SELF构建情景对话 PMC-LLaMA https://github.com/chaoyi-wu/PMC-LLaMA 医疗论文微调Llama https://github.com/allenai/s2orc NHS-LLM https://github.com/CogStack/OpenGPT/tree/main Chatgpt生成的医疗问答，对话，微调模型 https://github.com/CogStack/OpenGPT/tree/main Med-ChatGLM https://github.com/SCIR-HI/Med-ChatGLM 医学知识图谱和chatgpt构建中文医学指令数据集&#43;医学文献和chatgpt构建多轮问答数据 CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools 网新启真13B https://github.com/CMKRG/QiZhenGPT -基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集 https://github.com/CMKRG/QiZhenGPT/blob/main/data/train/sft-20k.json BenTsao -经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果 BianQue https://blog.csdn.net/m0_37201243/article/details/130334076 -经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调 hinese medical dialogue data 中文医疗问答数据集 https://github.com/Toyhom/Chinese-medical-dialogue-data cMedQA2 https://github.com/zhangsheng93/cMedQA2 IMCS-21 https://github.com/lemuria-wchen/imcs21 Medical-Dialogue-System https://github.com/UCSD-AI4H/Medical-Dialogue-System">
<meta name="application-name" content="Med-Eval">
<meta name="apple-mobile-web-app-title" content="Med-Eval"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="https://dujh22.github.io/data/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%94%A8%E8%AF%AD%E6%96%99%E5%BA%93%E6%A6%9C%E5%8D%95/" /><link rel="prev" href="https://dujh22.github.io/data/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%94%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%A6%9C%E5%8D%95/" /><link rel="next" href="https://dujh22.github.io/data/%E5%8C%BB%E5%AD%A6%E5%9C%BA%E6%99%AF/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "医学大模型训练用语料库榜单",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/dujh22.github.io\/data\/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%94%A8%E8%AF%AD%E6%96%99%E5%BA%93%E6%A6%9C%E5%8D%95\/"
    },"genre": "data","wordcount":  145 ,
    "url": "https:\/\/dujh22.github.io\/data\/%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%94%A8%E8%AF%AD%E6%96%99%E5%BA%93%E6%A6%9C%E5%8D%95\/","datePublished": "2023-12-20T19:49:10+08:00","dateModified": "2023-12-20T19:49:10+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="Med-Eval"><img loading="lazy" src="/fixit.svg" data-title="Med-Eval" data-alt="Med-Eval" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-desktop" class="typeit"></span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/med-eval"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li class="menu-item has-children">
              <a
                class="menu-link"
                href="/board"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 榜单</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
                <ul class="sub-menu">
                  <li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/model/"
                          
                          
                        >模型榜单</a>
                      </li><li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/data/"
                          
                          
                        >数据榜单</a>
                      </li><li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/tool/%E6%B5%8B%E8%AF%84%E5%B7%A5%E5%85%B7"
                          
                          
                        >测评工具</a>
                      </li></ul></li><li class="menu-item has-children">
              <a
                class="menu-link"
                href="https://www.opende.org.cn/arena/"
                
                rel="noopener noreferrer" target="_blank"
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 竞技场</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
                <ul class="sub-menu">
                  <li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="https://www.opende.org.cn/demo/chat"
                          
                          rel="noopener noreferrer" target="_blank"
                        ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> AiMed</a>
                      </li></ul></li><li class="menu-item">
              <a
                class="menu-link"
                href="/more"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 加入竞技</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="Med-Eval"><img loading="lazy" src="/fixit.svg" data-title="/fixit.svg" data-alt="/fixit.svg" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-title-mobile" class="typeit"></span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/med-eval"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li
              class="menu-item"
            ><span class="nested-item">
                  <a
                    class="menu-link"
                    href="/board"
                    
                    
                  ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 榜单</a>
                  <i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden="true"></i>
                </span>
                <ul class="sub-menu">
                  <li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/model/"
                          
                          
                        >模型榜单</a>
                      </li><li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/data/"
                          
                          
                        >数据榜单</a>
                      </li><li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="/tool/%E6%B5%8B%E8%AF%84%E5%B7%A5%E5%85%B7"
                          
                          
                        >测评工具</a>
                      </li></ul></li><li
              class="menu-item"
            ><span class="nested-item">
                  <a
                    class="menu-link"
                    href="https://www.opende.org.cn/arena/"
                    
                    rel="noopener noreferrer" target="_blank"
                  ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 竞技场</a>
                  <i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden="true"></i>
                </span>
                <ul class="sub-menu">
                  <li
                        class="menu-item"
                      >
                        <a
                          class="menu-link"
                          href="https://www.opende.org.cn/demo/chat"
                          
                          rel="noopener noreferrer" target="_blank"
                        ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> AiMed</a>
                      </li></ul></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/more"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 加入竞技</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><article class="page single special">
    <div class="header"><h1 class="single-title animate__animated animate__pulse animate__faster">医学大模型训练用语料库榜单</h1></div><div
      class="content"
      id="content"
      
      
    ><table>
  <thead>
      <tr>
          <th>模型</th>
          <th>开源地址</th>
          <th>所用数据</th>
          <th>数据下载地址</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Med-Flamingo 一种适用于医学领域的多模态少样本学习器</td>
          <td></td>
          <td>-基于OpenFlamingo-9B -对出版物和教科书中成对和交错的医学图像-文本数据进行预训练-4K数据集</td>
          <td><a href="https://huggingface.co/datasets/axiong/pmc_oa"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/datasets/axiong/pmc_oa</a></td>
      </tr>
      <tr>
          <td>BioMedLM（原PubMed GPT 2.7B） 用于生物医学文本的特定领域大型语言模型</td>
          <td><a href="https://github.com/stanford-crfm/BioMedLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/stanford-crfm/BioMedLM</a></td>
          <td>-基于HuggingFace GPT模型 -2.7B的参数和1024个标记的最大上下文长度 -数据是Pile数据集的部分——PubMed Abstracts和PubMed Central：涵盖由美国国立卫生研究院策划的来自生物医学文献的 16 万份摘要和 5 万篇全文文章的集合</td>
          <td><a href="https://github.com/thoppe/The-Pile-PubMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/thoppe/The-Pile-PubMed</a></td>
      </tr>
      <tr>
          <td>BioGPT 大规模生物医学文献上进行预训练的特定领域生成式 Transformer 语言模型</td>
          <td><a href="https://github.com/microsoft/BioGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/microsoft/BioGPT</a></td>
          <td>-GPT2作为骨干模型 -从 PubMed 收集文章，PubMed 是一个生物医学研究领域的大型数据库，团队共产生1500万条带有标题和摘要的内容 -使用 3.57 亿个参数改进了预训练的基于 GPT-2 的模型，用于下游任务：端到端关系提取、文本生成、问题回答和文档分类</td>
          <td></td>
      </tr>
      <tr>
          <td>Med-PaLM2 5400亿参数的转换器语言模型</td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>文心一言</td>
          <td></td>
          <td>对中国医疗信息数据提供商GBI Health的并购，通过GBI与其类ChatGPT产品“文心一言”等的结合</td>
          <td></td>
      </tr>
      <tr>
          <td>BioMedGPT-1.6B 生物医药领域基础模型</td>
          <td></td>
          <td>-把分子语言中蕴含的知识以及长期以来通过实验总结的文本和知识图谱信息融合压缩到一个大规模语言模型中，从而实现从序列模式中学习生物结构和功能规律，通过AI解码生命语言</td>
          <td></td>
      </tr>
      <tr>
          <td>OpenBioMed</td>
          <td><a href="https://github.com/BioFM/OpenBioMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/BioFM/OpenBioMed</a></td>
          <td>-基于Llama2的大型生成语言模型 -从Llama2-7B-Chat与<a href="https://github.com/allenai/s2orc/blob/master/README.md"target="_blank" rel="external nofollow noopener noreferrer">S2ORC语料库中</a>的数百万篇生物医学论文进行了微调 -开源轻量版BioMedGPT, 知识图谱&amp;20+生物研究领域多模态预训练模型</td>
          <td></td>
      </tr>
      <tr>
          <td>本草Huatuo</td>
          <td><a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese</a></td>
          <td>-经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型 -医学知识图谱和GPT3.5API构建中文医学指令数据集+医学文献和GPT3.5API构建多轮问答数据</td>
          <td><a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data-literature"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data-literature</a> <a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/data</a></td>
      </tr>
      <tr>
          <td>春雨慧问 基于大模型的AI在线问诊产品</td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>medGPT 国内首款大模型驱动的 AI 医生</td>
          <td></td>
          <td>-收集整理接近 20 亿条真实医患沟通对话、检验检测和病例信息进行深度训练学习 -同时利用医生真实反馈进行强化学习</td>
          <td></td>
      </tr>
      <tr>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">ChatGLM-6B-Med</a></td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM</a></td>
          <td>-医学知识图谱和GPT3.5 API构建了中文医学指令数据集 -并在此基础上对ChatGLM-6B进行了指令微调</td>
          <td></td>
      </tr>
      <tr>
          <td>MedPalm</td>
          <td></td>
          <td>-在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到 -同时构建了MultiMedQA</td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM/tree/main/data"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM/tree/main/data</a></td>
      </tr>
      <tr>
          <td>ChatDoctor</td>
          <td><a href="https://github.com/Kent0n-Li/ChatDoctor"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Kent0n-Li/ChatDoctor</a></td>
          <td>-基于Llama7b模型的医学垂直领域模型 -110K真实医患对话样本+5KChatGPT生成数据进行指令微调</td>
          <td><a href="https://github.com/Kent0n-Li/ChatDoctor"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Kent0n-Li/ChatDoctor</a></td>
      </tr>
      <tr>
          <td>Chinese-vicuna-med</td>
          <td><a href="https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Facico/Chinese-Vicuna/blob/master/docs/performance-medical.md</a>)</td>
          <td>Chinese-vicuna在cMedQA2数据上微调</td>
          <td><a href="https://github.com/zhangsheng93/cMedQA2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zhangsheng93/cMedQA2</a></td>
      </tr>
      <tr>
          <td>OpenBioMed</td>
          <td><a href="https://github.com/PharMolix/OpenBioMed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/PharMolix/OpenBioMed</a></td>
          <td>知识图谱&amp;20+生物研究领域多模态预训练模型</td>
          <td>CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools</td>
      </tr>
      <tr>
          <td>DoctorGLM 基于chatGLM6B模型的医学垂直领域模型</td>
          <td><a href="https://github.com/xionghonglin/DoctorGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xionghonglin/DoctorGLM</a></td>
          <td>ChatDoctor+MedDialog+CMD 多轮对话+单轮指令样本微调GLM</td>
          <td>CMD.中文医疗对话数据集 <a href="https://tianchi.aliyun.com/dataset/90163"target="_blank" rel="external nofollow noopener noreferrer">https://tianchi.aliyun.com/dataset/90163</a> MedDialog中文医疗对话数据集https://tianchi.aliyun.com/dataset/92110 HearlthcareMagic</td>
      </tr>
      <tr>
          <td>MedicalGPT-zh</td>
          <td><a href="https://github.com/MediaBrain-SJTU/MedicalGPT-zh"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/MediaBrain-SJTU/MedicalGPT-zh</a></td>
          <td>-基于Llama7b的医学垂域模型 -自建的医学数据库ChatGPT生成QA+16个情境下SELF构建情景对话</td>
          <td></td>
      </tr>
      <tr>
          <td>PMC-LLaMA</td>
          <td><a href="https://github.com/chaoyi-wu/PMC-LLaMA"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/chaoyi-wu/PMC-LLaMA</a></td>
          <td>医疗论文微调Llama</td>
          <td><a href="https://github.com/allenai/s2orc"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/allenai/s2orc</a></td>
      </tr>
      <tr>
          <td>NHS-LLM</td>
          <td><a href="https://github.com/CogStack/OpenGPT/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CogStack/OpenGPT/tree/main</a></td>
          <td>Chatgpt生成的医疗问答，对话，微调模型</td>
          <td><a href="https://github.com/CogStack/OpenGPT/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CogStack/OpenGPT/tree/main</a></td>
      </tr>
      <tr>
          <td>Med-ChatGLM</td>
          <td><a href="https://github.com/SCIR-HI/Med-ChatGLM"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/SCIR-HI/Med-ChatGLM</a></td>
          <td>医学知识图谱和chatgpt构建中文医学指令数据集+医学文献和chatgpt构建多轮问答数据</td>
          <td>CMeKG知识图谱https://github.com/king-yyf/CMeKG_tools</td>
      </tr>
      <tr>
          <td>网新启真13B</td>
          <td><a href="https://github.com/CMKRG/QiZhenGPT"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CMKRG/QiZhenGPT</a></td>
          <td>-基于Llama7b模型的医学垂域模型 -基于浙大知识库及在线问诊构建的中文医学指令数据集</td>
          <td><a href="https://github.com/CMKRG/QiZhenGPT/blob/main/data/train/sft-20k.json"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CMKRG/QiZhenGPT/blob/main/data/train/sft-20k.json</a></td>
      </tr>
      <tr>
          <td>BenTsao</td>
          <td></td>
          <td>-经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。 -通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果</td>
          <td></td>
      </tr>
      <tr>
          <td>BianQue</td>
          <td><a href="https://blog.csdn.net/m0_37201243/article/details/130334076"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/m0_37201243/article/details/130334076</a></td>
          <td>-经过指令与多轮问询对话联合微调的医疗对话大模型 -基于ClueAI/ChatYuan-large-v2作为底座 -使用中文医疗问答指令与多轮问询对话混合数据集进行微调</td>
          <td>hinese medical dialogue data 中文医疗问答数据集 <a href="https://github.com/Toyhom/Chinese-medical-dialogue-data"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Toyhom/Chinese-medical-dialogue-data</a> cMedQA2 <a href="https://github.com/zhangsheng93/cMedQA2"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zhangsheng93/cMedQA2</a> IMCS-21 <a href="https://github.com/lemuria-wchen/imcs21"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/lemuria-wchen/imcs21</a> Medical-Dialogue-System <a href="https://github.com/UCSD-AI4H/Medical-Dialogue-System"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/UCSD-AI4H/Medical-Dialogue-System</a></td>
      </tr>
  </tbody>
</table>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">Public - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/"></a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">Theme FixIt works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="preload" href="/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-desktop":"Med-Eval 医疗大语言模型测评基准","typeit-header-title-mobile":"Med-Eval 医疗大语言模型测评基准"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-desktop":["typeit-header-desktop"],"typeit-header-title-mobile":["typeit-header-title-mobile"]},"duration":-1,"loop":false,"speed":100}};</script><script src="/js/theme.min.js" defer></script></body>
</html>
